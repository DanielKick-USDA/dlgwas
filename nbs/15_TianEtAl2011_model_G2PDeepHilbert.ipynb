{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9456e3",
   "metadata": {},
   "source": [
    "# Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve \n",
    "\n",
    "> This model uses hilbert curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de638fa",
   "metadata": {},
   "source": [
    "<!-- It used data from panzea\n",
    "- Phenotypic data panzea\\phenotypes\\Buckler_etal_2009_Science_flowering_time_data-090807\\\n",
    "- Genotypic Data panzea\\genotypes\\GBS\\v27\\ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023.vcf.gz\n",
    "- Genomic Data ...  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ad3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n",
    "# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n",
    "#                          Hours to wait\n",
    "# import time; time.sleep( 24 * (60*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5bb0a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlgwas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2130467/3065178254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlgwas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdlgwas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkegg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensure_dir_path_exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdlgwas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkegg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cached_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlgwas'"
     ]
    }
   ],
   "source": [
    "# Run Settings:\n",
    "nb_name = '15_TianEtAl2011'# Set manually! -----------------------------------\n",
    "\n",
    "downsample_obs = False\n",
    "train_n = 90\n",
    "test_n = 10\n",
    "\n",
    "dataloader_batch_size = 8 #16 #64\n",
    "run_epochs = 200\n",
    "\n",
    "use_gpu_num = 0\n",
    "\n",
    "# Imports --------------------------------------------------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "\n",
    "import dlgwas\n",
    "from dlgwas.kegg import ensure_dir_path_exists\n",
    "from dlgwas.kegg import get_cached_result\n",
    "from dlgwas.kegg import put_cached_result\n",
    "\n",
    "from dlgwas.dlfn import calc_cs\n",
    "from dlgwas.dlfn import apply_cs\n",
    "from dlgwas.dlfn import reverse_cs\n",
    "\n",
    "from dlgwas.dlfn import TianEtAl2011Dataset\n",
    "from dlgwas.dlfn import train_loop\n",
    "from dlgwas.dlfn import train_error\n",
    "from dlgwas.dlfn import test_loop\n",
    "from dlgwas.dlfn import train_nn\n",
    "from dlgwas.dlfn import yhat_loop\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "ensure_dir_path_exists(dir_path = '../models/'+nb_name)\n",
    "ensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n",
    "\n",
    "ensure_dir_path_exists(dir_path = '../models/'+nb_name)\n",
    "ensure_dir_path_exists(dir_path = '../reports/'+nb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a8cf4",
   "metadata": {},
   "source": [
    "##  Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in cleaned data\n",
    "taxa_groupings = pd.read_csv('../models/10_TianEtAl2011/taxa_groupings.csv')\n",
    "data           = pd.read_csv('../models/10_TianEtAl2011/clean_data.csv')\n",
    "\n",
    "# Define holdout sets (Populations)\n",
    "uniq_pop = list(set(taxa_groupings['Population']))\n",
    "print(str(len(uniq_pop))+\" Unique Holdout Groups.\")\n",
    "taxa_groupings['Holdout'] = None\n",
    "for i in range(len(uniq_pop)):\n",
    "    mask = (taxa_groupings['Population'] == uniq_pop[i])\n",
    "    taxa_groupings.loc[mask, 'Holdout'] = i\n",
    "\n",
    "taxa_groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c685f85",
   "metadata": {},
   "source": [
    "## Setup Holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38432480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly holdout a population if there is not a file with the population held out.\n",
    "# Holdout_Int = 0\n",
    "Holdout_Int_path = '../models/'+nb_name+'/holdout_pop_int.pkl'\n",
    "if None != get_cached_result(Holdout_Int_path):\n",
    "    Holdout_Int = get_cached_result(Holdout_Int_path)\n",
    "else:\n",
    "    Holdout_Int = int(np.random.choice([i for i in range(len(uniq_pop))], 1))\n",
    "    put_cached_result(Holdout_Int_path, Holdout_Int)\n",
    "\n",
    "    \n",
    "print(\"Holding out i=\"+str(Holdout_Int)+\": \"+uniq_pop[Holdout_Int])\n",
    "\n",
    "mask = (taxa_groupings['Holdout'] == Holdout_Int)\n",
    "train_idxs = list(taxa_groupings.loc[~mask, ].index)\n",
    "test_idxs = list(taxa_groupings.loc[mask, ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample_obs = True\n",
    "# train_n = 900\n",
    "# test_n = 100\n",
    "\n",
    "if downsample_obs == True:\n",
    "    train_idxs = np.random.choice(train_idxs, train_n)\n",
    "    test_idxs = np.random.choice(test_idxs, test_n)\n",
    "    print([len(e) for e in [test_idxs, train_idxs]])\n",
    "    \n",
    "# used to go from index in tensor to index in data so that the right xs tensor can be loaded in\n",
    "idx_original = np.array(data.index)\n",
    "\n",
    "y1 = data['leaf_length']\n",
    "y2 = data['leaf_width']\n",
    "y3 = data['upper_leaf_angle']\n",
    "y1 = np.array(y1)\n",
    "y2 = np.array(y2)\n",
    "y3 = np.array(y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96baeb0b",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191aeb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict_path = '../models/'+nb_name+'/scale_dict.pkl'\n",
    "if None != get_cached_result(scale_dict_path):\n",
    "    scale_dict = get_cached_result(scale_dict_path)\n",
    "else:\n",
    "    scale_dict = {\n",
    "        'y1':calc_cs(y1[train_idxs]),\n",
    "        'y2':calc_cs(y2[train_idxs]),\n",
    "        'y3':calc_cs(y3[train_idxs])\n",
    "    }\n",
    "    put_cached_result(scale_dict_path, scale_dict)\n",
    "\n",
    "y1 = apply_cs(y1, scale_dict['y1'])\n",
    "y2 = apply_cs(y2, scale_dict['y2'])\n",
    "y3 = apply_cs(y3, scale_dict['y3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e73590",
   "metadata": {},
   "source": [
    "## Allow for cycling data onto and off of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07665257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading this into memory causes the session to crash\n",
    "\n",
    "y1_train = torch.from_numpy(y1[train_idxs])[:, None]\n",
    "y2_train = torch.from_numpy(y2[train_idxs])[:, None]\n",
    "y3_train = torch.from_numpy(y3[train_idxs])[:, None]\n",
    "\n",
    "idx_original_train = torch.from_numpy(idx_original[train_idxs])\n",
    "\n",
    "y1_test = torch.from_numpy(y1[test_idxs])[:, None]\n",
    "y2_test = torch.from_numpy(y2[test_idxs])[:, None]\n",
    "y3_test = torch.from_numpy(y3[test_idxs])[:, None]\n",
    "\n",
    "idx_original_test = torch.from_numpy(idx_original[test_idxs])\n",
    "\n",
    "\n",
    "# dataloader_batch_size = 64\n",
    "\n",
    "training_dataloader = DataLoader(\n",
    "    TianEtAl2011Dataset(\n",
    "        y1 = y1_train,\n",
    "        y2 = y2_train,\n",
    "        y3 = y3_train,\n",
    "        marker_type = 'hilbert',\n",
    "        idx_original = idx_original_train,\n",
    "        use_gpu_num = use_gpu_num,\n",
    "#         device = 'cpu'\n",
    "    ), \n",
    "    batch_size = dataloader_batch_size, \n",
    "    shuffle = True)\n",
    "\n",
    "testing_dataloader = DataLoader(\n",
    "    TianEtAl2011Dataset(\n",
    "        y1 = y1_test,\n",
    "        y2 = y2_test,\n",
    "        y3 = y3_test,\n",
    "        marker_type = 'hilbert',\n",
    "        idx_original = idx_original_test,\n",
    "        use_gpu_num = use_gpu_num,\n",
    "#         device = 'cpu'\n",
    "    ), \n",
    "    batch_size = dataloader_batch_size, \n",
    "    shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8081c",
   "metadata": {},
   "source": [
    "## Non-Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()    \n",
    "\n",
    "        # Block 1 ------------------------------------------------------------\n",
    "        self.long_way_0 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, # second channel\n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    bias = True\n",
    "                ),\n",
    "#             nn.BatchNorm1d(4),\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 1,\n",
    "                    padding = 1,\n",
    "                    bias = True\n",
    "                ),\n",
    "#             nn.BatchNorm1d(4),\n",
    "            nn.Dropout(p=0.75)\n",
    "            )\n",
    "        \n",
    "        self.shortcut_0 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    bias = True\n",
    "                )\n",
    "        )\n",
    "        # Block 2 ------------------------------------------------------------\n",
    "        self.long_way_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, # second channel\n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    bias = True\n",
    "                ),\n",
    "#             nn.BatchNorm1d(4),\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 1,\n",
    "                    padding = 1,\n",
    "                    bias = True\n",
    "                ),\n",
    "#             nn.BatchNorm1d(4),\n",
    "            nn.Dropout(p=0.75)\n",
    "            )\n",
    "        \n",
    "        self.shortcut_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    bias = True\n",
    "                )\n",
    "        )\n",
    "        # Block 3 ------------------------------------------------------------\n",
    "        self.long_way_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, # second channel\n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    bias = True\n",
    "                ),\n",
    "#             nn.BatchNorm1d(4),\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 1,\n",
    "                    padding = 1,\n",
    "                    bias = True\n",
    "                ),\n",
    "#             nn.BatchNorm1d(4),\n",
    "            nn.Dropout(p=0.75)\n",
    "            )\n",
    "        \n",
    "        self.shortcut_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    bias = True\n",
    "                )\n",
    "        )        \n",
    "        \n",
    "        \n",
    "        self.feature_processing = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride = 2,\n",
    "                    bias = True\n",
    "                ),\n",
    "            nn.Dropout(p=0.75)\n",
    "        )\n",
    "\n",
    "        self.output_processing = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(), # They used inverse square root activation $y = \\frac{x}{\\sqrt{1+ax^2}}$\n",
    "            nn.Dropout(p=0.75),\n",
    "            nn.Linear(15876, 1)\n",
    "        )            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_out = self.long_way_0(x)\n",
    "        x_shortcut = self.shortcut_0(x)\n",
    "        x_out += x_shortcut\n",
    "        \n",
    "        x = x_out\n",
    "        x_out = self.long_way_1(x)\n",
    "        x_shortcut = self.shortcut_1(x)\n",
    "        x_out += x_shortcut\n",
    "        \n",
    "        x = x_out\n",
    "        x_out = self.long_way_2(x)\n",
    "        x_shortcut = self.shortcut_2(x)        \n",
    "        x_out += x_shortcut\n",
    "        \n",
    "        x_out = self.feature_processing(x_out)\n",
    "        x_out = self.output_processing(x_out)\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n",
    "\n",
    "# model = NeuralNetwork().to(device)\n",
    "\n",
    "# res = model(xs_i) # try prediction on one batch\n",
    "# res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb72ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe7bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29402a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(\n",
    "    nb_name,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 64,\n",
    "    epochs = 500\n",
    "):\n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Optimizer with L2 normalization\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params':model.long_way_0.parameters(), 'weight_decay': 0.1},\n",
    "        {'params':model.shortcut_0.parameters(), 'weight_decay': 0.1},\n",
    "        {'params':model.long_way_1.parameters(), 'weight_decay': 0.1},\n",
    "        {'params':model.shortcut_1.parameters(), 'weight_decay': 0.1},\n",
    "        {'params':model.long_way_2.parameters(), 'weight_decay': 0.1},\n",
    "        {'params':model.shortcut_2.parameters(), 'weight_decay': 0.1},\n",
    "        {'params':model.feature_processing.parameters(),        'weight_decay': 0.1},\n",
    "        {'params':model.output_processing.parameters(),         'weight_decay': 0.01},\n",
    "    ], lr=learning_rate)\n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "#         print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%10: # Cache in case training is interupted\n",
    "#             print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n",
    "            torch.save(model.state_dict(), \n",
    "                       '../models/'+nb_name+'/model_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "            loss_df.to_csv('../reports/'+nb_name+'/loss_df'+str(t)+'_'+str(epochs)+'.csv', index=False) \n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b823b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run if either of these exist because there may be cases where we want the results but not the model\n",
    "\n",
    "if not os.path.exists('../models/'+nb_name+'/model.pt'): \n",
    "    # Shared setup (train from scratch and load latest)\n",
    "    model = NeuralNetwork()\n",
    "\n",
    "    # find the biggest model to save\n",
    "    saved_models = os.listdir('../models/'+nb_name+'/')\n",
    "    saved_models = [e for e in saved_models if re.match('model*', e)]\n",
    "\n",
    "    if saved_models == []:\n",
    "        epochs_run = 0\n",
    "    else:\n",
    "        # if there are saved models reload and resume training\n",
    "        saved_models_numbers = [int(e.replace('model_', ''\n",
    "                                    ).replace('.pt', ''\n",
    "                                    ).split('_')[0]) for e in saved_models]\n",
    "        # saved_models\n",
    "        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n",
    "        latest_model = [e for e in saved_models if re.match(\n",
    "            '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n",
    "        model.load_state_dict(torch.load('../models/'+nb_name+'/'+latest_model))\n",
    "        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n",
    "    \n",
    "    model.to(device)   \n",
    "\n",
    "    model, loss_df = train_nn(\n",
    "        nb_name,\n",
    "        training_dataloader,\n",
    "        testing_dataloader,\n",
    "        model,\n",
    "        learning_rate = 1e-3,\n",
    "        batch_size = dataloader_batch_size,\n",
    "        epochs = (run_epochs - epochs_run)\n",
    "    )\n",
    "    \n",
    "    # experimental outputs:\n",
    "    # 1. Model\n",
    "    torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n",
    "\n",
    "    # 2. loss_df\n",
    "    loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)  \n",
    "    \n",
    "    \n",
    "    # 3. predictions \n",
    "    yhats = pd.concat([\n",
    "        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n",
    "        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n",
    "\n",
    "    yhats.to_csv('../reports/'+nb_name+'/yhats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34a78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f36b1e",
   "metadata": {},
   "source": [
    "## Standard Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.read_csv('../reports/'+nb_name+'/loss_df.csv')\n",
    "\n",
    "loss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\n",
    "loss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n",
    "                    mode='lines', name='Test'))\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n",
    "                    mode='lines', name='Train'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04d25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42632173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49cafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats = pd.read_csv('../reports/'+nb_name+'/yhats.csv')\n",
    "\n",
    "yhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\n",
    "yhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n",
    "\n",
    "px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29140ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats['Error'] = yhats.y_true - yhats.y_pred\n",
    "\n",
    "px.histogram(yhats, x = 'Error', color = 'Split',\n",
    "             marginal=\"box\", # can be `rug`, `violin`\n",
    "             nbins= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically kill kernel after running. \n",
    "# This is a hacky way to free up _all_ space on the gpus\n",
    "# os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
