[
  {
    "objectID": "zma_kegg_download.html",
    "href": "zma_kegg_download.html",
    "title": "SNPS to Networks 1: KEGG",
    "section": "",
    "text": "import os\nimport tqdm\nfrom tqdm import tqdm\nimport re\nimport time\nimport sys\nimport requests\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nimport pickle5 as pkl"
  },
  {
    "objectID": "zma_kegg_download.html#helpful-custom-functions",
    "href": "zma_kegg_download.html#helpful-custom-functions",
    "title": "SNPS to Networks 1: KEGG",
    "section": "Helpful Custom Functions",
    "text": "Helpful Custom Functions\n\nsource\n\nensure_dir_path_exists\n\n ensure_dir_path_exists (dir_path='../ext_data')\n\n\nsource\n\n\nget_cached_result\n\n get_cached_result (save_path)\n\n\nsource\n\n\nput_cached_result\n\n put_cached_result (save_path, save_obj)\n\n\n# # This is the expected boiler plate for using the above functions. \n\n# save_path = './demofile.txt'\n\n# demo = get_cached_result(save_path=save_path)\n\n# if None == samples_and_matches:\n#     demo = CODE TO MAKE DEMO HERE\n    \n#     put_cached_result(\n#         save_path = save_path,\n#         save_obj = demo\n#     )"
  },
  {
    "objectID": "zma_kegg_download.html#identify-genes-to-be-downloaded",
    "href": "zma_kegg_download.html#identify-genes-to-be-downloaded",
    "title": "SNPS to Networks 1: KEGG",
    "section": "Identify genes to be downloaded:",
    "text": "Identify genes to be downloaded:\n\nsource\n\nget_kegg_species_list\n\n get_kegg_species_list (species='zma')\n\n\nsource\n\n\nmkdf_kegg_species_list\n\n mkdf_kegg_species_list (kegg_species_list)\n\nThe KEGG list contains a gene id that can be used to download further information. The chromosomal position will be key to match up collected SNPs to genes.\n\nkegg_zma_list = get_kegg_species_list(species = 'zma')\nkegg_list_zma = mkdf_kegg_species_list(kegg_species_list = kegg_zma_list)\n\nThere are also entries that don’t start with a chromosome number. These are non-nuclear sequences from plastid (Pltd), Mitochondria (MT), etc.\n\nno_chrm = [e for e in kegg_list_zma['chromosome_positon'] if re.match('\\D.+', e)]\nno_chrm = list(set(no_chrm))\nkegg_list_zma.loc[kegg_list_zma.chromosome_positon.isin(no_chrm), ]\n\n\n\n\n\n\n\n\ngene\nseq_type\nchromosome_positon\ngene_type\n\n\n\n\n35639\nzma:845199\nCDS\nPltd:complement(89..1150)\npsbA, ZemaCp002; photosystem II protein D1\n\n\n35640\nzma:845256\ntRNA\nPltd:complement(1386..3946)\ntrnK, ZemaCt121; tRNA-Lys\n\n\n35641\nzma:845178\nCDS\nPltd:complement(1674..3215)\nmatK, ZemaCp003; maturase K\n\n\n35642\nzma:845232\nCDS\nPltd:complement(4491..5604)\nrps16, ZemaCp004; ribosomal protein S16\n\n\n35643\nzma:845265\ntRNA\nPltd:complement(6773..6844)\ntrnQ, ZemaCt122; tRNA-Gln\n\n\n...\n...\n...\n...\n...\n\n\n37707\nzma:118475995\nrRNA\nUnknown\n28S ribosomal RNA\n\n\n37708\nzma:118475996\nrRNA\nUnknown\n5.8S ribosomal RNA\n\n\n37709\nzma:5951366\nCDS\nMT\npBMSmt19_00005; hypothetical protein\n\n\n37710\nzma:5951368\nCDS\nMT\npBMSmt19_00010; hypothetical protein\n\n\n37711\nzma:5951367\nCDS\nMT\npBMSmt19_00015; hypothetical protein\n\n\n\n\n2066 rows × 4 columns"
  },
  {
    "objectID": "zma_kegg_download.html#download-and-cache-kegg-entries",
    "href": "zma_kegg_download.html#download-and-cache-kegg-entries",
    "title": "SNPS to Networks 1: KEGG",
    "section": "Download and cache KEGG entries",
    "text": "Download and cache KEGG entries\n\nsource\n\ndownload_kegg_gene\n\n download_kegg_gene (kegg_gene='zma:103644366', **kwargs)\n\nDownloads kegg gene entry if it does not exist locally. Can optionally take a numeric value as sleep_for to sleep after downloading a file. Useful for controlling the rate requests being sent to the API.\n\nsource\n\n\nread_kegg_gene\n\n read_kegg_gene (kegg_gene='zma:103644366', **kwargs)\n\nReads in locally cached KEGG gene entries. Will download the requested entry if it doesn’t exist locally.\nExample usage for one gene. Species is inferred and if it doesn’t exist in ext_data then it will automatically be downloaded with download_kegg_gene().\n\nprint(read_kegg_gene(kegg_gene = 'zma:103644366'))\n\nENTRY       103644366         CDS       T01088\nNAME        (RefSeq) uncharacterized protein LOC103644366\nORTHOLOGY   K15032  mTERF domain-containing protein, mitochondrial\nORGANISM    zma  Zea mays (maize)\nBRITE       KEGG Orthology (KO) [BR:zma00001]\n             09180 Brite Hierarchies\n              09182 Protein families: genetic information processing\n               03012 Translation factors [BR:zma03012]\n                103644366\n               03029 Mitochondrial biogenesis [BR:zma03029]\n                103644366\n            Translation factors [BR:zma03012]\n             Eukaryotic type\n              Release factors\n               103644366\n            Mitochondrial biogenesis [BR:zma03029]\n             Mitochondrial DNA transcription, translation, and replication factors\n              Mitochondrial transcription and translation factors\n               Mitochondrial translation factors\n                103644366\nPOSITION    1:34607..40208\nMOTIF       Pfam: mTERF\nDBLINKS     NCBI-GeneID: 103644366\n            NCBI-ProteinID: XP_020400304\nAASEQ       644\n            MPWIMHAGTEQRHAACGASLLWSSLQPSTVVMAAAAATFGFLHPPIRKPAVPPLYILRLP\n            TKPHSKTHPRSPPLLFLLLGRRRGGPIAAFPNTTSSSTNAPASPTYDVREAEAAVADLLR\n            EGGASADDAASIAARAPAYAAMLADGVRELDELGLWASWSSGARARLGLSGVVEMEMGRL\n            GFRRKVYLMGRSKPDHGVVPLLESLGMRLSSAKLIAPYVAAAGLTVLIDRVKFLKEMLFS\n            SSDYAILIGRNAKRMMTYLSIPADDALQSTLSFFEKMEARYGGVSMLGHGDVSFPYLIES\n            FPMLLLCSEDNHLKPLVDFLEHIGIPKPKIASVLLLFPPIILSDVENDIKPRIREWEKAG\n            IEQDYVSRMLLKYPWILSTSVIENYSQMLLFFNQKRISSTVLAIAVKSWPHILGSSSKRM\n            NSVLELFHVLGISKKMVVPVITSSPQLLLRKPDQFMQNVLFFREMGVDKKTTGKILCRSP\n            EIFASNVDNTLKKKIDFLTNFGVSKHHLPRIIRKYPELLLLDINCTLLPRMNYLLEMGLS\n            KKDLCSMIFRFSPLLGYSIELVMKPKLEFLLRTMKKPLKAVVEYPRYFSYSLEGKIKPRF\n            WVLQSRNIDCTLTEMLAKNDELFAEEYLGLGGLLEKPLQSSIGS\nNTSEQ       1935\n            atgccatggataatgcacgcgggaacggaacaaagacacgcggcgtgcggggcctcgctc\n            ctctggtcttccctccagccctcgacggtggtcatggccgccgccgccgccactttcggc\n            ttcctccatcctccaatccggaaacctgcagtcccaccactgtacattctccggcttccc\n            accaaaccccactccaaaacgcaccctcgttctccccccctcctcttcctcctcctgggc\n            cgccgccggggaggccccatcgccgccttccccaacaccacatcttcatccacgaatgcc\n            cctgcctcgcccacctacgacgtccgggaggcagaggccgccgtcgcggatctcctccgc\n            gagggcggcgcctccgcggacgacgccgcctccatcgccgcgcgcgcgccggcctacgcc\n            gctatgctcgccgatggcgtccgcgagctggacgagcttggcctctgggcgtcgtggagc\n            tccggtgccagggcccggctgggcctcagcggggtcgtcgagatggagatggggaggctc\n            ggttttaggaggaaggtgtatctcatgggacggagcaagcctgaccacggcgtggtgccg\n            ctcctcgagagcttgggaatgcgtctctcctcggccaaactcatcgcgccttacgtcgcg\n            gctgcgggccttactgtgctgattgatagggttaagtttttgaaggaaatgttattttca\n            agcagtgattatgcaatactaattggaaggaatgctaagcggatgatgacatacttatca\n            atacctgcagatgatgcactccaaagtactttatctttttttgaaaaaatggaggctagg\n            tatggtggtgttagcatgttgggacatggagatgtgtcatttccttacctcattgaatca\n            tttccgatgcttcttctctgctcagaagataatcatctcaagccgttagttgattttctc\n            gagcacattggaattccaaagccaaagattgcatcagttctgctgctatttcctcctatc\n            attctttctgatgttgaaaatgatattaagcctaggattcgtgaatgggagaaggctggc\n            attgaacaagactatgttagtaggatgttgttgaagtatccatggattctttcaacgagt\n            gtgatagagaactacagtcaaatgctgttgtttttcaaccaaaaaaggatttccagtaca\n            gtcctcgctattgctgtgaaaagttggcctcatattcttggctcctcttcaaaaagaatg\n            aattcagttttggagctgtttcatgttctgggcatcagtaaaaaaatggtggttccagtc\n            attacatcaagtccacagttattactgagaaaacctgatcagtttatgcagaatgttttg\n            tttttcagagaaatgggtgttgataagaaaacaacaggaaaaattctgtgtcgttcgcct\n            gaaatatttgcttcaaacgtggataacaccctcaagaagaaaatcgattttcttaccaac\n            tttggtgtttctaaacatcatcttcctcgcatcattcggaagtatccagaacttttattg\n            ttggacataaattgtacattgctccctaggatgaactacttattggagatgggtttgtct\n            aagaaagatctgtgctcaatgatctttagattttccccacttctaggttacagtattgaa\n            cttgttatgaaaccaaagcttgagtttctgctaagaaccatgaagaagccacttaaagca\n            gttgtagaatacccaaggtacttcagttattcactcgaggggaagatcaaaccgcggttc\n            tgggtattgcagagtagaaacatagactgcactctgacagagatgttagcaaagaacgat\n            gaactctttgctgaagagtacttgggacttggaggattgctcgagaaacctctacaatca\n            agcataggcagttaa\n///\n\n\n\nTo cache all genes on KEGG for an organism, loop over all entries in the KEGG list for that species.\n\nfor kegg_gene in tqdm(kegg_list_zma.gene):\n    try:\n        download_kegg_gene(kegg_gene = kegg_gene, \n                           sleep_for = np.random.uniform(0.5, 1.5))\n    except:\n        print('Problem with '+kegg_gene)\n\n100%|██████████████████████████████████████████████████████████████████████████| 37712/37712 [00:00&lt;00:00, 76079.21it/s]"
  },
  {
    "objectID": "zma_kegg_download.html#imported-from-02",
    "href": "zma_kegg_download.html#imported-from-02",
    "title": "SNPS to Networks 1: KEGG",
    "section": "Imported from 02",
    "text": "Imported from 02"
  },
  {
    "objectID": "zma_kegg_download.html#load-zea-mays-gene-list",
    "href": "zma_kegg_download.html#load-zea-mays-gene-list",
    "title": "SNPS to Networks 1: KEGG",
    "section": "Load Zea mays gene list",
    "text": "Load Zea mays gene list\n\nkegg_zma_list = get_kegg_species_list(species = 'zma')\n\n\nkegg_zma_list = get_kegg_species_list(species = 'zma')\n\n\nkegg_list_zma = mkdf_kegg_species_list(kegg_species_list = kegg_zma_list)\n\n\nkegg_list_zma.head()\n\n\n\n\n\n\n\n\ngene\nseq_type\nchromosome_positon\ngene_type\n\n\n\n\n0\nzma:103644366\nCDS\n1:34607..40208\nuncharacterized protein LOC103644366\n\n\n1\nzma:100382519\nCDS\n1:complement(41254..46100)\nuncharacterized protein LOC100382519\n\n\n2\nzma:103649349\nCDS\n1:complement(92293..107876)\nprotein FAR1-RELATED SEQUENCE 5-like\n\n\n3\nzma:115032971\nCDS\n1:complement(187229..189591)\nuncharacterized protein LOC115032971\n\n\n4\nzma:103630223\nCDS\n1:complement(200404..203170)\ngranule-bound starch synthase 1b, chloroplasti..."
  },
  {
    "objectID": "zma_kegg_download.html#read-kegg-gene-entries",
    "href": "zma_kegg_download.html#read-kegg-gene-entries",
    "title": "SNPS to Networks 1: KEGG",
    "section": "Read KEGG gene entries",
    "text": "Read KEGG gene entries\n\nConvert a single file into a dictionary\nThe gene files contain sections (ENTRY, NAME, AASEQ, etc.). To aid processing each of these sections will be separated into a key in a dictionary.\n\nkegg_file = 'zma_100125650.txt'\n# convert back to KEGG format\nkegg_gene_name = kegg_file.replace('_', ':').replace('.txt', '')\n\n# convert back to kegg format\nr_text = read_kegg_gene(kegg_gene = kegg_gene_name)\n\n# parsing gene entry\nr_text_list = r_text.split('\\n')\nr_text_list[0:5]\n\n['ENTRY       100125650         CDS       T01088',\n 'NAME        (RefSeq) barren inflorescence 2',\n 'ORGANISM    zma  Zea mays (maize)',\n 'POSITION    1:177940567..177942323',\n 'MOTIF       Pfam: Pkinase PK_Tyr_Ser-Thr Kinase-like Pkinase_fungal']\n\n\n\n# Some files may not contain all sections so this list needs to be created for each file\nsection_names = [r_text_list[i][0:12].strip() for i in range(len(r_text_list)) ]\nsection_names = [e for e in section_names if e != '']\n\nsection_starts = [\n    [i for i in range(len(r_text_list)) if re.match('^'+e, r_text_list[i])][0] \n    for e in section_names]\n\nsection_names, section_starts\n\n(['ENTRY',\n  'NAME',\n  'ORGANISM',\n  'POSITION',\n  'MOTIF',\n  'DBLINKS',\n  'AASEQ',\n  'NTSEQ',\n  '///'],\n [0, 1, 2, 3, 4, 5, 8, 18, 44])\n\n\n\n# just split each file into text of it's sections\nout = {}\n\n# get lines associated with section\ndef _get_section_text(section_name = 'AASEQ'):\n    idx = section_names.index(section_name)\n    section_text = r_text_list[section_starts[idx]:section_starts[idx+1]]\n    # remove leading indent\n    section_text = [e[12:] for e in section_text]\n    return(section_text)\n\nfor section_name in section_names:\n    if section_name != '///': # end of file\n        out[section_name] = _get_section_text(section_name = section_name)\n\n\nout\n\n{'ENTRY': ['100125650         CDS       T01088'],\n 'NAME': ['(RefSeq) barren inflorescence 2'],\n 'ORGANISM': ['zma  Zea mays (maize)'],\n 'POSITION': ['1:177940567..177942323'],\n 'MOTIF': ['Pfam: Pkinase PK_Tyr_Ser-Thr Kinase-like Pkinase_fungal'],\n 'DBLINKS': ['NCBI-GeneID: 100125650',\n  'NCBI-ProteinID: NP_001106051',\n  'UniProt: A6MW92'],\n 'AASEQ': ['491',\n  'MDAAVRVPPALGNKTVTEVTPPPPPPAGEERLSDADTTASSTAAPNSSLSSASSAASLPR',\n  'CSSLSRLSFDCSPSAALSSSSAAAAAAAASSPAPAPARPHRAGDAAWAAIRAASASAAAP',\n  'LGPRDFRLLRRVGGGDVGTVYLCRLRAPPAPAPVCCLYAMKVVDRRVAAAKKKLEHAAAE',\n  'RRILRALDHPFLPTLFADFDAAPHFSCVVTEFCPGGDLHSLRHRMPNRRFPLPSARFYAA',\n  'EVLLALEYLHMMGIVYRDLKPENVLIRADGHIMLTDFDLSLQCTSTPSLEPCAAPEAAAA',\n  'SCFPDHLFRRRRARLRRAASARRPPTTLVAEPVEARSCSFVGTHEYVAPEVARGGPHGAA',\n  'VDWWALGVFLYELLHGRTPFAGADNEATLRNIARRPLSFPAAGAGDADARDLIARLLAKD',\n  'PRHRLGSRRGAADVKAHPFFRGLNFALLRSSRPPVVPAASRSPLHRSQSCSAARTRASKP',\n  'KPPPDTRFDLF'],\n 'NTSEQ': ['1476',\n  'atggacgccgcggtgcgcgtccccccggcgctcgggaacaagacggtgaccgaggtgacg',\n  'ccgccgccgccaccaccggcgggggaggagcggctgtcggacgccgacacgacggcgtcg',\n  'tcgacggcggcgcccaactcgagcctcagctcggccagcagcgccgccagcctgccgcgc',\n  'tgctccagcctgtcccgcctctccttcgactgctctccgtccgcggccctgtcctcttcc',\n  'tcggcggcggcggcggccgcggccgcgtcatcgccggcgccagcgccggcgcggccgcac',\n  'cgggcaggggacgcggcgtgggcggcgatccgcgcggcgtcggcgtcggccgcggcgccg',\n  'ctggggccgcgggacttcaggctgctgcgccgcgtgggcggcggcgacgtcggcaccgtg',\n  'tacctgtgccgcctcagggcgccacccgcgcccgcgcccgtctgctgcctgtacgcgatg',\n  'aaggtggtggaccggcgcgtggcggccgcgaagaagaagctggagcacgcggcggcggag',\n  'cggcggatcctgcgggcgctggaccatccgttcctgcccacgctcttcgccgacttcgac',\n  'gccgcgccgcacttctcctgcgtcgtcacggagttctgccccggcggggacctccactcg',\n  'ctccgccaccgcatgcccaaccgccgcttcccgctcccgtcagctcggttctacgcggcg',\n  'gaggtgttgctggcgctggagtacctgcacatgatgggcatcgtgtaccgcgacctcaag',\n  'ccggagaacgtgctgatccgcgcggacggccacatcatgctcacggacttcgacctgtcg',\n  'ctgcagtgcacgtcgacgccgtcgctcgagccgtgcgccgcccccgaggcggcggcggcg',\n  'tcctgcttcccggaccacctgttccgccgccggcgcgcgcgactccgccgtgccgcctcg',\n  'gcgcggcggccgccaacgaccctggtggcggagccggtggaggcgcggtcgtgctcgttc',\n  'gtgggcacgcacgagtacgtggcgcccgaggtggcccgcggcgggccccacggcgcggcc',\n  'gtcgactggtgggcgctcggcgtgttcctgtacgagctcctgcacgggcgcaccccgttc',\n  'gcgggcgccgacaacgaggccacgctccgcaacatcgcgcgccgcccgctgtccttcccc',\n  'gctgccggcgccggtgatgccgacgcgcgcgacctcatcgcccgcctcctcgccaaggac',\n  'ccgcgccaccggttggggtcccggcgcggcgccgccgacgtgaaggcgcacccgttcttc',\n  'cgcgggctcaacttcgcgctgctccggtcctcccgcccgcccgtcgtccccgccgcgtcg',\n  'cgctccccgctgcaccgctcgcagtcctgcagcgcggcgcgcacgagagcgtcgaagccg',\n  'aagccgccgccggacacccggttcgacctgttctga']}\n\n\n\n\nConvert all downloaded gene files\n\ndef kegg_gene_to_dict(kegg_file = 'zma_100125650.txt',\n                     **kwargs):\n    # convert back to kegg format\n    if 'encoding' in kwargs.keys():\n        r_text = read_kegg_gene(\n            kegg_gene= kegg_file.replace('_', ':').replace('.txt', ''),\n            encoding  = kwargs['encoding']\n        )\n    else:\n        r_text = read_kegg_gene(\n            kegg_gene= kegg_file.replace('_', ':').replace('.txt', '') )\n    \n    # parsing gene entry\n    r_text_list = r_text.split('\\n')\n\n    # Some files may not contain all sections so this list needs to be created for each file\n    section_names = [r_text_list[i][0:12].strip() for i in range(len(r_text_list)) ]\n    section_names = [e for e in section_names if e != '']\n\n    section_starts = [\n        [i for i in range(len(r_text_list)) if re.match('^'+e, r_text_list[i])][0] \n        for e in section_names]\n\n    # just split each file into text of it's sections\n    out = {}\n\n    # get lines associated with section\n    def _get_section_text(section_name = 'AASEQ'):\n        idx = section_names.index(section_name)\n        section_text = r_text_list[section_starts[idx]:section_starts[idx+1]]\n        # remove leading indent\n        section_text = [e[12:] for e in section_text]\n        return(section_text)\n\n    for section_name in section_names:\n        if section_name != '///': # end of file\n            out[section_name] = _get_section_text(section_name = section_name)\n\n    return(out)\n\n\nsave_dir = '../data/zma/kegg/'\nsave_path = save_dir+'cached_kegg_gene_files.pkl'\n\nensure_dir_path_exists(dir_path = save_dir)\n\n\n# 27258 zma_100037738.txt\n\n# kegg_gene_to_dict(\n#             kegg_file = 'zma_100037738.txt',\n# #             encoding=\"ISO-8859-1\"\n#         )\n\n\n# kegg_gene_files = os.listdir('../ext_data/zma/kegg/gene_entries/')\n\n# lst_out = []\n\n# for i in range(len(kegg_gene_files)):\n#     kegg_gene_file = kegg_gene_files[i]\n#     print(i, kegg_gene_file)\n#     lst_out += [    \n#         kegg_gene_to_dict(\n#             kegg_file = kegg_gene_file,\n#             encoding=\"ISO-8859-1\"\n#         )]\n\n\nif not os.path.exists(save_path):\n    # This is a pain. I would like to parallelize reading these files in, but since it only has to be done once per species doing so would be premature optimization.\n    # BUT it seems that nbdev is rerunning it as part of the CI workflow. So caching it makes sense. \n    kegg_gene_files = os.listdir('../ext_data/zma/kegg/gene_entries/')\n#     kegg_gene_entries = [kegg_gene_to_dict(kegg_file = kegg_gene_file) for kegg_gene_file in tqdm.tqdm(kegg_gene_files)]\n    kegg_gene_entries = [kegg_gene_to_dict(\n        kegg_file = kegg_gene_file,\n        encoding=\"ISO-8859-1\"\n    ) for kegg_gene_file in kegg_gene_files]\n    # 37712/37712 [07:12&lt;00:00, 87.23it/s]\n\n    with open(save_path, 'wb') as handle:\n        pkl.dump(kegg_gene_entries, \n                 handle, \n                 protocol=pkl.HIGHEST_PROTOCOL)\nelse:\n    # Reading in data\n    with open(save_path, 'rb') as handle:\n        kegg_gene_entries = pkl.load(handle)"
  },
  {
    "objectID": "zma_kegg_download.html#clean-up-kegg-gene-dictionaries",
    "href": "zma_kegg_download.html#clean-up-kegg-gene-dictionaries",
    "title": "SNPS to Networks 1: KEGG",
    "section": "Clean up KEGG gene dictionaries",
    "text": "Clean up KEGG gene dictionaries\nThe goal here is to use a finite-state machine to convert the lines for each section into a useful representation. This approach makes extending or altering this code easier.\nStarting out I needed to find how many states were featured across all files.\n\n# produce a flat list of keys then deduplicate them\nkegg_gene_sections = [entry for sublist in [list(kegg_gene_entries[i].keys()) for i in range(len(kegg_gene_entries))] \n                  for entry in sublist] # entry is defined here so without it the list comprehension fails instead of producing the list of sublists\nkegg_gene_sections = list(set(kegg_gene_sections))\nkegg_gene_sections\n\n['ORTHOLOGY',\n 'MODULE',\n 'DBLINKS',\n 'PATHWAY',\n 'ENTRY',\n 'MOTIF',\n 'AASEQ',\n 'SYMBOL',\n 'BRITE',\n 'ORGANISM',\n 'STRUCTURE',\n 'POSITION',\n 'NAME',\n 'NTSEQ']\n\n\nTo aid in writing the logic for each section, pull several examples as a reference.\n\n# Helper Functions ---------------------------------------------------------------------\n# helper fcn to pull indices and examples of sections\ndef get_section_examples(section = 'MOTIF', n = 5):\n    i_section_matches = [i for i in range(len(kegg_gene_entries)) if section in kegg_gene_entries[i].keys()]\n    out = [[i_section_matches[i], \n            kegg_gene_entries[i_section_matches[i]][section] ] for i in range(n)]\n    return(out)\n\n\nget_section_examples(section = 'SYMBOL', n = 5)\n\n[[89, ['MIR395l']],\n [100, ['lox6']],\n [133, ['HDA108']],\n [147, ['MIR169o']],\n [167, ['MIR169p']]]\n\n\nNow create a function to process each section of the file or more precisely, each section type that needs a different treatment.\n\n# Any non-hierarical list of attributes can go here. It will be transformed into a dict\ndef _gene_entry_flat_list(section_entry):\n    # split like so [['NCBI-GeneID', '103644366'], ['NCBI-ProteinID', 'XP_020400304']]\n    # then convert to dict\n    # {'NCBI-GeneID': '103644366', 'NCBI-ProteinID': 'XP_020400304'}\n    section_entry = [e.replace(section_name, '').strip().split(': ') for e in section_entry]\n    section_entry = dict(section_entry)\n    return(section_entry)\n\n\n# 'AASEQ', 'NTSEQ'\ndef _gene_seq_to_dict(seq_list):\n    out = {}\n    out['lenght'] = int(seq_list[0])\n    out['seq'] = ''.join(seq_list[1:])\n    return(out)\n\nThe hardest one to think through was the “BRITE”. This section contains a tree categorizing the gene’s role(s). Here’s a sample excerpt:\nKEGG Orthology (KO) [BR:zma00001]\n 09120 Genetic Information Processing\n  09121 Transcription\n   03020 RNA polymerase\n    100037782\n 09180 Brite Hierarchies\n  09182 Protein families: genetic information processing\n   03021 Transcription machinery [BR:zma03021]\n    100037782\n# ...\nThe goal is to represent these data in a way that can be used to construct a hierarchy for all genes that are in a given sample. Level is defined by leading whitespace.\nKEGG Orthology (KO) [BR:zma00001]\n├ 09120 Genetic Information Processing\n│ └ 09121 Transcription\n│   └ 03020 RNA polymerase\n│     └ 100037782\n└ 09180 Brite Hierarchies\n  └ 09182 Protein families: genetic information processing\n    └ 03021 Transcription machinery [BR:zma03021]\n      └ 100037782\nParsing into a dictionary makes sense (and then creating a full hierarchy would be as simple as merging on keys) but building one is tricky to do since you can’t pass a list of keys into a dictionary. Instead of reading from the root to the leaf, we work backwards to find the path from leaf to root. This makes finding the right parent easy – track the indent level and go up one line skipping lines with an indent level &gt;= the current level. These paths have redundant information (shared parents) but represent what is needed.\n\n# Figuring out how to process BRITE entries\nkegg_gene_entry = kegg_gene_entries[0]\nsection_entry = kegg_gene_entry['BRITE']\n\nsection_entry\n\n['KEGG Orthology (KO) [BR:zma00001]',\n ' 09180 Brite Hierarchies',\n '  09183 Protein families: signaling and cellular processes',\n '   02000 Transporters [BR:zma02000]',\n '    100283122',\n 'Transporters [BR:zma02000]',\n ' Solute carrier family (SLC)',\n '  SLC50: Sugar efflux transporter',\n '   100283122']\n\n\n\n# get the leading whitespace for each line\nindent_spaces = [re.findall('^ +', e)[0] if re.match('^ ', e) else [''][0] for e in section_entry]\nindent_spaces = [len(e) for e  in indent_spaces]\n\nlen(indent_spaces), indent_spaces\n\n(9, [0, 1, 2, 3, 4, 0, 1, 2, 3])\n\n\nBecause the lists are ordered, given a position one can start at a leaf and work back up to the root by walking backwards and looking for the next point at which the indent level is the expected level. IF it’s true that each of the leaves contain only an identifer number and no letters then the process is to 1. Find all leaves 1. Walk backwards from all leaves 1. Add a list of lists with these paths (could be used in a tidy format)\nBelow the process of building up the path from leaf to root and the indents at each step (as a sanity check) are shown.\n\ni = 4 # position in list\nindent = indent_spaces[i] # indent at that position\n\nj = i\ncurrent_indent = indent\n# work backawards to get the paths\nj_backtrack      = [j]\nindent_backtrack = [current_indent]\n\nwhile indent_backtrack[-1] &gt; 0:\n    while current_indent != indent_backtrack[-1]-1:\n        j = j-1\n        current_indent = indent_spaces[j]\n\n    j_backtrack.extend([j])\n    indent_backtrack.extend([current_indent])\n\n    # indent\n    print(j_backtrack, indent_backtrack) #\n\n[4, 3] [4, 3]\n[4, 3, 2] [4, 3, 2]\n[4, 3, 2, 1] [4, 3, 2, 1]\n[4, 3, 2, 1, 0] [4, 3, 2, 1, 0]\n\n\nWrap this in a function, then integrate into a processing function for BRITE\n\ndef _indent_backtrack_path(i, # position in list \n                           indent_spaces                                        \n                          ):\n    indent = indent_spaces[i] # indent at that position\n    j = i\n    current_indent = indent\n    # work backawards to get the paths\n    j_backtrack      = [j]\n    indent_backtrack = [current_indent]\n\n    while indent_backtrack[-1] &gt; 0:\n        while current_indent != indent_backtrack[-1]-1:\n            j = j-1\n            current_indent = indent_spaces[j]\n\n        j_backtrack.extend([j])\n        indent_backtrack.extend([current_indent])\n        \n    # indent_backtrack is not needed beyond debugging\n    # confirm that the indent only decreases as you walk through the backtrack\n    indent_check = indent_backtrack[1:]+[-1]\n    indent_check =  [True if indent_backtrack[i] &gt; indent_check[i] else False for i in range(len(indent_backtrack))]\n    assert False not in indent_check\n    \n    return(j_backtrack)\n\n\ndef _gene_entry_BRITE(section_entry):\n    # get the leading whitespace for each line\n    indent_spaces = [re.findall('^ +', e)[0] if re.match('^ ', e) else [''][0] for e in section_entry]\n    indent_spaces = [len(e) for e  in indent_spaces]\n\n    # find the leaves\n    # ['KEGG Orthology (KO) [BR:zma00001]',\n    #  ' 09120 Genetic Information Processing',\n    #  '  09121 Transcription',\n    #  '   03020 RNA polymerase',\n    #  '    100037782' &lt;------------- Whitespace followed by digits and no letters\n    leaf_idxs = [i for i in range(len(section_entry)) if re.match('^\\s+\\d+$', section_entry[i])]\n\n    # for each leaf get the backtrack path to the root\n    leaf_backtraces = [_indent_backtrack_path(i = leaf_idx, \n                                              indent_spaces = indent_spaces\n                                             ) for leaf_idx in leaf_idxs]\n    # reverse the inner lists\n    [e.reverse()  for e in leaf_backtraces]\n\n    out = [[section_entry[i] for i in leaf_backtraces[j]] for j in range(len(leaf_backtraces))]\n    # sample input\n    # ['KEGG Orthology (KO) [BR:zma00001]',\n    #  ' 09120 Genetic Information Processing',\n    #  '  09121 Transcription',\n    #  '   03020 RNA polymerase',\n    #  '    100037782',\n    #  ' 09180 Brite Hierarchies',\n    #  '  09182 Protein families: genetic information processing',\n    #  '   03021 Transcription machinery [BR:zma03021]',\n    #  '    100037782',\n    #  'Enzymes [BR:zma01000]',\n    #  ' 2. Transferases',\n    #  '  2.7  Transferring phosphorus-containing groups',\n    #  '   2.7.7  Nucleotidyltransferases',\n    #  '    2.7.7.6  DNA-directed RNA polymerase',\n    #  '     100037782',\n    #  'Transcription machinery [BR:zma03021]',\n    #  ' Eukaryotic type',\n    #  '  RNA polymerase II system',\n    #  '   RNA polymerase II',\n    #  '    Pol IV and V specific subunits',\n    #  '     100037782']\n\n    # Sample output\n    # [['KEGG Orthology (KO) [BR:zma00001]',\n    #   ' 09120 Genetic Information Processing',\n    #   '  09121 Transcription',\n    #   '   03020 RNA polymerase',\n    #   '    100037782'],\n    #  ['KEGG Orthology (KO) [BR:zma00001]',\n    #   ' 09180 Brite Hierarchies',\n    #   '  09182 Protein families: genetic information processing',\n    #   '   03021 Transcription machinery [BR:zma03021]',\n    #   '    100037782'],\n    #  ['Enzymes [BR:zma01000]',\n    #   ' 2. Transferases',\n    #   '  2.7  Transferring phosphorus-containing groups',\n    #   '   2.7.7  Nucleotidyltransferases',\n    #   '    2.7.7.6  DNA-directed RNA polymerase',\n    #   '     100037782'],\n    #  ['Transcription machinery [BR:zma03021]',\n    #   ' Eukaryotic type',\n    #   '  RNA polymerase II system',\n    #   '   RNA polymerase II',\n    #   '    Pol IV and V specific subunits',\n    #   '     100037782']]\n\n    # and then strip whitespace\n    out = [[ee.strip() for ee in e] for e in out]\n    return(out)\n\n\ndef parse_kegg_gene_entry(kegg_gene_entry):\n    for section in kegg_gene_entry.keys():\n        if type(kegg_gene_entry[section]) != list :\n            # This is a safeguard to prevent the code from breaking when rerun. All entries start out as list\n            # so if an entry isn't a list then it's already been transformed\n            pass\n\n        else:\n            if section in ['TEMPLATE']:\n                kegg_gene_entry[section] = kegg_gene_entry[section]\n\n            elif section in ['ENTRY', 'NAME', 'ORTHOLOGY', 'ORGANISM', 'POSITION', 'SYMBOL', 'STRUCTURE']:\n                kegg_gene_entry[section] = kegg_gene_entry[section][0]\n\n            elif section in ['PATHWAY', 'MODULE']: \n                # NOTE: PATHWAY contains two spaces between the identifier and name which is why I'm not splitting on the first instance of whitespace.\n                # 'zma00591  Linoleic acid metabolism'\n                #          ^^        ^    ^\n                kegg_gene_entry[section] = dict([e.split('  ') for e in kegg_gene_entry[section]])            \n\n            elif section in ['BRITE']:\n                # This dict is only expected to have a single key. Representing these data as a dict allows for the \n                # list checking logic above to still work.\n                kegg_gene_entry[section] = {\"BRITE_PATHS\": _gene_entry_BRITE(section_entry = kegg_gene_entry[section])}\n\n#             elif section in ['MODULE']:\n#                 pass\n#                 kegg_gene_entry[section] = _gene_entry_flat_list(section_entry=kegg_gene_entry[section])\n                \n            elif section in ['MOTIF', 'DBLINKS']:\n                kegg_gene_entry[section] = _gene_entry_flat_list(section_entry=kegg_gene_entry[section])\n\n            elif section in ['AASEQ', 'NTSEQ']:\n                kegg_gene_entry[section] = _gene_seq_to_dict(seq_list = kegg_gene_entry[section])\n\n            else:\n                print(\"No behavior defined for \"+section)\n\n    return(kegg_gene_entry)\n\n\nparsed_kegg_gene_entries = [parse_kegg_gene_entry(kegg_gene_entry = kegg_gene_entry\n#                                                  ) for kegg_gene_entry in tqdm.tqdm(kegg_gene_entries)]\n                                                 ) for kegg_gene_entry in kegg_gene_entries]"
  },
  {
    "objectID": "zma_kegg_download.html#write-out-cleaned-data",
    "href": "zma_kegg_download.html#write-out-cleaned-data",
    "title": "SNPS to Networks 1: KEGG",
    "section": "Write out cleaned data",
    "text": "Write out cleaned data\n\nsave_dir = '../data/zma/kegg/'\n\nensure_dir_path_exists(dir_path = '../data/zma/kegg/')\n\n\nwith open(save_dir+'kegg_gene_entries.pkl', 'wb') as handle:\n    pkl.dump(parsed_kegg_gene_entries, \n             handle, \n             protocol=pkl.HIGHEST_PROTOCOL)\n    \n# Reading in data\n# with open('./data/kegg_gene_entries.pkl', 'rb') as handle:\n#     kegg_gene_entries = pkl.load(handle)"
  },
  {
    "objectID": "zma_kegg_download.html#visualize-kegg-data",
    "href": "zma_kegg_download.html#visualize-kegg-data",
    "title": "SNPS to Networks 1: KEGG",
    "section": "Visualize KEGG data",
    "text": "Visualize KEGG data\n\n# Restrict to only those with pathway\nkegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n\ntqdm.std.tqdm\n\n\n\ni = 0\nis_list = []\nfor i in tqdm(range(len(kegg_gene_brite))):\n    js_list = []\n    for j in range(len(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'])): \n        entries_path = kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][j]\n        if entries_path != []:\n        \n            temp = pd.DataFrame(entries_path)\n            temp = temp.T\n            temp['ENTRY'] = kegg_gene_brite[i]['ENTRY']+'_'+str(j)\n            \n            js_list = js_list + [temp]\n            \n    if js_list != []:\n        is_list = is_list + [pd.concat(js_list)]\n    \n\n# is_list\n\nBRITE_df = pd.concat(is_list)\n\nBRITE_df = BRITE_df.drop_duplicates().reset_index().drop(columns = 'index')\nBRITE_df\n\n100%|████████████████████████████████████████████████████████████████████████████| 11835/11835 [00:25&lt;00:00, 456.36it/s]\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\nENTRY\n5\n6\n\n\n\n\n0\nKEGG Orthology (KO) [BR:zma00001]\n09180 Brite Hierarchies\n09183 Protein families: signaling and cellular...\n02000 Transporters [BR:zma02000]\n100283122\n100283122 CDS T01088_0\nNaN\nNaN\n\n\n1\nTransporters [BR:zma02000]\nSolute carrier family (SLC)\nSLC50: Sugar efflux transporter\n100283122\nNaN\n100283122 CDS T01088_1\nNaN\nNaN\n\n\n2\nKEGG Orthology (KO) [BR:zma00001]\n09180 Brite Hierarchies\n09182 Protein families: genetic information pr...\n03019 Messenger RNA biogenesis [BR:zma03019]\n100384259\n100384259 CDS T01088_0\nNaN\nNaN\n\n\n3\nKEGG Orthology (KO) [BR:zma00001]\n09180 Brite Hierarchies\n09182 Protein families: genetic information pr...\n03036 Chromosome and associated proteins [BR:z...\n100384259\n100384259 CDS T01088_1\nNaN\nNaN\n\n\n4\nMessenger RNA biogenesis [BR:zma03019]\nEukaryotic type\nmRNA surveillance and transport factors\nmRNA cycle factors\nCommon to processing body (P body) and stress ...\n100384259 CDS T01088_2\n100384259\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n40927\nBacterial toxins [BR:zma02042]\nType II toxins: Membrane damaging toxins\nToxins that enzymatically damage the membrane\n100384643\nNaN\n100384643 CDS T01088_5\nNaN\nNaN\n\n\n40928\nKEGG Orthology (KO) [BR:zma00001]\n09180 Brite Hierarchies\n09183 Protein families: signaling and cellular...\n02000 Transporters [BR:zma02000]\n100384530\n100384530 CDS T01088_0\nNaN\nNaN\n\n\n40929\nTransporters [BR:zma02000]\nOther transporters\nElectrochemical potential-driven transporters ...\n100384530\nNaN\n100384530 CDS T01088_1\nNaN\nNaN\n\n\n40930\nKEGG Orthology (KO) [BR:zma00001]\n09100 Metabolism\n09105 Amino acid metabolism\n00400 Phenylalanine, tyrosine and tryptophan b...\n100284499\n100284499 CDS T01088_0\nNaN\nNaN\n\n\n40931\nEnzymes [BR:zma01000]\n2. Transferases\n2.5 Transferring alkyl or aryl groups, other ...\n2.5.1 Transferring alkyl or aryl groups, othe...\n2.5.1.54 3-deoxy-7-phosphoheptulonate synthase\n100284499 CDS T01088_1\n100284499\nNaN\n\n\n\n\n40932 rows × 8 columns\n\n\n\n\n#TODO find out what is causing levels deeper than 3 or the indices below (and others) to fail\nfig = px.treemap(BRITE_df.loc[:\n#     (\n#     (BRITE_df.index != 7480) &\n#     (BRITE_df.index != 8131) &\n#     (BRITE_df.index != 8838) &\n#     (BRITE_df.index != 8837) &\n#     (BRITE_df.index != 8836)\n# )\n    , :], path=[px.Constant(\"all\"), 0, 1, 2, 3, #4, 5, 6\n            ], \n#                  values='a'\n                )\nfig.update_traces(root_color=\"lightgrey\")\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep.html",
    "href": "tianetal2011_model_g2pdeep.html",
    "title": "Tian et al. 2011 Model 3 (G2PDeep Inspired)",
    "section": "",
    "text": "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n#                          Hours to wait\n# import time; time.sleep( 18 * (60*60))\n# Run Settings:\nnb_name = '13_TianEtAl2011'# Set manually! -----------------------------------\n\ndownsample_obs = False\ntrain_n = 90\ntest_n = 10\n\ndataloader_batch_size = 8 #16 #64\nrun_epochs = 200\n\nuse_gpu_num = 1\n\n# Imports --------------------------------------------------------------------\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nimport tqdm\nfrom tqdm import tqdm\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n\nimport dlgwas\nfrom dlgwas.kegg import ensure_dir_path_exists\nfrom dlgwas.kegg import get_cached_result\nfrom dlgwas.kegg import put_cached_result\n\nfrom dlgwas.dlfn import calc_cs\nfrom dlgwas.dlfn import apply_cs\nfrom dlgwas.dlfn import reverse_cs\n\nfrom dlgwas.dlfn import TianEtAl2011Dataset\nfrom dlgwas.dlfn import train_loop\nfrom dlgwas.dlfn import train_error\nfrom dlgwas.dlfn import test_loop\nfrom dlgwas.dlfn import train_nn\nfrom dlgwas.dlfn import yhat_loop\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nUsing cuda device\n# use_gpu_num = 0 # This should change based on whichever gpu is free. \n#                 # If even notebooks are set to 0 then that will be a reasonable default. \n\n# import os\n# import pandas as pd\n# import numpy as np\n# import re\n\n# import torch\n# from torch.utils.data import Dataset\n# from torch.utils.data import DataLoader\n# from torch import nn\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# if use_gpu_num in [0, 1]: \n#     torch.cuda.set_device(use_gpu_num)\n# print(f\"Using {device} device\")\n# # # FIXME\n# # device = 'cpu'\n# import tqdm\n# from tqdm import tqdm\n\n# import plotly.graph_objects as go\n# import plotly.express as px\n# import plotly.io as pio\n# pio.templates.default = \"plotly_white\"\n\n\n# import dlgwas\n# from dlgwas.kegg import ensure_dir_path_exists\n# from dlgwas.kegg import get_cached_result\n# from dlgwas.kegg import put_cached_result\n\n# from dlgwas.dlfn import calc_cs\n# from dlgwas.dlfn import apply_cs\n# from dlgwas.dlfn import reverse_cs\n# # automatically extract notebook name so that I don't have to worrry about forgetting to change this.\n# nb_full_path = os.path.join(os.getcwd(), nb_name)\n# # set up directory for notebook artifacts\n# nb_name = nb_full_path.split('/')[-1]\n# ensure_dir_path_exists(dir_path = '../models/'+nb_name)\n# ensure_dir_path_exists(dir_path = '../reports/'+nb_name)"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep.html#load-cleaned-data",
    "href": "tianetal2011_model_g2pdeep.html#load-cleaned-data",
    "title": "Tian et al. 2011 Model 3 (G2PDeep Inspired)",
    "section": "Load Cleaned Data",
    "text": "Load Cleaned Data\n\n# Read in cleaned data\ntaxa_groupings = pd.read_csv('../models/10_TianEtAl2011/taxa_groupings.csv')\ndata           = pd.read_csv('../models/10_TianEtAl2011/clean_data.csv')\n\n# Define holdout sets (Populations)\nuniq_pop = list(set(taxa_groupings['Population']))\nprint(str(len(uniq_pop))+\" Unique Holdout Groups.\")\ntaxa_groupings['Holdout'] = None\nfor i in range(len(uniq_pop)):\n    mask = (taxa_groupings['Population'] == uniq_pop[i])\n    taxa_groupings.loc[mask, 'Holdout'] = i\n\ntaxa_groupings\n\n25 Unique Holdout Groups.\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nsample\nPopulation\nHoldout\n\n\n\n\n0\n0\nZ001E0001\nB73 x B97\n21\n\n\n1\n1\nZ001E0002\nB73 x B97\n21\n\n\n2\n2\nZ001E0003\nB73 x B97\n21\n\n\n3\n3\nZ001E0004\nB73 x B97\n21\n\n\n4\n4\nZ001E0005\nB73 x B97\n21\n\n\n...\n...\n...\n...\n...\n\n\n4671\n4671\nZ026E0196\nB73 x Tzi8\n7\n\n\n4672\n4672\nZ026E0197\nB73 x Tzi8\n7\n\n\n4673\n4673\nZ026E0198\nB73 x Tzi8\n7\n\n\n4674\n4674\nZ026E0199\nB73 x Tzi8\n7\n\n\n4675\n4675\nZ026E0200\nB73 x Tzi8\n7\n\n\n\n\n4676 rows × 4 columns\n\n\n\n\n# # Read in cleaned data\n# taxa_groupings = pd.read_csv('../models/10_TianEtAl2011/taxa_groupings.csv')\n# data           = pd.read_csv('../models/10_TianEtAl2011/clean_data.csv')\n\n\n# # Define holdout sets (Populations)\n# uniq_pop = list(set(taxa_groupings['Population']))\n# print(str(len(uniq_pop))+\" Unique Holdout Groups.\")\n# taxa_groupings['Holdout'] = None\n# for i in range(len(uniq_pop)):\n#     mask = (taxa_groupings['Population'] == uniq_pop[i])\n#     taxa_groupings.loc[mask, 'Holdout'] = i\n\n# taxa_groupings"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep.html#setup-holdouts",
    "href": "tianetal2011_model_g2pdeep.html#setup-holdouts",
    "title": "Tian et al. 2011 Model 3 (G2PDeep Inspired)",
    "section": "Setup Holdouts",
    "text": "Setup Holdouts\n\n#randomly holdout a population if there is not a file with the population held out.\n# Holdout_Int = 0\nHoldout_Int_path = '../models/'+nb_name+'/holdout_pop_int.pkl'\nif None != get_cached_result(Holdout_Int_path):\n    Holdout_Int = get_cached_result(Holdout_Int_path)\nelse:\n    Holdout_Int = int(np.random.choice([i for i in range(len(uniq_pop))], 1))\n    put_cached_result(Holdout_Int_path, Holdout_Int)\n\n    \nprint(\"Holding out i=\"+str(Holdout_Int)+\": \"+uniq_pop[Holdout_Int])\n\nmask = (taxa_groupings['Holdout'] == Holdout_Int)\ntrain_idxs = list(taxa_groupings.loc[~mask, ].index)\ntest_idxs = list(taxa_groupings.loc[mask, ].index)\n\nHolding out i=16: B73 x Ki11\n\n\n\n# downsample_obs = True\n# train_n = 900\n# test_n = 100\n\nif downsample_obs == True:\n    train_idxs = np.random.choice(train_idxs, train_n)\n    test_idxs = np.random.choice(test_idxs, test_n)\n    print([len(e) for e in [test_idxs, train_idxs]])\n    \n# used to go from index in tensor to index in data so that the right xs tensor can be loaded in\nidx_original = np.array(data.index)\n\ny1 = data['leaf_length']\ny2 = data['leaf_width']\ny3 = data['upper_leaf_angle']\ny1 = np.array(y1)\ny2 = np.array(y2)\ny3 = np.array(y3)\n\n\n# #randomly holdout a population if there is not a file with the population held out.\n# # Holdout_Int = 0\n# Holdout_Int_path = '../models/'+nb_name+'/holdout_pop_int.pkl'\n# if None != get_cached_result(Holdout_Int_path):\n#     Holdout_Int = get_cached_result(Holdout_Int_path)\n# else:\n#     Holdout_Int = int(np.random.choice([i for i in range(len(uniq_pop))], 1))\n#     put_cached_result(Holdout_Int_path, Holdout_Int)\n\n\n# print(\"Holding out i=\"+str(Holdout_Int)+\": \"+uniq_pop[Holdout_Int])\n\n# mask = (taxa_groupings['Holdout'] == Holdout_Int)\n# train_idxs = list(taxa_groupings.loc[~mask, ].index)\n# test_idxs = list(taxa_groupings.loc[mask, ].index)\n\n\n# # used to go from index in tensor to index in data so that the right xs tensor can be loaded in\n# idx_original = np.array(data.index)\n\n# y1 = data['leaf_length']\n# y2 = data['leaf_width']\n# y3 = data['upper_leaf_angle']\n# y1 = np.array(y1)\n# y2 = np.array(y2)\n# y3 = np.array(y3)\n\n\nScale data\n\nscale_dict_path = '../models/'+nb_name+'/scale_dict.pkl'\nif None != get_cached_result(scale_dict_path):\n    scale_dict = get_cached_result(scale_dict_path)\nelse:\n    scale_dict = {\n        'y1':calc_cs(y1[train_idxs]),\n        'y2':calc_cs(y2[train_idxs]),\n        'y3':calc_cs(y3[train_idxs])\n    }\n    put_cached_result(scale_dict_path, scale_dict)\n\ny1 = apply_cs(y1, scale_dict['y1'])\ny2 = apply_cs(y2, scale_dict['y2'])\ny3 = apply_cs(y3, scale_dict['y3'])\n\n\n# scale_dict = {\n#     'y1':calc_cs(y1[train_idxs]),\n#     'y2':calc_cs(y2[train_idxs]),\n#     'y3':calc_cs(y3[train_idxs])\n# }\n\n\n# y1 = apply_cs(y1, scale_dict['y1'])\n# y2 = apply_cs(y2, scale_dict['y2'])\n# y3 = apply_cs(y3, scale_dict['y3'])"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "href": "tianetal2011_model_g2pdeep.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "title": "Tian et al. 2011 Model 3 (G2PDeep Inspired)",
    "section": "Allow for cycling data onto and off of GPU",
    "text": "Allow for cycling data onto and off of GPU\n\n# loading this into memory causes the session to crash\n\ny1_train = torch.from_numpy(y1[train_idxs])[:, None]\ny2_train = torch.from_numpy(y2[train_idxs])[:, None]\ny3_train = torch.from_numpy(y3[train_idxs])[:, None]\n\nidx_original_train = torch.from_numpy(idx_original[train_idxs])\n\ny1_test = torch.from_numpy(y1[test_idxs])[:, None]\ny2_test = torch.from_numpy(y2[test_idxs])[:, None]\ny3_test = torch.from_numpy(y3[test_idxs])[:, None]\n\nidx_original_test = torch.from_numpy(idx_original[test_idxs])\n\n\n# dataloader_batch_size = 64\n\ntraining_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_train,\n        y2 = y2_train,\n        y3 = y3_train,\n        idx_original = idx_original_train,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\ntesting_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_test,\n        y2 = y2_test,\n        y3 = y3_test,\n        idx_original = idx_original_test,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\nUsing cuda device\nUsing cuda device\n\n\n\n# # loading this into memory causes the session to crash\n\n# y1_train = torch.from_numpy(y1[train_idxs])[:, None]\n# y2_train = torch.from_numpy(y2[train_idxs])[:, None]\n# y3_train = torch.from_numpy(y3[train_idxs])[:, None]\n\n# idx_original_train = torch.from_numpy(idx_original[train_idxs])\n\n# y1_test = torch.from_numpy(y1[test_idxs])[:, None]\n# y2_test = torch.from_numpy(y2[test_idxs])[:, None]\n# y3_test = torch.from_numpy(y3[test_idxs])[:, None]\n\n# idx_original_test = torch.from_numpy(idx_original[test_idxs])\n\n\n# class CustomDataset(Dataset):\n#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#     if use_gpu_num in [0, 1]: \n#         torch.cuda.set_device(use_gpu_num)\n#     print(f\"Using {device} device\")\n    \n#     def __init__(self, y1, y2, y3, \n#                  idx_original,\n#                  transform = None, target_transform = None, \n#                  **kwargs\n#                 ):\n#         self.y1 = y1\n#         self.y2 = y2\n#         self.y3 = y3\n#         self.idx_original = idx_original\n#         self.transform = transform\n#         self.target_transform = target_transform    \n    \n#     def __len__(self):\n#         return len(self.y1)\n    \n#     def __getitem__(self, idx):\n#         y1_idx = self.y1[idx].to(device).float()\n#         y2_idx = self.y2[idx].to(device).float()\n#         y3_idx = self.y3[idx].to(device).float()\n        \n        \n#         # Change type of xs loaded !! ----------------------------------------\n#         # load in xs as they are needed.\n#         # Non-Hilbert Version\n        \n        \n#         save_path = '../models/10_TianEtAl2011/markers/'\n#         # Hilbert version\n#         # save_path = '../models/'+nb_name+'/hilbert/'\n#         save_file_path = save_path+'m'+str(int(self.idx_original[idx]))+'.npz'\n#         xs_idx = np.load(save_file_path)['arr_0']\n#         xs_idx = torch.from_numpy(xs_idx).to(device).float()\n#         xs_idx = xs_idx.squeeze()\n        \n#         # to match pytorch's conventions channel must be in the second dim\n#         xs_idx = torch.swapaxes(xs_idx, 0, 1) \n        \n#         if self.transform:\n#             xs_idx = self.transform(xs_idx)\n            \n#         if self.target_transform:\n#             y1_idx = self.transform(y1_idx)\n#             y2_idx = self.transform(y2_idx)\n#             y3_idx = self.transform(y3_idx)\n#         return xs_idx, y1_idx, y2_idx, y3_idx\n\n\n# training_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_train,\n#         y2 = y2_train,\n#         y3 = y3_train,\n#         idx_original = idx_original_train\n#     ), \n#     batch_size = 8, \n#     shuffle = True)\n\n# testing_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_test,\n#         y2 = y2_test,\n#         y3 = y3_test,\n#         idx_original = idx_original_train\n#     ), \n#     batch_size = 8, \n#     shuffle = True)\n\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n\n\n# del training_dataloader\n\n\n# torch.cuda.empty_cache()\n\n\n# xs_i.shape"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep.html#non-boilerplate",
    "href": "tianetal2011_model_g2pdeep.html#non-boilerplate",
    "title": "Tian et al. 2011 Model 3 (G2PDeep Inspired)",
    "section": "Non-Boilerplate",
    "text": "Non-Boilerplate\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()    \n        \n        # Note: to get independent reguarlizations these layers must be split up. \n        # Since the inspiration model uses different l2s for the bias and the weights\n        # I'm using the weight one alone. To create layer specific weight decays the \n        # optimizer must be tweaked:\n        # optimizer = torch.optim.Adam([\n        #     {'params':model.input_processing_long_way_0.parameters(), 'weight_decay': 0.1},\n        #      # ... more dicts of params here.\n        # ], lr=learning_rate)\n        \n        self.input_processing_long_way = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 4, # second channel\n                    out_channels= 10,\n                    kernel_size= 4,\n                  # stride= 2\n                  # padding = 'same',\n                    bias = True\n                ),\n            nn.Conv1d(\n                    in_channels= 10, \n                    out_channels= 10,\n                    kernel_size= 20,\n                  padding = 'same',\n                    bias = True\n                ),\n            nn.Dropout(p=0.75)\n            )\n        # x = Conv1D(10, # filters (out channels)\n        #            4,  # kernel size\n        #            padding='same',\n        #            activation = 'linear',\n        #            kernel_initializer = 'TruncatedNormal', \n        #            kernel_regularizer=regularizers.l2(0.1),\n        #            bias_regularizer = regularizers.l2(0.01)\n        #           )(inputs)\n        # x = Conv1D(10,\n        #            20,\n        #            padding='same',activation = 'linear', \n        #            kernel_initializer = 'TruncatedNormal',\n        #            kernel_regularizer=regularizers.l2(0.1),\n        #            bias_regularizer = regularizers.l2(0.01))(x)\n        # x = Dropout(0.75)(x)\n        \n        self.input_processing_shortcut = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 4,\n                    out_channels= 10,\n                    kernel_size= 4,\n                    bias = True\n                )\n        )\n\n        self.feature_processing = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 10, \n                    out_channels= 10,\n                    kernel_size= 4,\n                    bias = True\n                ),\n            nn.Dropout(p=0.75)\n        )\n\n        self.output_processing = nn.Sequential(\n            nn.Flatten(),\n            nn.ReLU(), # They used inverse square root activation $y = \\frac{x}{\\sqrt{1+ax^2}}$\n            nn.Dropout(p=0.75),\n            nn.Linear(9434490, 1)\n        )            \n        \n    def forward(self, x):\n        x_out = self.input_processing_long_way(x)\n        \n        x_shortcut = self.input_processing_shortcut(x)\n\n        x_out += x_shortcut\n        \n        x_out = self.feature_processing(x_out)\n        x_out = self.output_processing(x_out)\n        \n        \n        return x_out\n\n\n# model = NeuralNetwork().to(device)\n\n# res = model(xs_i) # try prediction on one batch\n# res.shape\n# # torch.Size([64, 4, 943455])\n# # torch.Size([4, 10, 943452]) #&lt;- shortcut\n\n\n# if not os.path.exists('../models/'+nb_name+'/model.pt'): \n#     model = NeuralNetwork().to(device)\n#     # print(model)\n#     # model(xs_i).shape # try prediction on one batch\n\n\n# def train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n#     size = len(dataloader.dataset)\n#     for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n#         # Compute prediction and loss\n#         pred = model(xs_i)\n#         loss = loss_fn(pred, y1_i) # &lt;----------------------------------------\n\n#         # Backpropagation\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         if batch % 100 == 0:\n#             loss, current = loss.item(), batch * len(y1_i) # &lt;----------------\n#             if not silent:\n#                 print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n                \n# def train_error(dataloader, model, loss_fn, silent = False):\n#     size = len(dataloader.dataset)\n#     num_batches = len(dataloader)\n#     train_loss = 0\n\n#     with torch.no_grad():\n#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n#             pred = model(xs_i)\n#             train_loss += loss_fn(pred, y1_i).item() # &lt;----------------------\n            \n#     train_loss /= num_batches\n#     return(train_loss) \n\n            \n# def test_loop(dataloader, model, loss_fn, silent = False):\n#     size = len(dataloader.dataset)\n#     num_batches = len(dataloader)\n#     test_loss = 0\n\n#     with torch.no_grad():\n#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n#             pred = model(xs_i)\n#             test_loss += loss_fn(pred, y1_i).item() # &lt;-----------------------\n\n#     test_loss /= num_batches\n#     if not silent:\n#         print(f\"Test Error: Avg loss: {test_loss:&gt;8f}\")\n#     return(test_loss)\n\n\ndef train_nn(\n    nb_name,\n    training_dataloader,\n    testing_dataloader,\n    model,\n    learning_rate = 1e-3,\n    batch_size = 64,\n    epochs = 500\n):\n    # Initialize the loss function\n    loss_fn = nn.MSELoss()\n#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    # Optimizer with L2 normalization\n    optimizer = torch.optim.Adam([\n        {'params':model.input_processing_long_way.parameters(), 'weight_decay': 0.1},\n        {'params':model.input_processing_shortcut.parameters(), 'weight_decay': 0.1},\n        {'params':model.feature_processing.parameters(),        'weight_decay': 0.1},\n        {'params':model.output_processing.parameters(),         'weight_decay': 0.01},\n    ], lr=learning_rate)\n\n    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n    loss_df['TrainMSE'] = np.nan\n    loss_df['TestMSE']  = np.nan\n\n    for t in tqdm(range(epochs)):        \n#         print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n\n        loss_df.loc[loss_df.index == t, 'TrainMSE'\n                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n        \n        loss_df.loc[loss_df.index == t, 'TestMSE'\n                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n        \n        if (t+1)%10: # Cache in case training is interupted\n#             print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n            torch.save(model.state_dict(), \n                       '../models/'+nb_name+'/model_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n            loss_df.to_csv('../reports/'+nb_name+'/loss_df'+str(t)+'_'+str(epochs)+'.csv', index=False) \n        \n    return([model, loss_df])\n\n\n# don't run if either of these exist because there may be cases where we want the results but not the model\n\nif not os.path.exists('../models/'+nb_name+'/model.pt'): \n    # Shared setup (train from scratch and load latest)\n    model = NeuralNetwork()\n\n    # find the biggest model to save\n    saved_models = os.listdir('../models/'+nb_name+'/')\n    saved_models = [e for e in saved_models if re.match('model*', e)]\n\n    if saved_models == []:\n        epochs_run = 0\n    else:\n        # if there are saved models reload and resume training\n        saved_models_numbers = [int(e.replace('model_', ''\n                                    ).replace('.pt', ''\n                                    ).split('_')[0]) for e in saved_models]\n        # saved_models\n        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n        latest_model = [e for e in saved_models if re.match(\n            '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n        model.load_state_dict(torch.load('../models/'+nb_name+'/'+latest_model))\n        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n    \n    model.to(device)    \n\n    model, loss_df = train_nn(\n        nb_name,\n        training_dataloader,\n        testing_dataloader,\n        model,\n        learning_rate = 1e-3,\n        batch_size = dataloader_batch_size,\n        epochs = (run_epochs - epochs_run)\n    )\n    \n    # experimental outputs:\n    # 1. Model\n    torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n\n    # 2. loss_df\n    loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)  \n    \n    # 3. predictions \n    yhats = pd.concat([\n        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n\n    yhats.to_csv('../reports/'+nb_name+'/yhats.csv', index=False)\n\n\nNeuralNetwork()\n\nNeuralNetwork(\n  (input_processing_long_way): Sequential(\n    (0): Conv1d(4, 10, kernel_size=(4,), stride=(1,))\n    (1): Conv1d(10, 10, kernel_size=(20,), stride=(1,), padding=same)\n    (2): Dropout(p=0.75, inplace=False)\n  )\n  (input_processing_shortcut): Sequential(\n    (0): Conv1d(4, 10, kernel_size=(4,), stride=(1,))\n  )\n  (feature_processing): Sequential(\n    (0): Conv1d(10, 10, kernel_size=(4,), stride=(1,))\n    (1): Dropout(p=0.75, inplace=False)\n  )\n  (output_processing): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): ReLU()\n    (2): Dropout(p=0.75, inplace=False)\n    (3): Linear(in_features=9434490, out_features=1, bias=True)\n  )\n)"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep.html#standard-visualizations",
    "href": "tianetal2011_model_g2pdeep.html#standard-visualizations",
    "title": "Tian et al. 2011 Model 3 (G2PDeep Inspired)",
    "section": "Standard Visualizations",
    "text": "Standard Visualizations\n\nloss_df = pd.read_csv('../reports/'+nb_name+'/loss_df.csv')\n\nloss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\nloss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n                    mode='lines', name='Test'))\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n                    mode='lines', name='Train'))\nfig.show()\n\n\n                                                \n\n\n\nyhats = pd.read_csv('../reports/'+nb_name+'/yhats.csv')\n\nyhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\nyhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n\npx.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")\n\n\n                                                \n\n\n\nyhats['Error'] = yhats.y_true - yhats.y_pred\n\npx.histogram(yhats, x = 'Error', color = 'Split',\n             marginal=\"box\", # can be `rug`, `violin`\n             nbins= 50)\n\n\n                                                \n\n\n\n# # don't run if either of these exist because there may be cases where we want the results but not the model\n\n# if not os.path.exists('../models/'+nb_name+'/model.pt'): \n\n#     model, loss_df = train_nn(\n#         training_dataloader,\n#         testing_dataloader,\n#         model,\n#         learning_rate = 1e-3,\n#         batch_size = 64,\n#         epochs = 200\n#     )\n    \n#     # experimental outputs:\n#     # 1. Model\n#     torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n\n#     # 2. loss_df\n#     loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)  \n    \n# # 200/200 [19:20:18&lt;00:00, 348.09s/it]\n\n\n# loss_df = pd.read_csv('../reports/'+nb_name+'/loss_df.csv')\n\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n#                     mode='lines', name='Train'))\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n#                     mode='lines', name='Test'))\n# fig.show()\n\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, \n#                          y= reverse_cs(loss_df.TrainMSE, \n#                                        scale_dict['y1']),\n#                          mode='lines', name='Train'))\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, \n#                          y= reverse_cs(loss_df.TestMSE, \n#                                        scale_dict['y1']),\n#                          mode='lines', name='Test'))\n# fig.show()\n\n\n# # run on cpu -----\n# device = 'cpu'\n\n# training_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_train,\n#         y2 = y2_train,\n#         y3 = y3_train,\n#         idx_original = idx_original_train\n#     ), \n#     batch_size = 64, \n#     shuffle = True)\n\n# testing_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_test,\n#         y2 = y2_test,\n#         y3 = y3_test,\n#         idx_original = idx_original_train\n#     ), \n#     batch_size = 64, \n#     shuffle = True)\n\n# # If the model had to be trained from scratch, loading it in will not overwrite it. GPU will run out of memory.\n# # Option 1\n#     # Remove and reload the model\n#     # del model\n#     # torch.cuda.empty_cache()\n# # Option 2\n#     # Only load model if it's not in the scope already\n#     # model = NeuralNetwork()\n#     # model.load_state_dict(torch.load('../models/'+nb_name+'/model.pt'))\n\n# model_exists = 'model' in locals() or 'model' in globals()\n\n# if not model_exists:\n#     model = NeuralNetwork()\n#     model.load_state_dict(torch.load('../models/'+nb_name+'/model.pt'))\n#     model.to(device)\n\n\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n\n# model(xs_i)\n\n# # del xs_i, y1_i, y2_i, y3_i \n\n# # del model\n\n# torch.cuda.empty_cache()\n\n# def yhat_loop(dataloader, model):\n#     size = len(dataloader.dataset)\n#     num_batches = len(dataloader)\n    \n#     y_true = np.array([])\n#     y_pred = np.array([])\n    \n#     with torch.no_grad():\n#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n#             yhat_i = model(xs_i)\n#             y_i = y1_i # &lt;-----------------------\n# #             pdb.set_trace()\n#             y_true = np.append(y_true, np.array(yhat_i.cpu()))\n#             y_pred = np.append(y_pred, np.array(y_i.cpu()))\n    \n#     out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) \n#     out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])\n#     return(out)\n\n# out = yhat_loop(testing_dataloader, model)\n\n# # reverse_cs(out.y_true, scale_dict['y1'])\n# px.scatter(out, x = 'y_true', y = 'y_pred')\n\n# px.histogram(x = out.y_true - out.y_pred, nbins= 50)\n\n# data\n\n\n# automatically kill kernel after running. \n# This is a hacky way to free up _all_ space on the gpus\n# os._exit(00)"
  },
  {
    "objectID": "zma_trial.html",
    "href": "zma_trial.html",
    "title": "Retrieve genomic data for phenotypes",
    "section": "",
    "text": "import os\n\nimport numpy as np\nimport pandas as pd\n\nimport plotly.express as px\n\n# import hilbertcurve\n# from hilbertcurve.hilbertcurve import HilbertCurve\nimport tqdm\nfrom tqdm import tqdm\n\nimport dlgwas\nfrom dlgwas.dna import *\n\n# ! conda install openpyxl -y\n# ! conda install hilbertcurve -y\n# ! pip install hilbertcurve"
  },
  {
    "objectID": "zma_trial.html#load-phenotypic-data-to-explore",
    "href": "zma_trial.html#load-phenotypic-data-to-explore",
    "title": "Retrieve genomic data for phenotypes",
    "section": "Load phenotypic data to explore",
    "text": "Load phenotypic data to explore\nFor this I’m using data referenced in Wallace et al 2014 which is available through panzea. This study refers to data from 9 studies (including itself) as a source of phenotypes for the NAM data. This combination of a large set of published GWAS hits, phenotypes, and many rils makes it ideal for use here.\nThis file contains results I can use to check if my approaches are producing similar hits.\n\nWallace_etal_2014_PLoSGenet_GWAS_hits = pd.read_table('../ext_data/zma/panzea/GWASResults/Wallace_etal_2014_PLoSGenet_GWAS_hits-150112.txt')\nWallace_etal_2014_PLoSGenet_GWAS_hits.head()\n\n\n\n\n\n\n\n\ntrait\nchr\npos\nallele\nrmip\nsource\n\n\n\n\n0\n100 Kernel weight\n1\n3364007\nA/G\n1\nHapmap1\n\n\n1\n100 Kernel weight\n1\n22247033\nA/G\n3\nHapmap1\n\n\n2\n100 Kernel weight\n1\n22987420\nC/T\n1\nHapmap2\n\n\n3\n100 Kernel weight\n1\n23056483\nC/G\n1\nHapmap2\n\n\n4\n100 Kernel weight\n1\n23066099\nA/G\n1\nHapmap2\n\n\n\n\n\n\n\nThis file on I think matches Buckler et al 2009.\n\ntemp = pd.read_excel('../ext_data/zma/panzea/GWASResults/JointGLMModels090324QTLLocations.xlsx', \n                     skiprows=1\n                    ).rename(columns = {\n    'Unnamed: 1': 'ASI', \n    'Unnamed: 2': 'Days to Anthesis', \n    'Unnamed: 3': 'Days to Silk'\n})\ntemp = temp.loc[temp.index == 0]\ntemp.head()\n\n/home/labmember/mambaforge/envs/pytorch_mamba/lib/python3.10/site-packages/openpyxl/worksheet/_read_only.py:79: UserWarning: Unknown extension is not supported and will be removed\n  for idx, row in parser.parse():\n\n\n\n\n\n\n\n\n\nHeritability of line BLUPs across all crosses (excluding association panel):\nASI\nDays to Anthesis\nDays to Silk\n\n\n\n\n0\nHeritability of line BLUPs\n0.775037\n0.944619\n0.942365\n\n\n\n\n\n\n\n\ntemp = pd.read_excel('../ext_data/zma/panzea/GWASResults/JointGLMModels090324QTLLocations.xlsx', \n                     skiprows=4\n                    ).rename(columns = {\n    'Unnamed: 1': 'ASI', \n    'Unnamed: 2': 'Days to Anthesis', \n    'Unnamed: 3': 'Days to Silk'\n})\ntemp.head()\n\n/home/labmember/mambaforge/envs/pytorch_mamba/lib/python3.10/site-packages/openpyxl/worksheet/_read_only.py:79: UserWarning: Unknown extension is not supported and will be removed\n  for idx, row in parser.parse():\n\n\n\n\n\n\n\n\n\nHeritability of line BLUPs within each cross:\nASI\nDays to Anthesis\nDays to Silk\n\n\n\n\n0\nB73×B97\n0.64151\n0.770513\n0.766082\n\n\n1\nB73×CML103\n0.613736\n0.777381\n0.734942\n\n\n2\nB73×CML228\n0.680939\n0.903684\n0.892442\n\n\n3\nB73×CML247\n0.748584\n0.900707\n0.894291\n\n\n4\nB73×CML277\n0.708526\n0.918574\n0.911493\n\n\n\n\n\n\n\n\n# pull in some of the data that Wallace et al 2014 point to:\n\nbuckler_2009_path = '../ext_data/zma/panzea/phenotypes/Buckler_etal_2009_Science_flowering_time_data-090807/'\nos.listdir(buckler_2009_path)\n\n['NAM_DaysToTassel.txt',\n 'NAM_TasselingDate.txt',\n 'markergenotypes062508.txt',\n 'NAM_SilkingDate.txt',\n 'NAMSum0607FloweringTraitBLUPsAcross8Envs.txt',\n 'markers061208.txt',\n 'NAM_DaysToSilk.txt']\n\n\n\nnam_dts = pd.read_table(buckler_2009_path+'NAM_DaysToSilk.txt', encoding=\"ISO-8859-1\")\nnam_dts_taxa = list(nam_dts.loc[:, 'accename'].drop_duplicates())\n\n\n# Look for the right taxa\n\n\n# genome_files.sort()\n# genome_files[0:10]\n\n\n# # I think we need to match everything before the __\n# def find_AGPv4_genome(\n#     taxa, # should be the desired taxa or a regex fragment (stopping before the __). E.g. 'B73' or 'B\\d+'\n#     **kwargs # optionally pass in a genome list (this allows for a different path or precomputing if we're finding a lot of genomes)\n#     ):\n#     if 'genome_files' not in kwargs.keys():\n#         import os\n#         genome_files = os.listdir(\n#     '../data/zma/panzea/genotypes/GBS/v27/ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_Table/')\n#     else:\n#         genome_files = kwargs['genome_files']\n#     import re\n#     return( [e for e in genome_files if re.match(taxa+'__.+', e)] )\n\n\ngenome_files = os.listdir(\n    '../data/zma/panzea/genotypes/GBS/v27/ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_Table/')\n\n\npossible_matches = [{'taxa': e,\n                     'matches': find_AGPv4(\n                         taxa = e,\n                         genome_files = genome_files\n)} for e in tqdm(nam_dts_taxa)]\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5369/5369 [01:25&lt;00:00, 63.16it/s]\n\n\n\n# how many have more than one match?\nlen(\n[[len(e['matches']), e] for e in possible_matches if len(e['matches']) != 1])\n\n1297\n\n\n\n'Z018E0021'\n\n\n\n\n# ith_taxa = '05-397:250007467'\n# res = get_AGPv4(taxa_to_filename(taxa = ith_taxa))   # Retrieve record\n# temp = AGPv4_site.loc[:, ['Chromosome', 'Position']]  \n# temp[res[0]] = res[1:]                               # Add Col. with Nucleotides\n# temp.head()\n\n'Z018E0021'\n\n\n\npd.read_table(buckler_2009_path+'NAMSum0607FloweringTraitBLUPsAcross8Envs.txt', encoding=\"ISO-8859-1\")\n\n\n\n\n\n\n\n\nGeno_Code\nEntry_ID\nGroup\npop\nentry\nDays_To_Anthesis_BLUP_Sum0607\nDays_To_Silk_BLUP_Sum0607\nASI_BLUP_Sum0607\n\n\n\n\n0\nZ001E0001\n04P1367A51A\nZ001\n1\n1\n75.5364\n77.1298\n1.4600\n\n\n1\nZ001E0002\n04P1368A51A\nZ001\n1\n2\n76.9075\n77.7945\n1.3928\n\n\n2\nZ001E0003\n04P1368B51A\nZ001\n1\n3\n75.2646\n75.2555\n0.8644\n\n\n3\nZ001E0004\n04P1370B51A\nZ001\n1\n4\n73.6933\n75.7604\n2.0012\n\n\n4\nZ001E0005\n04P1371B51A\nZ001\n1\n5\n79.2441\n81.2611\n1.8931\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5458\nZ027E0277\nW64A\nNaN\n27\n277\n71.9008\n73.9811\n2.6756\n\n\n5459\nZ027E0278\nWD\nNaN\n27\n278\n62.0212\n60.5992\n-0.5733\n\n\n5460\nZ027E0279\nWf9\nNaN\n27\n279\n71.9970\n72.2319\n0.8338\n\n\n5461\nZ027E0280\nYu796_NS\nNaN\n27\n280\n74.5107\n73.9727\n0.2935\n\n\n5462\nZ027E0282\nMo17\nNaN\n27\n282\n72.7428\n75.5080\n3.0455\n\n\n\n\n5463 rows × 8 columns\n\n\n\n\npd.read_table(buckler_2009_path+'NAM_TasselingDate.txt', encoding=\"ISO-8859-1\")\n\n\n\n\n\n\n\n\nloc_name\nyear\nseason\npop\nblock\nplot\nrow\nplot_id\nentry_num\npedigree\nseed_source\naccename\nvalue\n\n\n\n\n0\nUrbana, IL\n2006\nSummer\n18\n1\n1\n1\n0650001\n21\n(B73×Mo18W)S5\n05CL1572-blk\nZ018E0021\n07/28/06\n\n\n1\nUrbana, IL\n2006\nSummer\n18\n1\n2\n2\n0650002\n41\n(B73×Mo18W)S5\n05FL4779-blk\nZ018E0041\n07/31/06\n\n\n2\nUrbana, IL\n2006\nSummer\n18\n1\n3\n3\n0650003\n128\n(B73×Mo18W)S5\n24MM1520\nZ018E0128\n07/31/06\n\n\n3\nUrbana, IL\n2006\nSummer\n18\n1\n4\n4\n0650004\n146\n(B73×Mo18W)S5\n05P114751A\nZ018E0146\n07/31/06\n\n\n4\nUrbana, IL\n2006\nSummer\n18\n1\n5\n5\n0650005\n105\n(B73×Mo18W)S5\n24MM1482\nZ018E0105\n08/02/06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n43454\nColumbia, MO\n2007\nSummer\n12\n3\n16\n2936\n27M32936\n179\n(B73×Ki11)S5\n05P025251A\nZ012E0179\n7/25/2007\n\n\n43455\nColumbia, MO\n2007\nSummer\n12\n3\n17\n2937\n27M32937\n182\n(B73×Ki11)S5\n05P025351A\nZ012E0182\n7/23/2007\n\n\n43456\nColumbia, MO\n2007\nSummer\n12\n3\n18\n2938\n27M32938\n180\n(B73×Ki11)S5\n05P029351A\nZ012E0180\n7/23/2007\n\n\n43457\nColumbia, MO\n2007\nSummer\n12\n3\n19\n2939\n27M32939\n168\n(B73×Ki11)S5\n05P030451A\nZ012E0168\n7/25/2007\n\n\n43458\nColumbia, MO\n2007\nSummer\n12\n3\n20\n2940\n27M32940\n8\n(B73×Ki11)S5\n05CL1149-blk\nZ012E0008\n7/21/2007\n\n\n\n\n43459 rows × 13 columns\n\n\n\n\n# pd.read_table(buckler_2009_path+'markergenotypes062508.txt', encoding=\"ISO-8859-1\")\n\n\npd.read_table(buckler_2009_path+'NAMSum0607FloweringTraitBLUPsAcross8Envs.txt', encoding=\"ISO-8859-1\")\n\n\n\n\n\n\n\n\nGeno_Code\nEntry_ID\nGroup\npop\nentry\nDays_To_Anthesis_BLUP_Sum0607\nDays_To_Silk_BLUP_Sum0607\nASI_BLUP_Sum0607\n\n\n\n\n0\nZ001E0001\n04P1367A51A\nZ001\n1\n1\n75.5364\n77.1298\n1.4600\n\n\n1\nZ001E0002\n04P1368A51A\nZ001\n1\n2\n76.9075\n77.7945\n1.3928\n\n\n2\nZ001E0003\n04P1368B51A\nZ001\n1\n3\n75.2646\n75.2555\n0.8644\n\n\n3\nZ001E0004\n04P1370B51A\nZ001\n1\n4\n73.6933\n75.7604\n2.0012\n\n\n4\nZ001E0005\n04P1371B51A\nZ001\n1\n5\n79.2441\n81.2611\n1.8931\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5458\nZ027E0277\nW64A\nNaN\n27\n277\n71.9008\n73.9811\n2.6756\n\n\n5459\nZ027E0278\nWD\nNaN\n27\n278\n62.0212\n60.5992\n-0.5733\n\n\n5460\nZ027E0279\nWf9\nNaN\n27\n279\n71.9970\n72.2319\n0.8338\n\n\n5461\nZ027E0280\nYu796_NS\nNaN\n27\n280\n74.5107\n73.9727\n0.2935\n\n\n5462\nZ027E0282\nMo17\nNaN\n27\n282\n72.7428\n75.5080\n3.0455\n\n\n\n\n5463 rows × 8 columns\n\n\n\n\n# os.listdir('../data/zma/panzea/genotypes/GBS/v27/ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_Table/')\n\n\nAGPv4_path = '../data/zma/panzea/genotypes/GBS/v27/'\n\n\n# Other than listing the taxa this isn't expected to be of much use for our purposes.\nAGPv4_taxa=pd.read_table(AGPv4_path+'ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_TaxaList.txt')\nAGPv4_taxa.head()\n\n\n\n\n\n\n\n\nTaxa\nLibraryPrepID\nStatus\nDNAPlate\nGENUS\nINBREEDF\nSPECIES\nDNASample\nFlowcell_Lane\nNumLanes\n...\nGermplasmSet\nBarcode\nLibraryPlate\nTassel4SampleName\nPopulation\nLibraryPlateWell\nSampleDNAWell\nOwnerEmail\nPEDIGREE\nSeedLot\n\n\n\n\n0\n05-397:250007467\n250007467\npublic\nP3-GS-F\nZea\n0.95\nmays\n05-397\nC00R8ABXX_4\n1.0\n...\nMargaret Smith lines\nGTTGAA\nP3-GS-F\n05-397:C00R8ABXX:4:250007467\nInbred\nF11\nF11\nesb33@cornell.edu\nNaN\nNaN\n\n\n1\n05-438:250007407\n250007407\npublic\nP3-GS-F\nZea\n0.95\nmays\n05-438\nC00R8ABXX_4\n1.0\n...\nMargaret Smith lines\nGTATT\nP3-GS-F\n05-438:C00R8ABXX:4:250007407\nInbred\nB03\nB03\nesb33@cornell.edu\nNaN\nNaN\n\n\n2\n12E:250032344\n250032344\npublic\nAmes12\nZea\n0.95\nmays\n12E\n81FE8ABXX_4\n1.0\n...\nAmes\nGCTGTGGA\nAmes12\n12E:81FE8ABXX:4:250032344\ninbred\nH08\nH08\nesb33@cornell.edu\n12E\nNaN\n\n\n3\n207:250007202\n250007202\npublic\nP1-GS-F\nZea\n0.95\nmays\n207\nC00R8ABXX_2\n1.0\n...\nMargaret Smith lines\nTACAT\nP1-GS-F\n207:C00R8ABXX:2:250007202\nInbred\nE12\nE12\nesb33@cornell.edu\nNaN\nNaN\n\n\n4\n22612:250007466\n250007466\npublic\nP3-GS-F\nZea\n0.95\nmays\n22612\nC00R8ABXX_4\n1.0\n...\nMargaret Smith lines\nGTACTT\nP3-GS-F\n22612:C00R8ABXX:4:250007466\nInbred\nF10\nF10\nesb33@cornell.edu\nNaN\nNaN\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n# Useful for converting between the physical location and site\nAGPv4_site = pd.read_table(AGPv4_path+'ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_PositionList.txt')\nAGPv4_site.head()\n\n\n\n\n\n\n\n\nSite\nName\nChromosome\nPosition\n\n\n\n\n0\n0\nS1_6370\n1\n52399\n\n\n1\n1\nS1_8210\n1\n54239\n\n\n2\n2\nS1_8376\n1\n54405\n\n\n3\n3\nS1_9889\n1\n55917\n\n\n4\n4\nS1_9899\n1\n55927\n\n\n\n\n\n\n\nRetrieving a genome by taxa name:\n\n# The genomes are in a folder with an identical name as their source table\ntable_directory = AGPv4_path+'ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_Table/'\n# Note however that the naming is altered to not use ':'\nos.listdir(table_directory)[0:3]\n\n['xztea', 'xzedh', 'Z020E0024__250026132']\n\n\n\ntaxa_to_filename(taxa = '05-397:250007467')\n\n'05-397__250007467'\n\n\n\ndef get_AGPv4(taxa):\n    with open(table_directory+taxa, 'r') as f:\n        data = f.read()    \n    data = data.split('\\t')\n    return(data)\n\n\nget_AGPv4('05-397__250007467')[0:4]\n\n['05-397:250007467', 'T', 'T', 'A']\n\n\nIn addition to returning a specific taxa, the table’s headers can be retieved with “taxa”.\n\nget_AGPv4(taxa = 'taxa')[0:4]\n\n['Taxa', '52399', '54239', '54405']\n\n\nConverting between site and chromosome/position requires the AGPv4_site dataframe. A given record contains the taxa as well as the nucleotides, so with that entry excluded the chromosome / position can be paired up.\n\nlen(get_AGPv4(taxa = 'taxa')), AGPv4_site.shape\n\n(943456, (943455, 4))\n\n\n\nith_taxa = '05-397:250007467'\nres = get_AGPv4(taxa_to_filename(taxa = ith_taxa))   # Retrieve record\ntemp = AGPv4_site.loc[:, ['Chromosome', 'Position']]  \ntemp[res[0]] = res[1:]                               # Add Col. with Nucleotides\ntemp.head()\n\n\n\n\n\n\n\n\nChromosome\nPosition\n05-397:250007467\n\n\n\n\n0\n1\n52399\nT\n\n\n1\n1\n54239\nT\n\n\n2\n1\n54405\nA\n\n\n3\n1\n55917\nN\n\n\n4\n1\n55927\nN"
  },
  {
    "objectID": "zma_trial.html#look-at-snp-coverage",
    "href": "zma_trial.html#look-at-snp-coverage",
    "title": "Retrieve genomic data for phenotypes",
    "section": "Look at SNP coverage",
    "text": "Look at SNP coverage\n\nmask = (temp.Chromosome == 1)\n\ntemp_pos = temp.loc[mask, ['Position']]\n\n\ntemp_pos['Shift'] = 0\ntemp_pos.loc[1: , ['Shift']] = np.array(temp_pos.Position)[:-1]\ntemp_pos['Diff'] = temp_pos['Position'] - temp_pos['Shift']\n\ntemp_pos.loc[0, 'Diff'] = None\n\n\ntemp_pos\n\n\n\n\n\n\n\n\nPosition\nShift\nDiff\n\n\n\n\n0\n52399\n0\nNaN\n\n\n1\n54239\n52399\n1840.0\n\n\n2\n54405\n54239\n166.0\n\n\n3\n55917\n54405\n1512.0\n\n\n4\n55927\n55917\n10.0\n\n\n...\n...\n...\n...\n\n\n147145\n306971046\n306910117\n60929.0\n\n\n147146\n306971061\n306971046\n15.0\n\n\n147147\n306971063\n306971061\n2.0\n\n\n147148\n306971073\n306971063\n10.0\n\n\n147149\n306971080\n306971073\n7.0\n\n\n\n\n147150 rows × 3 columns\n\n\n\n\n# px.histogram(temp_pos, x = 'Diff')"
  },
  {
    "objectID": "zma_trial.html#demonstrate-hilbert-curve",
    "href": "zma_trial.html#demonstrate-hilbert-curve",
    "title": "Retrieve genomic data for phenotypes",
    "section": "Demonstrate Hilbert Curve",
    "text": "Demonstrate Hilbert Curve\n\n# demonstrating the hilbert curve\ntemp = np.linspace(1, 100, num= 50)\n# px.scatter(x = temp, y = [0 for e in range(len(temp))], color = temp)\npx.imshow(temp.reshape((1, temp.shape[0])))\n\n\n                                                \n\n\n\n# hilbert_curve = HilbertCurve(p = 10, # iterations i.e. hold 4^p positions\n#                              n = 2    # dimensions\n#                             )\n# distances = list(range(len(temp)))\n\n# points = hilbert_curve.points_from_distances(distances)\n# # px.line(pd.DataFrame(points, columns = ['i', 'j']), x = 'i', y = 'j')\n\nNameError: name 'HilbertCurve' is not defined\n\n\n\n# dim_0 = np.max(np.array(points)[:, 0])+1 # add 1 to account for 0 indexing\n# dim_1 = np.max(np.array(points)[:, 1])+1\n# temp_mat = np.zeros(shape = [dim_0, dim_1])\n# temp_mat[temp_mat == 0] = np.nan         #  empty values being used for visualization\n\n# for i in range(len(temp)):\n# #     print(i)\n#     temp_mat[points[i][0], points[i][1]] = temp[i]\n    \n# # temp2 = pd.DataFrame(points, columns = ['i', 'j'])\n# # temp2['value'] = temp\n# # px.scatter(temp2, x = 'i', y = 'j', color = 'value')\n\n# px.imshow(temp_mat)\n\nNameError: name 'points' is not defined\n\n\n\n# # Data represented need not be continuous -- it need only have int positions\n# # a sequence or a sequence with gaps can be encoded\n# hilbert_curve = HilbertCurve(p = 10, # iterations i.e. hold 4^p positions\n#                              n = 2    # dimensions\n#                             )\n\n\n# fake_dists = list(range(len(temp)))\n# # Introdude a gap in the sequence\n# fake_dists = [e if e&gt;10 else e+5 for e in fake_dists]\n# distances = fake_dists\n\n# points = hilbert_curve.points_from_distances(distances)\n# dim_0 = np.max(np.array(points)[:, 0])+1 # add 1 to account for 0 indexing\n# dim_1 = np.max(np.array(points)[:, 1])+1\n# temp_mat = np.zeros(shape = [dim_0, dim_1])\n# temp_mat[temp_mat == 0] = np.nan         #  empty values being used for visualization\n\n# for i in range(len(temp)):\n# #     print(i)\n#     temp_mat[points[i][0], points[i][1]] = temp[i]\n# px.imshow(temp_mat)\n\nNameError: name 'HilbertCurve' is not defined\n\n\n\nHilbert curve for one sequence\n\n# temp_pos['Present'] = 1\n\n\n# temp_pos.shape[0]\n\n147150\n\n\n\n# def calc_needed_hilbert_p(n_needed = 1048576,\n                        #                           max_p = 20):\n                        #     out = None\n                        #     for i in range(1, max_p):\n                        #         if 4**i &gt; n_needed:\n                        #             out = i\n                        #             break\n                        #     return(out)\n\n                        # calc_needed_hilbert_p(n_needed=147150)\n\n9\n\n\n\n# temp_pos['Position']\n\n0             52399\n1             54239\n2             54405\n3             55917\n4             55927\n            ...    \n147145    306971046\n147146    306971061\n147147    306971063\n147148    306971073\n147149    306971080\nName: Position, Length: 147150, dtype: int64\n\n\n\n# # Data represented need not be continuous -- it need only have int positions\n# # a sequence or a sequence with gaps can be encoded\n# hilbert_curve = HilbertCurve(p = 10, # iterations i.e. hold 4^p positions\n#                              n = 2    # dimensions\n#                             )\n\n\n# fake_dists = list(range(len(temp)))\n# # Introdude a gap in the sequence\n# fake_dists = [e if e&gt;10 else e+5 for e in fake_dists]\n# distances = fake_dists\n\n# points = hilbert_curve.points_from_distances(distances)\n# dim_0 = np.max(np.array(points)[:, 0])+1 # add 1 to account for 0 indexing\n# dim_1 = np.max(np.array(points)[:, 1])+1\n# temp_mat = np.zeros(shape = [dim_0, dim_1])\n# temp_mat[temp_mat == 0] = np.nan         #  empty values being used for visualization\n\n# for i in range(len(temp)):\n# #     print(i)\n#     temp_mat[points[i][0], points[i][1]] = temp[i]\n# px.imshow(temp_mat)"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert_nodropout.html",
    "href": "tianetal2011_model_g2pdeephilbert_nodropout.html",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "",
    "text": "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n#                          Hours to wait\n# import time; time.sleep( 24 * (60*60))\n# Run Settings:\nnb_name = '15.1_TianEtAl2011'# Set manually! -----------------------------------\n\ndownsample_obs = False\ntrain_n = 90\ntest_n = 10\n\ndataloader_batch_size = 64 #8 #16 #64\nrun_epochs = 40#0\n\nuse_gpu_num = 0\n\n# Imports --------------------------------------------------------------------\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nimport tqdm\nfrom tqdm import tqdm\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n\nimport dlgwas\nfrom dlgwas.kegg import ensure_dir_path_exists\nfrom dlgwas.kegg import get_cached_result\nfrom dlgwas.kegg import put_cached_result\n\nfrom dlgwas.dlfn import calc_cs\nfrom dlgwas.dlfn import apply_cs\nfrom dlgwas.dlfn import reverse_cs\n\nfrom dlgwas.dlfn import TianEtAl2011Dataset\nfrom dlgwas.dlfn import train_loop\nfrom dlgwas.dlfn import train_error\nfrom dlgwas.dlfn import test_loop\nfrom dlgwas.dlfn import train_nn\nfrom dlgwas.dlfn import yhat_loop\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nUsing cuda device\n# # overwrite bc python 3.7 isn't co\n\n# def get_cached_result(\n#     save_path\n# ):\n#     import os\n#     import pickle5 as pkl\n#     if not os.path.exists(save_path):\n#         cached_result = None\n#     else:\n#         with open(save_path, 'rb') as handle:\n#                 cached_result = pkl.load(handle)\n#     return(cached_result)"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert_nodropout.html#load-cleaned-data",
    "href": "tianetal2011_model_g2pdeephilbert_nodropout.html#load-cleaned-data",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Load Cleaned Data",
    "text": "Load Cleaned Data\n\n# Read in cleaned data\ntaxa_groupings = pd.read_csv('../models/10_TianEtAl2011/taxa_groupings.csv')\ndata           = pd.read_csv('../models/10_TianEtAl2011/clean_data.csv')\n\n# Define holdout sets (Populations)\nuniq_pop = list(set(taxa_groupings['Population']))\nprint(str(len(uniq_pop))+\" Unique Holdout Groups.\")\ntaxa_groupings['Holdout'] = None\nfor i in range(len(uniq_pop)):\n    mask = (taxa_groupings['Population'] == uniq_pop[i])\n    taxa_groupings.loc[mask, 'Holdout'] = i\n\ntaxa_groupings\n\n25 Unique Holdout Groups.\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nsample\nPopulation\nHoldout\n\n\n\n\n0\n0\nZ001E0001\nB73 x B97\n1\n\n\n1\n1\nZ001E0002\nB73 x B97\n1\n\n\n2\n2\nZ001E0003\nB73 x B97\n1\n\n\n3\n3\nZ001E0004\nB73 x B97\n1\n\n\n4\n4\nZ001E0005\nB73 x B97\n1\n\n\n...\n...\n...\n...\n...\n\n\n4671\n4671\nZ026E0196\nB73 x Tzi8\n4\n\n\n4672\n4672\nZ026E0197\nB73 x Tzi8\n4\n\n\n4673\n4673\nZ026E0198\nB73 x Tzi8\n4\n\n\n4674\n4674\nZ026E0199\nB73 x Tzi8\n4\n\n\n4675\n4675\nZ026E0200\nB73 x Tzi8\n4\n\n\n\n\n4676 rows × 4 columns"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert_nodropout.html#setup-holdouts",
    "href": "tianetal2011_model_g2pdeephilbert_nodropout.html#setup-holdouts",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Setup Holdouts",
    "text": "Setup Holdouts\n\n#randomly holdout a population if there is not a file with the population held out.\n# Holdout_Int = 0\nHoldout_Int_path = '../models/'+nb_name+'/holdout_pop_int.pkl'\nif None != get_cached_result(Holdout_Int_path):\n    Holdout_Int = get_cached_result(Holdout_Int_path)\nelse:\n    Holdout_Int = int(np.random.choice([i for i in range(len(uniq_pop))], 1))\n    put_cached_result(Holdout_Int_path, Holdout_Int)\n\n    \nprint(\"Holding out i=\"+str(Holdout_Int)+\": \"+uniq_pop[Holdout_Int])\n\nmask = (taxa_groupings['Holdout'] == Holdout_Int)\ntrain_idxs = list(taxa_groupings.loc[~mask, ].index)\ntest_idxs = list(taxa_groupings.loc[mask, ].index)\n\nHolding out i=11: B73 x CML333\n\n\n\n# downsample_obs = True\n# train_n = 900\n# test_n = 100\n\nif downsample_obs == True:\n    train_idxs = np.random.choice(train_idxs, train_n)\n    test_idxs = np.random.choice(test_idxs, test_n)\n    print([len(e) for e in [test_idxs, train_idxs]])\n    \n# used to go from index in tensor to index in data so that the right xs tensor can be loaded in\nidx_original = np.array(data.index)\n\ny1 = data['leaf_length']\ny2 = data['leaf_width']\ny3 = data['upper_leaf_angle']\ny1 = np.array(y1)\ny2 = np.array(y2)\ny3 = np.array(y3)\n\n\nScale data\n\nscale_dict_path = '../models/'+nb_name+'/scale_dict.pkl'\nif None != get_cached_result(scale_dict_path):\n    scale_dict = get_cached_result(scale_dict_path)\nelse:\n    scale_dict = {\n        'y1':calc_cs(y1[train_idxs]),\n        'y2':calc_cs(y2[train_idxs]),\n        'y3':calc_cs(y3[train_idxs])\n    }\n    put_cached_result(scale_dict_path, scale_dict)\n\ny1 = apply_cs(y1, scale_dict['y1'])\ny2 = apply_cs(y2, scale_dict['y2'])\ny3 = apply_cs(y3, scale_dict['y3'])"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert_nodropout.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "href": "tianetal2011_model_g2pdeephilbert_nodropout.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Allow for cycling data onto and off of GPU",
    "text": "Allow for cycling data onto and off of GPU\n\n# loading this into memory causes the session to crash\n\ny1_train = torch.from_numpy(y1[train_idxs])[:, None]\ny2_train = torch.from_numpy(y2[train_idxs])[:, None]\ny3_train = torch.from_numpy(y3[train_idxs])[:, None]\n\nidx_original_train = torch.from_numpy(idx_original[train_idxs])\n\ny1_test = torch.from_numpy(y1[test_idxs])[:, None]\ny2_test = torch.from_numpy(y2[test_idxs])[:, None]\ny3_test = torch.from_numpy(y3[test_idxs])[:, None]\n\nidx_original_test = torch.from_numpy(idx_original[test_idxs])\n\n\n# dataloader_batch_size = 64\n\ntraining_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_train,\n        y2 = y2_train,\n        y3 = y3_train,\n        marker_type = 'hilbert',\n        idx_original = idx_original_train,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\ntesting_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_test,\n        y2 = y2_test,\n        y3 = y3_test,\n        marker_type = 'hilbert',\n        idx_original = idx_original_test,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\nUsing cuda device\nUsing cuda device"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert_nodropout.html#non-boilerplate",
    "href": "tianetal2011_model_g2pdeephilbert_nodropout.html#non-boilerplate",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Non-Boilerplate",
    "text": "Non-Boilerplate\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()    \n\n        # Block 1 ------------------------------------------------------------\n        self.long_way_0 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, # second channel\n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 1,\n                    padding = 1,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Dropout(p=0.0)\n            )\n        \n        self.shortcut_0 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                )\n        )\n        # Block 2 ------------------------------------------------------------\n        self.long_way_1 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, # second channel\n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 1,\n                    padding = 1,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Dropout(p=0.0)\n            )\n        \n        self.shortcut_1 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                )\n        )\n        # Block 3 ------------------------------------------------------------\n        self.long_way_2 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, # second channel\n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 1,\n                    padding = 1,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Dropout(p=0.0)\n            )\n        \n        self.shortcut_2 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                )\n        )        \n        \n        \n        self.feature_processing = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride = 2,\n                    bias = True\n                ),\n            nn.Dropout(p=0.0)\n        )\n\n        self.output_processing = nn.Sequential(\n            nn.Flatten(),\n            nn.ReLU(), # They used inverse square root activation $y = \\frac{x}{\\sqrt{1+ax^2}}$\n            nn.Dropout(p=0.0),\n            nn.Linear(15876, 1)\n        )            \n        \n    def forward(self, x):\n        x_out = self.long_way_0(x)\n        x_shortcut = self.shortcut_0(x)\n        x_out += x_shortcut\n        \n        x = x_out\n        x_out = self.long_way_1(x)\n        x_shortcut = self.shortcut_1(x)\n        x_out += x_shortcut\n        \n        x = x_out\n        x_out = self.long_way_2(x)\n        x_shortcut = self.shortcut_2(x)        \n        x_out += x_shortcut\n        \n        x_out = self.feature_processing(x_out)\n        x_out = self.output_processing(x_out)\n        \n        return x_out\n\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n\n# model = NeuralNetwork().to(device)\n\n# res = model(xs_i) # try prediction on one batch\n# res.shape\n\n\ndef train_nn(\n    nb_name,\n    training_dataloader,\n    testing_dataloader,\n    model,\n    learning_rate = 1e-3,\n    batch_size = 64,\n    epochs = 500\n):\n    # Initialize the loss function\n    loss_fn = nn.MSELoss()\n#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    # Optimizer with L2 normalization\n    optimizer = torch.optim.Adam([\n        {'params':model.long_way_0.parameters(), 'weight_decay': 0.1},\n        {'params':model.shortcut_0.parameters(), 'weight_decay': 0.1},\n        {'params':model.long_way_1.parameters(), 'weight_decay': 0.1},\n        {'params':model.shortcut_1.parameters(), 'weight_decay': 0.1},\n        {'params':model.long_way_2.parameters(), 'weight_decay': 0.1},\n        {'params':model.shortcut_2.parameters(), 'weight_decay': 0.1},\n        {'params':model.feature_processing.parameters(),        'weight_decay': 0.1},\n        {'params':model.output_processing.parameters(),         'weight_decay': 0.01},\n    ], lr=learning_rate)\n\n    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n    loss_df['TrainMSE'] = np.nan\n    loss_df['TestMSE']  = np.nan\n\n    for t in tqdm(range(epochs)):        \n#         print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n\n        loss_df.loc[loss_df.index == t, 'TrainMSE'\n                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n        \n        loss_df.loc[loss_df.index == t, 'TestMSE'\n                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n        \n        if (t+1)%10: # Cache in case training is interupted\n#             print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n            torch.save(model.state_dict(), \n                       '../models/'+nb_name+'/model_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n            loss_df.to_csv('../reports/'+nb_name+'/loss_df'+str(t)+'_'+str(epochs)+'.csv', index=False) \n        \n    return([model, loss_df])\n\n\n# don't run if either of these exist because there may be cases where we want the results but not the model\n\nif not os.path.exists('../models/'+nb_name+'/model.pt'): \n    # Shared setup (train from scratch and load latest)\n    model = NeuralNetwork()\n\n    # find the biggest model to save\n    saved_models = os.listdir('../models/'+nb_name+'/')\n    saved_models = [e for e in saved_models if re.match('model*', e)]\n\n    if saved_models == []:\n        epochs_run = 0\n    else:\n        # if there are saved models reload and resume training\n        saved_models_numbers = [int(e.replace('model_', ''\n                                    ).replace('.pt', ''\n                                    ).split('_')[0]) for e in saved_models]\n        # saved_models\n        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n        latest_model = [e for e in saved_models if re.match(\n            '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n        model.load_state_dict(torch.load('../models/'+nb_name+'/'+latest_model))\n        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n    \n    model.to(device)   \n\n    model, loss_df = train_nn(\n        nb_name,\n        training_dataloader,\n        testing_dataloader,\n        model,\n        learning_rate = 1e-3,\n        batch_size = dataloader_batch_size,\n        epochs = (run_epochs - epochs_run)\n    )\n    \n    # experimental outputs:\n    # 1. Model\n    torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n\n    # 2. loss_df\n    loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)  \n    \n    \n    # 3. predictions \n    yhats = pd.concat([\n        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n\n    yhats.to_csv('../reports/'+nb_name+'/yhats.csv', index=False)\n\n\nNeuralNetwork()\n\nNeuralNetwork(\n  (long_way_0): Sequential(\n    (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2))\n    (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): Dropout(p=0.0, inplace=False)\n  )\n  (shortcut_0): Sequential(\n    (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2))\n  )\n  (long_way_1): Sequential(\n    (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2))\n    (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): Dropout(p=0.0, inplace=False)\n  )\n  (shortcut_1): Sequential(\n    (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2))\n  )\n  (long_way_2): Sequential(\n    (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2))\n    (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): Dropout(p=0.0, inplace=False)\n  )\n  (shortcut_2): Sequential(\n    (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2))\n  )\n  (feature_processing): Sequential(\n    (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2))\n    (1): Dropout(p=0.0, inplace=False)\n  )\n  (output_processing): Sequential(\n    (0): Flatten()\n    (1): ReLU()\n    (2): Dropout(p=0.0, inplace=False)\n    (3): Linear(in_features=15876, out_features=1, bias=True)\n  )\n)"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert_nodropout.html#standard-visualizations",
    "href": "tianetal2011_model_g2pdeephilbert_nodropout.html#standard-visualizations",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Standard Visualizations",
    "text": "Standard Visualizations\n\nloss_df = pd.read_csv('../reports/'+nb_name+'/loss_df.csv')\n\nloss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\nloss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n                    mode='lines', name='Test'))\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n                    mode='lines', name='Train'))\nfig.show()\n\n\n                                                \n\n\n\nyhats = pd.read_csv('../reports/'+nb_name+'/yhats.csv')\n\nyhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\nyhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n\npx.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")\n\n\n                                                \n\n\n\nyhats['Error'] = yhats.y_true - yhats.y_pred\n\npx.histogram(yhats, x = 'Error', color = 'Split',\n             marginal=\"box\", # can be `rug`, `violin`\n             nbins= 50)\n\n\n                                                \n\n\n\n# automatically kill kernel after running. \n# This is a hacky way to free up _all_ space on the gpus\n# os._exit(00)"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "This project is currently pre-release. Instructions on use will be added in the future."
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert.html",
    "href": "tianetal2011_model_g2pdeephilbert.html",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "",
    "text": "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n#                          Hours to wait\n# import time; time.sleep( 24 * (60*60))\n# Run Settings:\nnb_name = '15_TianEtAl2011'# Set manually! -----------------------------------\n\ndownsample_obs = False\ntrain_n = 90\ntest_n = 10\n\ndataloader_batch_size = 8 #16 #64\nrun_epochs = 200\n\nuse_gpu_num = 0\n\n# Imports --------------------------------------------------------------------\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nimport tqdm\nfrom tqdm import tqdm\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n\nimport dlgwas\nfrom dlgwas.kegg import ensure_dir_path_exists\nfrom dlgwas.kegg import get_cached_result\nfrom dlgwas.kegg import put_cached_result\n\nfrom dlgwas.dlfn import calc_cs\nfrom dlgwas.dlfn import apply_cs\nfrom dlgwas.dlfn import reverse_cs\n\nfrom dlgwas.dlfn import TianEtAl2011Dataset\nfrom dlgwas.dlfn import train_loop\nfrom dlgwas.dlfn import train_error\nfrom dlgwas.dlfn import test_loop\nfrom dlgwas.dlfn import train_nn\nfrom dlgwas.dlfn import yhat_loop\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nModuleNotFoundError: No module named 'dlgwas'"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert.html#load-cleaned-data",
    "href": "tianetal2011_model_g2pdeephilbert.html#load-cleaned-data",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Load Cleaned Data",
    "text": "Load Cleaned Data\n\n# Read in cleaned data\ntaxa_groupings = pd.read_csv('../models/10_TianEtAl2011/taxa_groupings.csv')\ndata           = pd.read_csv('../models/10_TianEtAl2011/clean_data.csv')\n\n# Define holdout sets (Populations)\nuniq_pop = list(set(taxa_groupings['Population']))\nprint(str(len(uniq_pop))+\" Unique Holdout Groups.\")\ntaxa_groupings['Holdout'] = None\nfor i in range(len(uniq_pop)):\n    mask = (taxa_groupings['Population'] == uniq_pop[i])\n    taxa_groupings.loc[mask, 'Holdout'] = i\n\ntaxa_groupings"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert.html#setup-holdouts",
    "href": "tianetal2011_model_g2pdeephilbert.html#setup-holdouts",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Setup Holdouts",
    "text": "Setup Holdouts\n\n#randomly holdout a population if there is not a file with the population held out.\n# Holdout_Int = 0\nHoldout_Int_path = '../models/'+nb_name+'/holdout_pop_int.pkl'\nif None != get_cached_result(Holdout_Int_path):\n    Holdout_Int = get_cached_result(Holdout_Int_path)\nelse:\n    Holdout_Int = int(np.random.choice([i for i in range(len(uniq_pop))], 1))\n    put_cached_result(Holdout_Int_path, Holdout_Int)\n\n    \nprint(\"Holding out i=\"+str(Holdout_Int)+\": \"+uniq_pop[Holdout_Int])\n\nmask = (taxa_groupings['Holdout'] == Holdout_Int)\ntrain_idxs = list(taxa_groupings.loc[~mask, ].index)\ntest_idxs = list(taxa_groupings.loc[mask, ].index)\n\n\n# downsample_obs = True\n# train_n = 900\n# test_n = 100\n\nif downsample_obs == True:\n    train_idxs = np.random.choice(train_idxs, train_n)\n    test_idxs = np.random.choice(test_idxs, test_n)\n    print([len(e) for e in [test_idxs, train_idxs]])\n    \n# used to go from index in tensor to index in data so that the right xs tensor can be loaded in\nidx_original = np.array(data.index)\n\ny1 = data['leaf_length']\ny2 = data['leaf_width']\ny3 = data['upper_leaf_angle']\ny1 = np.array(y1)\ny2 = np.array(y2)\ny3 = np.array(y3)\n\n\nScale data\n\nscale_dict_path = '../models/'+nb_name+'/scale_dict.pkl'\nif None != get_cached_result(scale_dict_path):\n    scale_dict = get_cached_result(scale_dict_path)\nelse:\n    scale_dict = {\n        'y1':calc_cs(y1[train_idxs]),\n        'y2':calc_cs(y2[train_idxs]),\n        'y3':calc_cs(y3[train_idxs])\n    }\n    put_cached_result(scale_dict_path, scale_dict)\n\ny1 = apply_cs(y1, scale_dict['y1'])\ny2 = apply_cs(y2, scale_dict['y2'])\ny3 = apply_cs(y3, scale_dict['y3'])"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "href": "tianetal2011_model_g2pdeephilbert.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Allow for cycling data onto and off of GPU",
    "text": "Allow for cycling data onto and off of GPU\n\n# loading this into memory causes the session to crash\n\ny1_train = torch.from_numpy(y1[train_idxs])[:, None]\ny2_train = torch.from_numpy(y2[train_idxs])[:, None]\ny3_train = torch.from_numpy(y3[train_idxs])[:, None]\n\nidx_original_train = torch.from_numpy(idx_original[train_idxs])\n\ny1_test = torch.from_numpy(y1[test_idxs])[:, None]\ny2_test = torch.from_numpy(y2[test_idxs])[:, None]\ny3_test = torch.from_numpy(y3[test_idxs])[:, None]\n\nidx_original_test = torch.from_numpy(idx_original[test_idxs])\n\n\n# dataloader_batch_size = 64\n\ntraining_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_train,\n        y2 = y2_train,\n        y3 = y3_train,\n        marker_type = 'hilbert',\n        idx_original = idx_original_train,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\ntesting_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_test,\n        y2 = y2_test,\n        y3 = y3_test,\n        marker_type = 'hilbert',\n        idx_original = idx_original_test,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert.html#non-boilerplate",
    "href": "tianetal2011_model_g2pdeephilbert.html#non-boilerplate",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Non-Boilerplate",
    "text": "Non-Boilerplate\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()    \n\n        # Block 1 ------------------------------------------------------------\n        self.long_way_0 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, # second channel\n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 1,\n                    padding = 1,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Dropout(p=0.75)\n            )\n        \n        self.shortcut_0 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                )\n        )\n        # Block 2 ------------------------------------------------------------\n        self.long_way_1 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, # second channel\n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 1,\n                    padding = 1,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Dropout(p=0.75)\n            )\n        \n        self.shortcut_1 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                )\n        )\n        # Block 3 ------------------------------------------------------------\n        self.long_way_2 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, # second channel\n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 1,\n                    padding = 1,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Dropout(p=0.75)\n            )\n        \n        self.shortcut_2 = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride= 2,\n                    bias = True\n                )\n        )        \n        \n        \n        self.feature_processing = nn.Sequential(\n            nn.Conv2d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= (3, 3),\n                    stride = 2,\n                    bias = True\n                ),\n            nn.Dropout(p=0.75)\n        )\n\n        self.output_processing = nn.Sequential(\n            nn.Flatten(),\n            nn.ReLU(), # They used inverse square root activation $y = \\frac{x}{\\sqrt{1+ax^2}}$\n            nn.Dropout(p=0.75),\n            nn.Linear(15876, 1)\n        )            \n        \n    def forward(self, x):\n        x_out = self.long_way_0(x)\n        x_shortcut = self.shortcut_0(x)\n        x_out += x_shortcut\n        \n        x = x_out\n        x_out = self.long_way_1(x)\n        x_shortcut = self.shortcut_1(x)\n        x_out += x_shortcut\n        \n        x = x_out\n        x_out = self.long_way_2(x)\n        x_shortcut = self.shortcut_2(x)        \n        x_out += x_shortcut\n        \n        x_out = self.feature_processing(x_out)\n        x_out = self.output_processing(x_out)\n        \n        return x_out\n\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n\n# model = NeuralNetwork().to(device)\n\n# res = model(xs_i) # try prediction on one batch\n# res.shape\n\n\ndef train_nn(\n    nb_name,\n    training_dataloader,\n    testing_dataloader,\n    model,\n    learning_rate = 1e-3,\n    batch_size = 64,\n    epochs = 500\n):\n    # Initialize the loss function\n    loss_fn = nn.MSELoss()\n#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    # Optimizer with L2 normalization\n    optimizer = torch.optim.Adam([\n        {'params':model.long_way_0.parameters(), 'weight_decay': 0.1},\n        {'params':model.shortcut_0.parameters(), 'weight_decay': 0.1},\n        {'params':model.long_way_1.parameters(), 'weight_decay': 0.1},\n        {'params':model.shortcut_1.parameters(), 'weight_decay': 0.1},\n        {'params':model.long_way_2.parameters(), 'weight_decay': 0.1},\n        {'params':model.shortcut_2.parameters(), 'weight_decay': 0.1},\n        {'params':model.feature_processing.parameters(),        'weight_decay': 0.1},\n        {'params':model.output_processing.parameters(),         'weight_decay': 0.01},\n    ], lr=learning_rate)\n\n    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n    loss_df['TrainMSE'] = np.nan\n    loss_df['TestMSE']  = np.nan\n\n    for t in tqdm(range(epochs)):        \n#         print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n\n        loss_df.loc[loss_df.index == t, 'TrainMSE'\n                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n        \n        loss_df.loc[loss_df.index == t, 'TestMSE'\n                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n        \n        if (t+1)%10: # Cache in case training is interupted\n#             print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n            torch.save(model.state_dict(), \n                       '../models/'+nb_name+'/model_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n            loss_df.to_csv('../reports/'+nb_name+'/loss_df'+str(t)+'_'+str(epochs)+'.csv', index=False) \n        \n    return([model, loss_df])\n\n\n# don't run if either of these exist because there may be cases where we want the results but not the model\n\nif not os.path.exists('../models/'+nb_name+'/model.pt'): \n    # Shared setup (train from scratch and load latest)\n    model = NeuralNetwork()\n\n    # find the biggest model to save\n    saved_models = os.listdir('../models/'+nb_name+'/')\n    saved_models = [e for e in saved_models if re.match('model*', e)]\n\n    if saved_models == []:\n        epochs_run = 0\n    else:\n        # if there are saved models reload and resume training\n        saved_models_numbers = [int(e.replace('model_', ''\n                                    ).replace('.pt', ''\n                                    ).split('_')[0]) for e in saved_models]\n        # saved_models\n        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n        latest_model = [e for e in saved_models if re.match(\n            '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n        model.load_state_dict(torch.load('../models/'+nb_name+'/'+latest_model))\n        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n    \n    model.to(device)   \n\n    model, loss_df = train_nn(\n        nb_name,\n        training_dataloader,\n        testing_dataloader,\n        model,\n        learning_rate = 1e-3,\n        batch_size = dataloader_batch_size,\n        epochs = (run_epochs - epochs_run)\n    )\n    \n    # experimental outputs:\n    # 1. Model\n    torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n\n    # 2. loss_df\n    loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)  \n    \n    \n    # 3. predictions \n    yhats = pd.concat([\n        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n\n    yhats.to_csv('../reports/'+nb_name+'/yhats.csv', index=False)\n\n\nNeuralNetwork()"
  },
  {
    "objectID": "tianetal2011_model_g2pdeephilbert.html#standard-visualizations",
    "href": "tianetal2011_model_g2pdeephilbert.html#standard-visualizations",
    "title": "Tian et al. 2011 Model 5 Conv. 2d Hilbert Curve",
    "section": "Standard Visualizations",
    "text": "Standard Visualizations\n\nloss_df = pd.read_csv('../reports/'+nb_name+'/loss_df.csv')\n\nloss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\nloss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n                    mode='lines', name='Test'))\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n                    mode='lines', name='Train'))\nfig.show()\n\n\nyhats = pd.read_csv('../reports/'+nb_name+'/yhats.csv')\n\nyhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\nyhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n\npx.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")\n\n\nyhats['Error'] = yhats.y_true - yhats.y_pred\n\npx.histogram(yhats, x = 'Error', color = 'Split',\n             marginal=\"box\", # can be `rug`, `violin`\n             nbins= 50)\n\n\n# automatically kill kernel after running. \n# This is a hacky way to free up _all_ space on the gpus\n# os._exit(00)"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep3resblocks.html",
    "href": "tianetal2011_model_g2pdeep3resblocks.html",
    "title": "Tian et al. 2011 Model 4 (G2PDeep Inspired, Smaller)",
    "section": "",
    "text": "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n#                          Hours to wait\n# import time; time.sleep( 24 * (60*60))\n# Run Settings:\nnb_name = '14_TianEtAl2011'# Set manually! -----------------------------------\n\ndownsample_obs = False\ntrain_n = 90\ntest_n = 10\n\ndataloader_batch_size = 8 #16 #64\nrun_epochs = 200\n\nuse_gpu_num = 0\n\n# Imports --------------------------------------------------------------------\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nimport tqdm\nfrom tqdm import tqdm\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n\nimport dlgwas\nfrom dlgwas.kegg import ensure_dir_path_exists\nfrom dlgwas.kegg import get_cached_result\nfrom dlgwas.kegg import put_cached_result\n\nfrom dlgwas.dlfn import calc_cs\nfrom dlgwas.dlfn import apply_cs\nfrom dlgwas.dlfn import reverse_cs\n\nfrom dlgwas.dlfn import TianEtAl2011Dataset\nfrom dlgwas.dlfn import train_loop\nfrom dlgwas.dlfn import train_error\nfrom dlgwas.dlfn import test_loop\nfrom dlgwas.dlfn import train_nn\nfrom dlgwas.dlfn import yhat_loop\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nUsing cuda device"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep3resblocks.html#load-cleaned-data",
    "href": "tianetal2011_model_g2pdeep3resblocks.html#load-cleaned-data",
    "title": "Tian et al. 2011 Model 4 (G2PDeep Inspired, Smaller)",
    "section": "Load Cleaned Data",
    "text": "Load Cleaned Data\n\n# Read in cleaned data\ntaxa_groupings = pd.read_csv('../models/10_TianEtAl2011/taxa_groupings.csv')\ndata           = pd.read_csv('../models/10_TianEtAl2011/clean_data.csv')\n\n# Define holdout sets (Populations)\nuniq_pop = list(set(taxa_groupings['Population']))\nprint(str(len(uniq_pop))+\" Unique Holdout Groups.\")\ntaxa_groupings['Holdout'] = None\nfor i in range(len(uniq_pop)):\n    mask = (taxa_groupings['Population'] == uniq_pop[i])\n    taxa_groupings.loc[mask, 'Holdout'] = i\n\ntaxa_groupings\n\n25 Unique Holdout Groups.\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nsample\nPopulation\nHoldout\n\n\n\n\n0\n0\nZ001E0001\nB73 x B97\n15\n\n\n1\n1\nZ001E0002\nB73 x B97\n15\n\n\n2\n2\nZ001E0003\nB73 x B97\n15\n\n\n3\n3\nZ001E0004\nB73 x B97\n15\n\n\n4\n4\nZ001E0005\nB73 x B97\n15\n\n\n...\n...\n...\n...\n...\n\n\n4671\n4671\nZ026E0196\nB73 x Tzi8\n10\n\n\n4672\n4672\nZ026E0197\nB73 x Tzi8\n10\n\n\n4673\n4673\nZ026E0198\nB73 x Tzi8\n10\n\n\n4674\n4674\nZ026E0199\nB73 x Tzi8\n10\n\n\n4675\n4675\nZ026E0200\nB73 x Tzi8\n10\n\n\n\n\n4676 rows × 4 columns"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep3resblocks.html#setup-holdouts",
    "href": "tianetal2011_model_g2pdeep3resblocks.html#setup-holdouts",
    "title": "Tian et al. 2011 Model 4 (G2PDeep Inspired, Smaller)",
    "section": "Setup Holdouts",
    "text": "Setup Holdouts\n\n#randomly holdout a population if there is not a file with the population held out.\n# Holdout_Int = 0\nHoldout_Int_path = '../models/'+nb_name+'/holdout_pop_int.pkl'\nif None != get_cached_result(Holdout_Int_path):\n    Holdout_Int = get_cached_result(Holdout_Int_path)\nelse:\n    Holdout_Int = int(np.random.choice([i for i in range(len(uniq_pop))], 1))\n    put_cached_result(Holdout_Int_path, Holdout_Int)\n\n    \nprint(\"Holding out i=\"+str(Holdout_Int)+\": \"+uniq_pop[Holdout_Int])\n\nmask = (taxa_groupings['Holdout'] == Holdout_Int)\ntrain_idxs = list(taxa_groupings.loc[~mask, ].index)\ntest_idxs = list(taxa_groupings.loc[mask, ].index)\n\nHolding out i=1: B73 x CML52\n\n\n\n# downsample_obs = True\n# train_n = 900\n# test_n = 100\n\nif downsample_obs == True:\n    train_idxs = np.random.choice(train_idxs, train_n)\n    test_idxs = np.random.choice(test_idxs, test_n)\n    print([len(e) for e in [test_idxs, train_idxs]])\n    \n# used to go from index in tensor to index in data so that the right xs tensor can be loaded in\nidx_original = np.array(data.index)\n\ny1 = data['leaf_length']\ny2 = data['leaf_width']\ny3 = data['upper_leaf_angle']\ny1 = np.array(y1)\ny2 = np.array(y2)\ny3 = np.array(y3)\n\n\nScale data\n\nscale_dict_path = '../models/'+nb_name+'/scale_dict.pkl'\nif None != get_cached_result(scale_dict_path):\n    scale_dict = get_cached_result(scale_dict_path)\nelse:\n    scale_dict = {\n        'y1':calc_cs(y1[train_idxs]),\n        'y2':calc_cs(y2[train_idxs]),\n        'y3':calc_cs(y3[train_idxs])\n    }\n    put_cached_result(scale_dict_path, scale_dict)\n\ny1 = apply_cs(y1, scale_dict['y1'])\ny2 = apply_cs(y2, scale_dict['y2'])\ny3 = apply_cs(y3, scale_dict['y3'])"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep3resblocks.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "href": "tianetal2011_model_g2pdeep3resblocks.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "title": "Tian et al. 2011 Model 4 (G2PDeep Inspired, Smaller)",
    "section": "Allow for cycling data onto and off of GPU",
    "text": "Allow for cycling data onto and off of GPU\n\n# loading this into memory causes the session to crash\n\ny1_train = torch.from_numpy(y1[train_idxs])[:, None]\ny2_train = torch.from_numpy(y2[train_idxs])[:, None]\ny3_train = torch.from_numpy(y3[train_idxs])[:, None]\n\nidx_original_train = torch.from_numpy(idx_original[train_idxs])\n\ny1_test = torch.from_numpy(y1[test_idxs])[:, None]\ny2_test = torch.from_numpy(y2[test_idxs])[:, None]\ny3_test = torch.from_numpy(y3[test_idxs])[:, None]\n\nidx_original_test = torch.from_numpy(idx_original[test_idxs])\n\n\n# dataloader_batch_size = 64\n\ntraining_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_train,\n        y2 = y2_train,\n        y3 = y3_train,\n        idx_original = idx_original_train,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\ntesting_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_test,\n        y2 = y2_test,\n        y3 = y3_test,\n        idx_original = idx_original_test,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\nUsing cuda device\nUsing cuda device"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep3resblocks.html#non-boilerplate",
    "href": "tianetal2011_model_g2pdeep3resblocks.html#non-boilerplate",
    "title": "Tian et al. 2011 Model 4 (G2PDeep Inspired, Smaller)",
    "section": "Non-Boilerplate",
    "text": "Non-Boilerplate\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()    \n\n        # Block 1 ------------------------------------------------------------\n        self.long_way_0 = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 4, # second channel\n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride= 2,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Conv1d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride= 1,\n                    padding = 1,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Dropout(p=0.75)\n            )\n        \n        self.shortcut_0 = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride= 2,\n                    bias = True\n                )\n        )\n        # Block 2 ------------------------------------------------------------\n        self.long_way_1 = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 4, # second channel\n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride= 2,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Conv1d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride= 1,\n                    padding = 1,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Dropout(p=0.75)\n            )\n        \n        self.shortcut_1 = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride= 2,\n                    bias = True\n                )\n        )\n        # Block 3 ------------------------------------------------------------\n        self.long_way_2 = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 4, # second channel\n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride= 2,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Conv1d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride= 1,\n                    padding = 1,\n                    bias = True\n                ),\n#             nn.BatchNorm1d(4),\n            nn.Dropout(p=0.75)\n            )\n        \n        self.shortcut_2 = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride= 2,\n                    bias = True\n                )\n        )        \n        \n        \n        self.feature_processing = nn.Sequential(\n            nn.Conv1d(\n                    in_channels= 4, \n                    out_channels= 4,\n                    kernel_size= 3,\n                    stride = 2,\n                    bias = True\n                ),\n            nn.Dropout(p=0.75)\n        )\n\n        self.output_processing = nn.Sequential(\n            nn.Flatten(),\n            nn.ReLU(), # They used inverse square root activation $y = \\frac{x}{\\sqrt{1+ax^2}}$\n            nn.Dropout(p=0.75),\n            nn.Linear(235860, 1)\n        )            \n        \n    def forward(self, x):\n        x_out = self.long_way_0(x)\n        x_shortcut = self.shortcut_0(x)\n        x_out += x_shortcut\n        \n        x = x_out\n        x_out = self.long_way_1(x)\n        x_shortcut = self.shortcut_1(x)\n        x_out += x_shortcut\n        \n        x = x_out\n        x_out = self.long_way_2(x)\n        x_shortcut = self.shortcut_2(x)        \n        x_out += x_shortcut\n        \n        x_out = self.feature_processing(x_out)\n        x_out = self.output_processing(x_out)\n        \n        return x_out\n    \n# model = NeuralNetwork().to(device)\n\n# res = model(xs_i) # try prediction on one batch\n# res.shape\n\n\ndef train_nn(\n    nb_name,\n    training_dataloader,\n    testing_dataloader,\n    model,\n    learning_rate = 1e-3,\n    batch_size = 64,\n    epochs = 500\n):\n    # Initialize the loss function\n    loss_fn = nn.MSELoss()\n#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    # Optimizer with L2 normalization\n    optimizer = torch.optim.Adam([\n        {'params':model.long_way_0.parameters(), 'weight_decay': 0.1},\n        {'params':model.shortcut_0.parameters(), 'weight_decay': 0.1},\n        {'params':model.long_way_1.parameters(), 'weight_decay': 0.1},\n        {'params':model.shortcut_1.parameters(), 'weight_decay': 0.1},\n        {'params':model.long_way_2.parameters(), 'weight_decay': 0.1},\n        {'params':model.shortcut_2.parameters(), 'weight_decay': 0.1},\n        {'params':model.feature_processing.parameters(),        'weight_decay': 0.1},\n        {'params':model.output_processing.parameters(),         'weight_decay': 0.01},\n    ], lr=learning_rate)\n\n    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n    loss_df['TrainMSE'] = np.nan\n    loss_df['TestMSE']  = np.nan\n\n    for t in tqdm(range(epochs)):        \n#         print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n\n        loss_df.loc[loss_df.index == t, 'TrainMSE'\n                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n        \n        loss_df.loc[loss_df.index == t, 'TestMSE'\n                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n        \n        if (t+1)%10: # Cache in case training is interupted\n#             print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n            torch.save(model.state_dict(), \n                       '../models/'+nb_name+'/model_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n            loss_df.to_csv('../reports/'+nb_name+'/loss_df'+str(t)+'_'+str(epochs)+'.csv', index=False) \n        \n    return([model, loss_df])\n\n\n# don't run if either of these exist because there may be cases where we want the results but not the model\n\nif not os.path.exists('../models/'+nb_name+'/model.pt'): \n    # Shared setup (train from scratch and load latest)\n    model = NeuralNetwork()\n\n    # find the biggest model to save\n    saved_models = os.listdir('../models/'+nb_name+'/')\n    saved_models = [e for e in saved_models if re.match('model*', e)]\n\n    if saved_models == []:\n        epochs_run = 0\n    else:\n        # if there are saved models reload and resume training\n        saved_models_numbers = [int(e.replace('model_', ''\n                                    ).replace('.pt', ''\n                                    ).split('_')[0]) for e in saved_models]\n        # saved_models\n        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n        latest_model = [e for e in saved_models if re.match(\n            '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n        model.load_state_dict(torch.load('../models/'+nb_name+'/'+latest_model))\n        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n    \n    model.to(device) \n\n    model, loss_df = train_nn(\n        nb_name,\n        training_dataloader,\n        testing_dataloader,\n        model,\n        learning_rate = 1e-3,\n        batch_size = dataloader_batch_size,\n        epochs = (run_epochs - epochs_run)\n    )\n    \n    # experimental outputs:\n    # 1. Model\n    torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n\n    # 2. loss_df\n    loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)  \n    \n    \n    # 3. predictions \n    yhats = pd.concat([\n        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n\n    yhats.to_csv('../reports/'+nb_name+'/yhats.csv', index=False)\n\n\nNeuralNetwork()\n\nNeuralNetwork(\n  (long_way_0): Sequential(\n    (0): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (1): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n    (2): Dropout(p=0.75, inplace=False)\n  )\n  (shortcut_0): Sequential(\n    (0): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n  )\n  (long_way_1): Sequential(\n    (0): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (1): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n    (2): Dropout(p=0.75, inplace=False)\n  )\n  (shortcut_1): Sequential(\n    (0): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n  )\n  (long_way_2): Sequential(\n    (0): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (1): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n    (2): Dropout(p=0.75, inplace=False)\n  )\n  (shortcut_2): Sequential(\n    (0): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n  )\n  (feature_processing): Sequential(\n    (0): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (1): Dropout(p=0.75, inplace=False)\n  )\n  (output_processing): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): ReLU()\n    (2): Dropout(p=0.75, inplace=False)\n    (3): Linear(in_features=235860, out_features=1, bias=True)\n  )\n)"
  },
  {
    "objectID": "tianetal2011_model_g2pdeep3resblocks.html#standard-visualizations",
    "href": "tianetal2011_model_g2pdeep3resblocks.html#standard-visualizations",
    "title": "Tian et al. 2011 Model 4 (G2PDeep Inspired, Smaller)",
    "section": "Standard Visualizations",
    "text": "Standard Visualizations\n\nloss_df = pd.read_csv('../reports/'+nb_name+'/loss_df.csv')\n\nloss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\nloss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n                    mode='lines', name='Test'))\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n                    mode='lines', name='Train'))\nfig.show()\n\n\n                                                \n\n\n\nyhats = pd.read_csv('../reports/'+nb_name+'/yhats.csv')\n\nyhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\nyhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n\npx.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")\n\n\n                                                \n\n\n\nyhats['Error'] = yhats.y_true - yhats.y_pred\n\npx.histogram(yhats, x = 'Error', color = 'Split',\n             marginal=\"box\", # can be `rug`, `violin`\n             nbins= 50)\n\n\n                                                \n\n\n\n# automatically kill kernel after running. \n# This is a hacky way to free up _all_ space on the gpus\n# os._exit(00)"
  },
  {
    "objectID": "tianetal2011_model_simple_cnn1d.html",
    "href": "tianetal2011_model_simple_cnn1d.html",
    "title": "Tian et al. 2011 Model 2",
    "section": "",
    "text": "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n#                          Hours to wait\n# import time; time.sleep( 24 * (60*60))\n# Run Settings:\nnb_name = '12_TianEtAl2011'# Set manually! -----------------------------------\n\ndownsample_obs = False\ntrain_n = 90\ntest_n = 10\n\ndataloader_batch_size = 16 #64\nrun_epochs = 200\n\nuse_gpu_num = 0\n\n# Imports --------------------------------------------------------------------\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nimport tqdm\nfrom tqdm import tqdm\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n\nimport dlgwas\nfrom dlgwas.kegg import ensure_dir_path_exists\nfrom dlgwas.kegg import get_cached_result\nfrom dlgwas.kegg import put_cached_result\n\nfrom dlgwas.dlfn import calc_cs\nfrom dlgwas.dlfn import apply_cs\nfrom dlgwas.dlfn import reverse_cs\n\nfrom dlgwas.dlfn import TianEtAl2011Dataset\nfrom dlgwas.dlfn import train_loop\nfrom dlgwas.dlfn import train_error\nfrom dlgwas.dlfn import test_loop\nfrom dlgwas.dlfn import train_nn\nfrom dlgwas.dlfn import yhat_loop\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nUsing cuda device\n# use_gpu_num = 0 # This should change based on whichever gpu is free. \n#                 # If even notebooks are set to 0 then that will be a reasonable default. \n\n# import os\n# import pandas as pd\n# import numpy as np\n# import re\n\n# import torch\n# from torch.utils.data import Dataset\n# from torch.utils.data import DataLoader\n# from torch import nn\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# if use_gpu_num in [0, 1]: \n#     torch.cuda.set_device(use_gpu_num)\n# print(f\"Using {device} device\")\n\n# import tqdm\n# from tqdm import tqdm\n\n# import plotly.graph_objects as go\n# import plotly.express as px\n# import plotly.io as pio\n# pio.templates.default = \"plotly_white\"\n\n\n# import dlgwas\n# from dlgwas.kegg import ensure_dir_path_exists\n# from dlgwas.kegg import get_cached_result\n# from dlgwas.kegg import put_cached_result\n\n# from dlgwas.dlfn import calc_cs\n# from dlgwas.dlfn import apply_cs\n# from dlgwas.dlfn import reverse_cs\n# # set up directory for notebook artifacts\n# nb_name = '12_TianEtAl2011'\n# ensure_dir_path_exists(dir_path = '../models/'+nb_name)\n# ensure_dir_path_exists(dir_path = '../reports/'+nb_name)"
  },
  {
    "objectID": "tianetal2011_model_simple_cnn1d.html#load-cleaned-data",
    "href": "tianetal2011_model_simple_cnn1d.html#load-cleaned-data",
    "title": "Tian et al. 2011 Model 2",
    "section": "Load Cleaned Data",
    "text": "Load Cleaned Data\n\n# Read in cleaned data\ntaxa_groupings = pd.read_csv('../models/10_TianEtAl2011/taxa_groupings.csv')\ndata           = pd.read_csv('../models/10_TianEtAl2011/clean_data.csv')\n\n# Define holdout sets (Populations)\nuniq_pop = list(set(taxa_groupings['Population']))\nprint(str(len(uniq_pop))+\" Unique Holdout Groups.\")\ntaxa_groupings['Holdout'] = None\nfor i in range(len(uniq_pop)):\n    mask = (taxa_groupings['Population'] == uniq_pop[i])\n    taxa_groupings.loc[mask, 'Holdout'] = i\n\ntaxa_groupings\n\n25 Unique Holdout Groups.\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nsample\nPopulation\nHoldout\n\n\n\n\n0\n0\nZ001E0001\nB73 x B97\n22\n\n\n1\n1\nZ001E0002\nB73 x B97\n22\n\n\n2\n2\nZ001E0003\nB73 x B97\n22\n\n\n3\n3\nZ001E0004\nB73 x B97\n22\n\n\n4\n4\nZ001E0005\nB73 x B97\n22\n\n\n...\n...\n...\n...\n...\n\n\n4671\n4671\nZ026E0196\nB73 x Tzi8\n4\n\n\n4672\n4672\nZ026E0197\nB73 x Tzi8\n4\n\n\n4673\n4673\nZ026E0198\nB73 x Tzi8\n4\n\n\n4674\n4674\nZ026E0199\nB73 x Tzi8\n4\n\n\n4675\n4675\nZ026E0200\nB73 x Tzi8\n4\n\n\n\n\n4676 rows × 4 columns\n\n\n\n\n# # Read in cleaned data\n# taxa_groupings = pd.read_csv('../models/10_TianEtAl2011/taxa_groupings.csv')\n# data           = pd.read_csv('../models/10_TianEtAl2011/clean_data.csv')\n\n\n# # Define holdout sets (Populations)\n# uniq_pop = list(set(taxa_groupings['Population']))\n# print(str(len(uniq_pop))+\" Unique Holdout Groups.\")\n# taxa_groupings['Holdout'] = None\n# for i in range(len(uniq_pop)):\n#     mask = (taxa_groupings['Population'] == uniq_pop[i])\n#     taxa_groupings.loc[mask, 'Holdout'] = i\n\n# taxa_groupings"
  },
  {
    "objectID": "tianetal2011_model_simple_cnn1d.html#setup-holdouts",
    "href": "tianetal2011_model_simple_cnn1d.html#setup-holdouts",
    "title": "Tian et al. 2011 Model 2",
    "section": "Setup Holdouts",
    "text": "Setup Holdouts\n\n#randomly holdout a population if there is not a file with the population held out.\n# Holdout_Int = 0\nHoldout_Int_path = '../models/'+nb_name+'/holdout_pop_int.pkl'\nif None != get_cached_result(Holdout_Int_path):\n    Holdout_Int = get_cached_result(Holdout_Int_path)\nelse:\n    Holdout_Int = int(np.random.choice([i for i in range(len(uniq_pop))], 1))\n    put_cached_result(Holdout_Int_path, Holdout_Int)\n\n    \nprint(\"Holding out i=\"+str(Holdout_Int)+\": \"+uniq_pop[Holdout_Int])\n\nmask = (taxa_groupings['Holdout'] == Holdout_Int)\ntrain_idxs = list(taxa_groupings.loc[~mask, ].index)\ntest_idxs = list(taxa_groupings.loc[mask, ].index)\n\nHolding out i=13: B73 x CML52\n\n\n\n[len(e) for e in [test_idxs, train_idxs]]\n\n[196, 4480]\n\n\n\n# downsample_obs = True\n# train_n = 900\n# test_n = 100\n\nif downsample_obs == True:\n    train_idxs = np.random.choice(train_idxs, train_n)\n    test_idxs = np.random.choice(test_idxs, test_n)\n    print([len(e) for e in [test_idxs, train_idxs]])\n\n\n# used to go from index in tensor to index in data so that the right xs tensor can be loaded in\nidx_original = np.array(data.index)\n\ny1 = data['leaf_length']\ny2 = data['leaf_width']\ny3 = data['upper_leaf_angle']\ny1 = np.array(y1)\ny2 = np.array(y2)\ny3 = np.array(y3)\n\n\n# #randomly holdout a population if there is not a file with the population held out.\n# # Holdout_Int = 0\n# Holdout_Int_path = '../models/'+nb_name+'/holdout_pop_int.pkl'\n# if None != get_cached_result(Holdout_Int_path):\n#     Holdout_Int = get_cached_result(Holdout_Int_path)\n# else:\n#     Holdout_Int = int(np.random.choice([i for i in range(len(uniq_pop))], 1))\n#     put_cached_result(Holdout_Int_path, Holdout_Int)\n\n\n# print(\"Holding out i=\"+str(Holdout_Int)+\": \"+uniq_pop[Holdout_Int])\n\n# mask = (taxa_groupings['Holdout'] == Holdout_Int)\n# train_idxs = list(taxa_groupings.loc[~mask, ].index)\n# test_idxs = list(taxa_groupings.loc[mask, ].index)\n\n\n# # used to go from index in tensor to index in data so that the right xs tensor can be loaded in\n# idx_original = np.array(data.index)\n\n# y1 = data['leaf_length']\n# y2 = data['leaf_width']\n# y3 = data['upper_leaf_angle']\n# y1 = np.array(y1)\n# y2 = np.array(y2)\n# y3 = np.array(y3)\n\n\nScale data\n\nscale_dict_path = '../models/'+nb_name+'/scale_dict.pkl'\nif None != get_cached_result(scale_dict_path):\n    scale_dict = get_cached_result(scale_dict_path)\nelse:\n    scale_dict = {\n        'y1':calc_cs(y1[train_idxs]),\n        'y2':calc_cs(y2[train_idxs]),\n        'y3':calc_cs(y3[train_idxs])\n    }\n    put_cached_result(scale_dict_path, scale_dict)\n\ny1 = apply_cs(y1, scale_dict['y1'])\ny2 = apply_cs(y2, scale_dict['y2'])\ny3 = apply_cs(y3, scale_dict['y3'])\n\n\n# scale_dict = {\n#     'y1':calc_cs(y1[train_idxs]),\n#     'y2':calc_cs(y2[train_idxs]),\n#     'y3':calc_cs(y3[train_idxs])\n# }\n\n\n# y1 = apply_cs(y1, scale_dict['y1'])\n# y2 = apply_cs(y2, scale_dict['y2'])\n# y3 = apply_cs(y3, scale_dict['y3'])"
  },
  {
    "objectID": "tianetal2011_model_simple_cnn1d.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "href": "tianetal2011_model_simple_cnn1d.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "title": "Tian et al. 2011 Model 2",
    "section": "Allow for cycling data onto and off of GPU",
    "text": "Allow for cycling data onto and off of GPU\n\n# loading this into memory causes the session to crash\n\ny1_train = torch.from_numpy(y1[train_idxs])[:, None]\ny2_train = torch.from_numpy(y2[train_idxs])[:, None]\ny3_train = torch.from_numpy(y3[train_idxs])[:, None]\n\nidx_original_train = torch.from_numpy(idx_original[train_idxs])\n\ny1_test = torch.from_numpy(y1[test_idxs])[:, None]\ny2_test = torch.from_numpy(y2[test_idxs])[:, None]\ny3_test = torch.from_numpy(y3[test_idxs])[:, None]\n\nidx_original_test = torch.from_numpy(idx_original[test_idxs])\n\n\n# dataloader_batch_size = 64\n\ntraining_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_train,\n        y2 = y2_train,\n        y3 = y3_train,\n        idx_original = idx_original_train,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\ntesting_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n        y1 = y1_test,\n        y2 = y2_test,\n        y3 = y3_test,\n        idx_original = idx_original_test,\n        use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\nUsing cuda device\nUsing cuda device\n\n\n\n# # loading this into memory causes the session to crash\n\n# y1_train = torch.from_numpy(y1[train_idxs])[:, None]\n# y2_train = torch.from_numpy(y2[train_idxs])[:, None]\n# y3_train = torch.from_numpy(y3[train_idxs])[:, None]\n\n# idx_original_train = torch.from_numpy(idx_original[train_idxs])\n\n# y1_test = torch.from_numpy(y1[test_idxs])[:, None]\n# y2_test = torch.from_numpy(y2[test_idxs])[:, None]\n# y3_test = torch.from_numpy(y3[test_idxs])[:, None]\n\n# idx_original_test = torch.from_numpy(idx_original[test_idxs])\n\n\n# class CustomDataset(Dataset):\n#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#     if use_gpu_num in [0, 1]: \n#         torch.cuda.set_device(use_gpu_num)\n#     print(f\"Using {device} device\")\n    \n#     def __init__(self, y1, y2, y3, \n#                  idx_original,\n#                  transform = None, target_transform = None, \n#                  **kwargs\n#                 ):\n#         self.y1 = y1\n#         self.y2 = y2\n#         self.y3 = y3\n#         self.idx_original = idx_original\n#         self.transform = transform\n#         self.target_transform = target_transform    \n    \n#     def __len__(self):\n#         return len(self.y1)\n    \n#     def __getitem__(self, idx):\n#         y1_idx = self.y1[idx].to(device).float()\n#         y2_idx = self.y2[idx].to(device).float()\n#         y3_idx = self.y3[idx].to(device).float()\n        \n        \n#         # Change type of xs loaded !! ----------------------------------------\n#         # load in xs as they are needed.\n#         # Non-Hilbert Version\n        \n        \n#         save_path = '../models/10_TianEtAl2011/markers/'\n#         # Hilbert version\n#         # save_path = '../models/'+nb_name+'/hilbert/'\n#         save_file_path = save_path+'m'+str(int(self.idx_original[idx]))+'.npz'\n#         xs_idx = np.load(save_file_path)['arr_0']\n#         xs_idx = torch.from_numpy(xs_idx).to(device).float()\n#         xs_idx = xs_idx.squeeze()\n        \n#         # to match pytorch's conventions channel must be in the second dim\n#         xs_idx = torch.swapaxes(xs_idx, 0, 1) \n        \n#         if self.transform:\n#             xs_idx = self.transform(xs_idx)\n            \n#         if self.target_transform:\n#             y1_idx = self.transform(y1_idx)\n#             y2_idx = self.transform(y2_idx)\n#             y3_idx = self.transform(y3_idx)\n#         return xs_idx, y1_idx, y2_idx, y3_idx\n\n\n# training_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_train,\n#         y2 = y2_train,\n#         y3 = y3_train,\n#         idx_original = idx_original_train\n#     ), \n#     batch_size = 64, \n#     shuffle = True)\n\n# testing_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_test,\n#         y2 = y2_test,\n#         y3 = y3_test,\n#         idx_original = idx_original_train\n#     ), \n#     batch_size = 64, \n#     shuffle = True)\n\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n\n\n# del training_dataloader\n\n\n# torch.cuda.empty_cache()\n\n\n# xs_i.shape"
  },
  {
    "objectID": "tianetal2011_model_simple_cnn1d.html#non-boilerplate",
    "href": "tianetal2011_model_simple_cnn1d.html#non-boilerplate",
    "title": "Tian et al. 2011 Model 2",
    "section": "Non-Boilerplate",
    "text": "Non-Boilerplate\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()    \n        self.x_network = nn.Sequential(\n            nn.Conv1d(\n                in_channels= 4, # second channel\n                out_channels= 4,\n                kernel_size= 3,\n                stride= 2\n            ), \n            nn.MaxPool1d((3,), stride=2),\n            nn.Conv1d(\n                in_channels= 4, # second channel\n                out_channels= 4,\n                kernel_size= 3,\n                stride= 2\n            ), \n            nn.MaxPool1d((3,), stride=2),\n            nn.Conv1d(\n                in_channels= 4, # second channel\n                out_channels= 4,\n                kernel_size= 3,\n                stride= 2\n            ), \n            nn.MaxPool1d((3,), stride=2),\n            nn.Conv1d(\n                in_channels= 4, # second channel\n                out_channels= 4,\n                kernel_size= 3,\n                stride= 2\n            ), \n            nn.MaxPool1d((3,), stride=2),\n            nn.Conv1d(\n                in_channels= 4, # second channel\n                out_channels= 4,\n                kernel_size= 3,\n                stride= 2\n            ), \n            nn.MaxPool1d((3,), stride=2),\n            nn.Conv1d(\n                in_channels= 4, # second channel\n                out_channels= 4,\n                kernel_size= 3,\n                stride= 2\n            ), \n            nn.MaxPool1d((3,), stride=2),\n            nn.Conv1d(\n                in_channels= 4, # second channel\n                out_channels= 4,\n                kernel_size= 3,\n                stride= 2\n            ), \n            nn.MaxPool1d((3,), stride=2),\n            nn.Conv1d(\n                in_channels= 4, # second channel\n                out_channels= 4,\n                kernel_size= 3,\n                stride= 2\n            ), \n            nn.MaxPool1d((3,), stride=2),\n            \n            nn.Flatten(),\n            nn.Linear(52, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n                        \n            nn.Linear(32, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            \n            nn.Linear(32, 1)\n        )\n        \n    def forward(self, x):\n        x_out = self.x_network(x)\n        return x_out\n\n\n# model = NeuralNetwork().to(device)\n\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n# xs_i.shape\n\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(testing_dataloader))\n# xs_i.shape\n\n\n# model(xs_i)\n\n\n# model, loss_df = train_nn(\n#     nb_name,\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = dataloader_batch_size,\n#     epochs = run_epochs\n# )\n\n\n# don't run if either of these exist because there may be cases where we want the results but not the model\n\nif not os.path.exists('../models/'+nb_name+'/model.pt'): \n    # Shared setup (train from scratch and load latest)\n    model = NeuralNetwork()\n\n    # find the biggest model to save\n    saved_models = os.listdir('../models/'+nb_name+'/')\n    saved_models = [e for e in saved_models if re.match('model*', e)]\n\n    if saved_models == []:\n        epochs_run = 0\n    else:\n        # if there are saved models reload and resume training\n        saved_models_numbers = [int(e.replace('model_', ''\n                                    ).replace('.pt', ''\n                                    ).split('_')[0]) for e in saved_models]\n        # saved_models\n        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n        latest_model = [e for e in saved_models if re.match(\n            '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n        model.load_state_dict(torch.load('../models/'+nb_name+'/'+latest_model))\n        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n    \n    model.to(device)   \n\n    model, loss_df = train_nn(\n        nb_name,\n        training_dataloader,\n        testing_dataloader,\n        model,\n        learning_rate = 1e-3,\n        batch_size = dataloader_batch_size,\n        epochs = (run_epochs - epochs_run)\n    )\n    \n    # experimental outputs:\n    # 1. Model\n    torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n\n    # 2. loss_df\n    loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)  \n    \n    \n    # 3. predictions \n    yhats = pd.concat([\n        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n\n    yhats.to_csv('../reports/'+nb_name+'/yhats.csv', index=False)\n\n\nNeuralNetwork()\n\nNeuralNetwork(\n  (x_network): Sequential(\n    (0): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (1): MaxPool1d(kernel_size=(3,), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (3): MaxPool1d(kernel_size=(3,), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (5): MaxPool1d(kernel_size=(3,), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (7): MaxPool1d(kernel_size=(3,), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (8): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (9): MaxPool1d(kernel_size=(3,), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (11): MaxPool1d(kernel_size=(3,), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (13): MaxPool1d(kernel_size=(3,), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv1d(4, 4, kernel_size=(3,), stride=(2,))\n    (15): MaxPool1d(kernel_size=(3,), stride=2, padding=0, dilation=1, ceil_mode=False)\n    (16): Flatten(start_dim=1, end_dim=-1)\n    (17): Linear(in_features=52, out_features=32, bias=True)\n    (18): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU()\n    (20): Linear(in_features=32, out_features=32, bias=True)\n    (21): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU()\n    (23): Linear(in_features=32, out_features=1, bias=True)\n  )\n)"
  },
  {
    "objectID": "tianetal2011_model_simple_cnn1d.html#standard-visualizations",
    "href": "tianetal2011_model_simple_cnn1d.html#standard-visualizations",
    "title": "Tian et al. 2011 Model 2",
    "section": "Standard Visualizations",
    "text": "Standard Visualizations\n\nloss_df = pd.read_csv('../reports/'+nb_name+'/loss_df.csv')\n\nloss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\nloss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n                    mode='lines', name='Test'))\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n                    mode='lines', name='Train'))\nfig.show()\n\n\n                                                \n\n\n\nyhats = pd.read_csv('../reports/'+nb_name+'/yhats.csv')\n\nyhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\nyhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n\npx.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")\n\n\n                                                \n\n\n\nyhats['Error'] = yhats.y_true - yhats.y_pred\n\npx.histogram(yhats, x = 'Error', color = 'Split',\n             marginal=\"box\", # can be `rug`, `violin`\n             nbins= 50)\n\n\n                                                \n\n\n\n# automatically kill kernel after running. \n# This is a hacky way to free up _all_ space on the gpus\n# os._exit(00)"
  },
  {
    "objectID": "deep_learning_convenience_functions.html",
    "href": "deep_learning_convenience_functions.html",
    "title": "Deep Learning Convenience Functions",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "deep_learning_convenience_functions.html#boilerplate-functions-for-tian-et-al-2011",
    "href": "deep_learning_convenience_functions.html#boilerplate-functions-for-tian-et-al-2011",
    "title": "Deep Learning Convenience Functions",
    "section": "Boilerplate Functions for Tian et al 2011",
    "text": "Boilerplate Functions for Tian et al 2011\n\nsource\n\nTianEtAl2011Dataset\n\n TianEtAl2011Dataset (y1, y2, y3, idx_original, marker_type='markers',\n                      transform=None, target_transform=None,\n                      use_gpu_num=0, **kwargs)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny1\n\n\n\n\n\ny2\n\n\n\n\n\ny3\n\n\nxs,\n\n\nidx_original\n\n\n\n\n\nmarker_type\nstr\nmarkers\n\n\n\ntransform\nNoneType\nNone\n\n\n\ntarget_transform\nNoneType\nNone\n\n\n\nuse_gpu_num\nint\n0\n\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\ntrain_loop\n\n train_loop (dataloader, model, loss_fn, optimizer, silent=False)\n\n\nsource\n\n\ntrain_error\n\n train_error (dataloader, model, loss_fn, silent=False)\n\n\nsource\n\n\ntest_loop\n\n test_loop (dataloader, model, loss_fn, silent=False)\n\n\nsource\n\n\ntrain_nn\n\n train_nn (nb_name, training_dataloader, testing_dataloader, model,\n           learning_rate=0.001, batch_size=64, epochs=500)\n\n\nsource\n\n\nyhat_loop\n\n yhat_loop (dataloader, model)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dlgwas",
    "section": "",
    "text": "Ultimately this project will be installable via pip with the following command. Note that this is not yet implemented."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "dlgwas",
    "section": "Install",
    "text": "Install\npip install dlgwas"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "dlgwas",
    "section": "How to use",
    "text": "How to use\nThis project is currently pre-release. Instructions on use will be added in the future."
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "dlgwas",
    "section": "Project Structure",
    "text": "Project Structure\nThe files of primary interest in this project are notebooks, external data, processed data, models, and reports. Additional files are produced by nbdev for building documentation, testing, and other tasks. This project is intended to applicable to multiple organisms. This long term goal motivates encapsulating data relevant to a species in a species subfolder even though only Zea mays is considered at present.\nBelow is an outline to illustrate the project’s target structure.\n\nNotebooks (nbs)\n\nWhere possible, analysis will be done in jupyter notebooks.\nNotebooks are named in snake case with\n\nThe expected run order\nThe species if applicable (using KEGG naming conventions)\nA brief description\n\nDuplicate notebook numbers are allowed for now (e.g. 01_zma_kegg_download and 01_taes_kegg_download) for parallel tasks but reserving a block of notebook numbers may end up being better (e.g. 01-10 for zma, 11-20 for taes).\n\nExternal Data (ext_data)\n\nSubfolders for different species (Arabidopsis ath, Wheat taes, and Maize zma shown here)\nSubfolders may contain data from public databases (cyverse, panzea, kegg, etc.) or data from specific studies.\nStudy data should be named according to the citation (e.g. buckler_et_al_2009) rather than the repository that the data is stored in (e.g. figshare, zenodo, etc.).\n\nData (data)\n\nCleaned or otherwise transformed data from ext_data should be kept here.\nComputational artifacts (e.g. pickled objects) that are expensive to recompute should also be stored here.\nData storage isn’t set yet. It will either aim:\n\nTo make the origin of produced objects clear, folders will have names matching the notebooks that created them.\nTo make the use of produced objects easy, the folders will have names matching those in ext_data.\n\n\n\nModels (models)\n\nComputationally expensive models are to be saved here.\nFolders will have names matching the notebooks that created them\n\nReports (reports)\n\nFigures, tables, and other human readable artifacts are to be stored here.\nFolders will have names matching the notebooks that created them\n\n\nIllustrative Directory Structure:\n.\n├── nbs\n│   ├── 00_core.ipynb\n│   ├── 01_zma_kegg_download.ipynb\n│   ├── ...\n│   └── index.ipynb\n├── ext_data\n│   ├── ath\n│   ├── taes\n│   └── zma\n│       ├── buckler_et_al_2009\n│       ├── e2p2_computed\n│       ├── ensemble\n│       ├── kegg\n│       ├── panzea\n│       └── plant_reactome\n├── data\n│   ├── zma                  ?\n│   │   └── ...\n│   └── 01_zma_kegg_download ?\n│       └── ...\n├── models\n│   └── 01_zma_kegg_download\n│       └── ...\n└── reports\n    └── 01_zma_kegg_download\n        └── ..."
  },
  {
    "objectID": "tianetal2011.html",
    "href": "tianetal2011.html",
    "title": "Tian et al. 2011",
    "section": "",
    "text": "use_gpu_num = 1\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\n\n# import torch\n# from torch.utils.data import Dataset\n# from torch.utils.data import DataLoader\n# from torch import nn\n\n# TODO fixme\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# if use_gpu_num in [0, 1]: \n#     torch.cuda.set_device(use_gpu_num)\n# print(f\"Using {device} device\")\n\nimport tqdm\nfrom tqdm import tqdm\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# [e for e in os.listdir() if re.match(\".+\\\\.txt\", e)]\nimport dlgwas\nfrom dlgwas.dna import *\n\nfrom dlgwas.kegg import ensure_dir_path_exists\nfrom dlgwas.kegg import get_cached_result\nfrom dlgwas.kegg import put_cached_result\n# set up directory for notebook artifacts\nnb_name = '10_TianEtAl2011'\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)"
  },
  {
    "objectID": "tianetal2011.html#load-data",
    "href": "tianetal2011.html#load-data",
    "title": "Tian et al. 2011",
    "section": "Load Data",
    "text": "Load Data\n\nwith open('../ext_data/zma/panzea/phenotypes/Tian_etal_2011_NatGen_leaf_pheno_data-110221/Tian_etal_2011_NatGen_readme.txt', \n          'r') as f:\n    dat = f.read()\nprint(dat)\n\nTian F, Bradbury PJ, Brown PJ, Hung H, Sun Q, Flint-Garcia S, Rocheford TR, McMullen MD, Holland JB, Buckler ES. 2011. Genome-wide association study of leaf architecture in the maize nested association mapping population. Nature Genetics 43. http://dx.doi.org/doi:10.1038/ng.746\n\n------------------------------------------------\nFrom: Feng Tian\nSent: Sunday, November 21, 2010 1:03 PM\nSubject: Leaf traits data\n\nThe file \"Tian_etal2011NatGenet.leaf_trait_phenotype.xlsx\" contains the phenotypes I used in the paper. In the paper, we used boxcox transformed upper leaf angle. This is included in the file. Before I started mapping, I removed 4 obvious outlier data points from the raw BLUP data from Jim (set them as missing): \nleaf length and width of Z002E0060\nUpper leaf angle of Z017E0082 and Z022E0007\n    \nFeng Tian, Ph.D. \nPost-doctoral Associate\nCornell University\nInstitute for Genomic Diversity\n175 Biotechnology Building\nIthaca, NY 14853-2703\n \nEmail:ft55@cornell.edu\n\n\n\n\n\n\ndata = pd.read_excel('../ext_data/zma/panzea/phenotypes/Tian_etal_2011_NatGen_leaf_pheno_data-110221/Tian_etal2011NatGenet.leaf_trait_phenotype.xlsx')\ndata\n\n\n\n\n\n\n\n\nsample\npop\nleaf_length\nleaf_width\nupper_leaf_angle\nleaf_angle_boxcox_transformed\n\n\n\n\n0\nZ001E0001\n1\n850.6304\n88.0488\n65.3152\n9.620754e+06\n\n\n1\nZ001E0002\n1\n654.2202\n95.8449\n59.8256\n6.659548e+06\n\n\n2\nZ001E0003\n1\n836.4517\n93.4534\n66.1322\n1.013518e+07\n\n\n3\nZ001E0004\n1\n595.5967\n100.3453\n66.3374\n1.026761e+07\n\n\n4\nZ001E0005\n1\n822.9404\n95.9405\n76.4436\n1.860027e+07\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4887\nMO380\n17\n676.5082\n82.9771\n68.6168\n1.182904e+07\n\n\n4888\nMO381\n17\n624.0939\n77.0269\n62.5098\n8.004169e+06\n\n\n4889\nMO382\n17\n669.3213\n81.5181\n62.0530\n7.761913e+06\n\n\n4890\nMO383\n17\n693.4092\n91.2571\n76.6321\n1.879322e+07\n\n\n4891\nMO384\n17\n695.5563\n80.4511\n65.9832\n1.003983e+07\n\n\n\n\n4892 rows × 6 columns"
  },
  {
    "objectID": "tianetal2011.html#find-marker-data-to-use-along-with-the-phenotypic-data-here",
    "href": "tianetal2011.html#find-marker-data-to-use-along-with-the-phenotypic-data-here",
    "title": "Tian et al. 2011",
    "section": "Find Marker data to use along with the phenotypic data here",
    "text": "Find Marker data to use along with the phenotypic data here\n\nsamples = list(set(data['sample']))\n\n\n# this can take a while to calculate so it's worth cacheing\nsave_path = '../models/'+nb_name+'/samples_and_matches.pkl'\n\nsamples_and_matches = get_cached_result(save_path=save_path)\n\nif None == samples_and_matches:\n    samples_and_matches = [{\n        'sample': sample,\n        'matches': find_AGPv4(taxa = sample) } for sample in tqdm.tqdm(samples)]\n\n    put_cached_result(\n        save_path = save_path,\n        save_obj = samples_and_matches\n    )\n\n\n#TODO: Some of these samples have multiple possible matches. \n# For the time being I'm usign the first one.\n[e for e in samples_and_matches if len(e['matches']) &gt; 1][0:10]\n\n[{'sample': 'Z006E0119',\n  'matches': ['Z006E0119__250065007', 'Z006E0119__250022599']},\n {'sample': 'Z025E0162',\n  'matches': ['Z025E0162__250183159', 'Z025E0162__250027524']},\n {'sample': 'Z006E0023',\n  'matches': ['Z006E0023__250022411', 'Z006E0023__250178848']},\n {'sample': 'Z001E0045',\n  'matches': ['Z001E0045__250079822', 'Z001E0045__250021154']},\n {'sample': 'Z023E0068',\n  'matches': ['Z023E0068__250026869', 'Z023E0068__250080182']},\n {'sample': 'Z024E0154',\n  'matches': ['Z024E0154__250027303', 'Z024E0154__250183143']},\n {'sample': 'Z018E0103',\n  'matches': ['Z018E0103__250025674', 'Z018E0103__250179096']},\n {'sample': 'Z001E0039',\n  'matches': ['Z001E0039__250021192', 'Z001E0039__250079845']},\n {'sample': 'Z009E0109',\n  'matches': ['Z009E0109__250023292', 'Z009E0109__250080072']},\n {'sample': 'Z019E0083',\n  'matches': ['Z019E0083__250028399', 'Z019E0083__250179105']}]\n\n\n\nsamples_one_match = [e for e in samples_and_matches if len(e['matches']) == 1]\n\nprint(\"Warning: \"+str(len(samples_and_matches)-len(samples_one_match)\n    )+\" samples (\"+str(round(100*((len(samples_and_matches)-len(samples_one_match))/len(samples_and_matches))) \n    )+\"%) have zero matches or more than one match in AGPv4. The first is being used.\")\n\nWarning: 1045 samples (21%) have zero matches or more than one match in AGPv4. The first is being used."
  },
  {
    "objectID": "tianetal2011.html#filter-data-to-only-those-with-unambiguous-genotypes",
    "href": "tianetal2011.html#filter-data-to-only-those-with-unambiguous-genotypes",
    "title": "Tian et al. 2011",
    "section": "Filter data to only those with unambiguous genotypes",
    "text": "Filter data to only those with unambiguous genotypes\n\noriginal_rows = data.shape[0]\n\n# mask to restrict to only those samples with one or more GBS marker set in AGPv4\nmask = [True if e in [e1['sample'] for e1 in \n                      [e for e in samples_and_matches if len(e['matches']) &gt;= 1]\n              ] else False for e in data['sample'] ]\ndata = data.loc[mask,].reset_index().drop(columns = 'index')\nprint(str(original_rows - data.shape[0])+' rows dropped.')\n\n209 rows dropped.\n\n\n\n# repeat for data completeness:\noriginal_rows = data.shape[0]\n# exclude rows with missing values in y\nmask = (data.leaf_length.notna() &                    \n        data.leaf_width.notna() &                     \n        data.upper_leaf_angle.notna())\n\ndata = data.loc[mask,].reset_index().drop(columns = 'index')\nprint(str(original_rows - data.shape[0])+' rows dropped.')\n\n7 rows dropped.\n\n\n\n# ys = data.loc[:, ['leaf_length', 'leaf_width', 'upper_leaf_angle']]\n\n# geno_sample = data.loc[:, 'sample']\n# pop_sample  = data.loc[:, 'pop']\n\n\n# Sample withing Group\ndata.assign(n = 1).groupby(['pop', 'sample']).agg(nsum = ('n', np.mean)).reset_index().sort_values('nsum')\n\n\n\n\n\n\n\n\npop\nsample\nnsum\n\n\n\n\n0\n1\nZ001E0001\n1.0\n\n\n3121\n18\nZ018E0132\n1.0\n\n\n3120\n18\nZ018E0131\n1.0\n\n\n3119\n18\nZ018E0130\n1.0\n\n\n3118\n18\nZ018E0129\n1.0\n\n\n...\n...\n...\n...\n\n\n1555\n9\nZ009E0025\n1.0\n\n\n1554\n9\nZ009E0024\n1.0\n\n\n1553\n9\nZ009E0023\n1.0\n\n\n1569\n9\nZ009E0039\n1.0\n\n\n4675\n26\nZ026E0200\n1.0\n\n\n\n\n4676 rows × 3 columns\n\n\n\n\n# But sample is also usable as a uid\ndata.assign(n = 1).groupby(['sample']).agg(nsum = ('n', np.mean)).reset_index().sort_values('nsum')\n\n\n\n\n\n\n\n\nsample\nnsum\n\n\n\n\n0\nZ001E0001\n1.0\n\n\n3121\nZ018E0132\n1.0\n\n\n3120\nZ018E0131\n1.0\n\n\n3119\nZ018E0130\n1.0\n\n\n3118\nZ018E0129\n1.0\n\n\n...\n...\n...\n\n\n1555\nZ009E0025\n1.0\n\n\n1554\nZ009E0024\n1.0\n\n\n1553\nZ009E0023\n1.0\n\n\n1569\nZ009E0039\n1.0\n\n\n4675\nZ026E0200\n1.0\n\n\n\n\n4676 rows × 2 columns"
  },
  {
    "objectID": "tianetal2011.html#retrieve-marker-data",
    "href": "tianetal2011.html#retrieve-marker-data",
    "title": "Tian et al. 2011",
    "section": "Retrieve Marker data",
    "text": "Retrieve Marker data\n\nInitial approach: Convert lists to arrays\nThe original approach to converting marker lists to np arrays was straighforward but required looping over the marker lists such that it takes about a second per sample. This puts the conversion at ~1h.\nn_samples = len(list(data['sample']))\n\nmarkers = np.zeros(shape = (\n    n_samples, \n    len(get_AGPv4(taxa = 'taxa'))-1, # don't include taxa\n    4\n))\n\nfor i in tqdm(range(n_samples)):\n    search_taxa = data['sample'][i]\n    markers[i, :, :] = list_to_ACGT(\n            in_seq = get_AGPv4(\n                taxa = find_AGPv4(\n                    taxa = search_taxa)[0] \n            )[1:]\n        )\n\nn_samples = len(list(data['sample']))\n\nmarkers = np.zeros(shape = (\n    n_samples, \n    len(get_AGPv4(taxa = 'taxa'))-1, # don't include taxa\n    4\n))\n\n\n\nimport time\ntimes_list = []\n\nfor i in tqdm(range(2)):\n    times = []\n    times += [time.time()]\n    search_taxa = data['sample'][i]\n    times += [time.time()] #---- 0\n    aa = find_AGPv4(taxa = search_taxa)[0]\n    times += [time.time()] #---- 1\n    bb = get_AGPv4(taxa =  aa)[1:]\n    times += [time.time()] #---- 2\n    cc = list_to_ACGT(in_seq = bb) # &lt;-- This is where almost all of the time is coming from\n    times += [time.time()] #---- 3\n    markers[i, :, :] = cc\n    times += [time.time()] #---- 4\n    times_list += [times]\n\n100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02&lt;00:00,  1.05s/it]\n\n\n\ndiff_times = []\nfor times in times_list:\n    diff_times += [ [times[i+1]-times[i] for i in range(len(times)-1)]]\n\n\npx.imshow(np.asarray(diff_times))\n\n\n                                                \n\n\n\n\nImproved approach: convert dataframes to arrays\nIn addition to caching the data, alternate approaches are available. Here I tested a function that would work off of a dataframe rather than a list.\n\n# What about a dataframe based version of list_to_ACGT?\n\ndef df_to_ACGT(\n    in_df, # This should be a dataframe containing samples and SNPs\n    sample_axis, # this is an int with the axis of the samples. If samples are not in the 0th axis they will be swapped and returned there.\n    progress = False,\n    silent = False\n):\n    # Note! in_df may have samples second! if so then.\n    irows, jcols = in_df.shape\n\n    # Convert IUPAC codes into pr ACGT -------------------------------------------\n    encode_dict = {\n        #     https://www.bioinformatics.org/sms/iupac.html\n        #     A     C     G     T\n        'A': [1,    0,    0,    0   ],\n        'C': [0,    1,    0,    0   ],\n        'G': [0,    0,    1,    0   ],\n        'T': [0,    0,    0,    1   ],\n        'K': [0,    0,    0.5,  0.5 ],\n        'M': [0.5,  0.5,  0,    0   ],\n        'N': [0.25, 0.25, 0.25, 0.25],\n        'R': [0.5,  0,    0.5,  0   ],\n        'S': [0,    0.5,  0.5,  0   ],\n        'W': [0.5,  0,    0,    0.5 ],\n        'Y': [0,    0.5,  0,    0.5 ],\n        #     Other values (assumed empty)\n        #     A     C     G     T\n    #      '': [0,    0,    0,    0   ],\n    #     '-': [0,    0,    0,    0   ],\n    #     '0': [0,    0,    0,    0   ],\n    }\n\n    # fix newline in last row\n    for j in range(jcols):\n        in_df[j] = in_df[j].str.replace('\\n', '')\n\n    not_in_dict = [e for e in set(in_df[j]) if e not in list(encode_dict.keys())]\n\n    if not_in_dict != []:\n        if silent != True:\n            print(\"Waring: The following are not in the encoding dictionary and will be set as missing.\\n\"+str(not_in_dict))\n\n    # output matrix\n    GMat = np.zeros(shape = [irows,\n                             jcols, \n                             4])\n    # convert all nucleotides to probabilities\n    if progress == True:\n        for nucleotide in tqdm(encode_dict.keys()):\n            mask = (in_df == nucleotide)\n            GMat[mask, :] = encode_dict[nucleotide] \n    else:\n        for nucleotide in encode_dict.keys():\n            mask = (in_df == nucleotide)\n            GMat[mask, :] = encode_dict[nucleotide] \n\n    # if needed rotate to have desired shape    \n    if sample_axis != 0:\n        GMat = np.swapaxes(GMat, 0, sample_axis)\n\n    return(GMat)\n\nConfirm that these are equivalent:\n\nsearch_taxa = data['sample'][i]\naa = find_AGPv4(taxa = search_taxa)[0]\nbb = get_AGPv4(taxa =  aa)[1:]\ncc = list_to_ACGT(in_seq = bb) \n\ndd = df_to_ACGT(\n    in_df = pd.DataFrame(bb),# This should be a dataframe containing samples and SNPs\n    sample_axis = 0, # this is an int with the axis of the samples. If samples are not in the 0th axis they will be swapped and returned there.\n    progress = False\n)\n\ncc.shape ==  dd.squeeze().shape\n\nWaring: The following are not in the encoding dictionary and will be set as missing.\n['-', '0']\n\n\nTrue\n\n\nCheck if this is faster. Previous version takes ~1sec/iter.\n\nj = 2\n\nvals = pd.concat([pd.DataFrame(get_AGPv4(taxa = find_AGPv4(taxa = data['sample'][i])[0])[1:]\n                       ) for i in tqdm(range(j))], axis = 1)\nvals.columns = [i for i in range(j)]\n\nvals.shape\n\nvals = df_to_ACGT(\n    in_df = vals,# This should be a dataframe containing samples and SNPs\n    sample_axis = 1# this is an int with the axis of the samples. If samples are not in the 0th axis they will be swapped and returned there.\n)\n\n100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 20.76it/s]\n\n\nWaring: The following are not in the encoding dictionary and will be set as missing.\n['-', '0']\n\n\nThis is slow as well. Still an improvement over but not an impressive one.\n\n# save_path = '../models/'+nb_name+'/markers/'\n\n# for e in os.listdir(save_path):\n#     np.savez_compressed(save_path+e[0:-1]+'z',\n#                         np.load(save_path+e))\n\n# # np.savez_compressed('../models/'+nb_name+'/markers/test_zip.npz', vals[0, :, :])\n\n\n# numpy.savez_compressed\n\n\n# Loading and processing is the time consuming part. \nsave_path = '../models/'+nb_name+'/markers/'\nensure_dir_path_exists(save_path)\n\nn_samples = len(list(data['sample']))\n\nfor i in  tqdm(range(n_samples)):\n    save_file_path = save_path+'m'+str(i)+'.npz'\n    if os.path.exists(save_file_path) == False:\n        markers = pd.DataFrame(get_AGPv4(taxa = find_AGPv4(taxa = data['sample'][i])[0])[1:])    \n\n        markers = df_to_ACGT(\n            in_df = markers,# This should be a dataframe containing samples and SNPs\n            sample_axis = 1,# this is an int with the axis of the samples. If samples are not in the 0th axis they will be swapped and returned there.\n            silent = True\n        )\n\n        np.savez_compressed(save_file_path, markers)\n# 4683/4683 [59:54&lt;00:00,  1.30it/s]\n\n100%|███████████████████████████████████████████████████████████████████████████| 4676/4676 [00:00&lt;00:00, 470663.92it/s]\n\n\n\n\nApply Hilbert Transform\n\n# This should be done on as many samples as possible at one time. \n# Transforming 1 sample and transforming 11 each take 7 and 7.74 seconds.\n# markers_hilbert = np_3d_to_hilbert(\n#     in_seq = np.load('../models/10_TianEtAl2011/markers/m0.npy')\n# )\n\n\n# load_path = '../models/'+nb_name+'/markers/'\n# save_path = '../models/'+nb_name+'/hilbert/'\n# ensure_dir_path_exists(save_path)\n\n# cached_markers = os.listdir(load_path)\n# cached_hilbert_markers = os.listdir(save_path)\n# # only process markers that haven't been transformed already\n# cached_markers = [ee for ee in cached_markers if ee not in ['m'+e[1:] for e in cached_hilbert_markers]]\n\n# # if need be, this can always be chunked\n# # process these as a large array for speed.\n# print('Applying Hilbert Transformation.')\n# markers_hilbert = np_3d_to_hilbert(\n#     in_seq = np.concatenate([np.load(load_path+e) for e in cached_markers])\n# )\n\n# # then save each sample separately so they can be loaded as needed.\n# print('Saving.')\n# for i in tqdm(range(len(cached_markers))):\n#     np.save(save_path+'h'+cached_markers[i][1:], markers_hilbert[i])\n\n\nchunk_size = 500\nimport math\n\nload_path = '../models/'+nb_name+'/markers/'\nsave_path = '../models/'+nb_name+'/hilbert/'\nensure_dir_path_exists(save_path)\n\ncached_markers = os.listdir(load_path)\ncached_hilbert_markers = os.listdir(save_path)\n# only process markers that haven't been transformed already\ncached_markers = [ee for ee in cached_markers if ee not in ['m'+e[1:] for e in cached_hilbert_markers]]\n\nn_chunks = math.ceil(len(cached_markers)/chunk_size)\nfor ith_chunk in range(n_chunks):\n\n    print(str(ith_chunk)+'/'+str(n_chunks))\n    chunk_start = ith_chunk*chunk_size\n    chunk_stop = min((ith_chunk+1)*chunk_size, \n                     len(cached_markers)+1 ) # Last chunk may not be evenly divisible\n\n\n    cached_marker_slice = cached_markers[chunk_start:chunk_stop]\n    # if need be, this can always be chunked. \n    # process these as a large array for speed.\n    print('Applying Hilbert Transformation.')\n\n    markers_hilbert = np_3d_to_hilbert(\n        #                                                v--- zipped np arrays use position (arr_0, arr_1, ...) if no name is passed\n        in_seq = np.concatenate([np.load(load_path+e)['arr_0'] for e in cached_marker_slice ]) \n    )\n\n    #then save each sample separately so they can be loaded as needed.\n    print('Saving.')\n    for i in tqdm(range(len(cached_marker_slice))):\n        np.savez_compressed(save_path+'h'+cached_marker_slice[i][1:-1]+'z', markers_hilbert[i])\n\n\n\nMisc\n\n# Useful for converting between the physical location and site\nAGPv4_site = pd.read_table('../data/zma/panzea/genotypes/GBS/v27/'+'ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_PositionList.txt')\nAGPv4_site.head()\n\n\n\n\n\n\n\n\nSite\nName\nChromosome\nPosition\n\n\n\n\n0\n0\nS1_6370\n1\n52399\n\n\n1\n1\nS1_8210\n1\n54239\n\n\n2\n2\nS1_8376\n1\n54405\n\n\n3\n3\nS1_9889\n1\n55917\n\n\n4\n4\nS1_9899\n1\n55927"
  },
  {
    "objectID": "tianetal2011.html#taxa-groupings",
    "href": "tianetal2011.html#taxa-groupings",
    "title": "Tian et al. 2011",
    "section": "Taxa Groupings",
    "text": "Taxa Groupings\n\ntaxa_groupings = pd.read_table('../data/zma/panzea/genotypes/GBS/v27/ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_TaxaList.txt')\ntaxa_groupings = taxa_groupings.loc[:, ['Taxa', 'Tassel4SampleName', 'Population']]\n\ntaxa_groupings[['sample', 'sample2']] = taxa_groupings['Taxa'].str.split(':', expand = True)\n\ntaxa_groupings = taxa_groupings.loc[:, ['sample', 'Population']].drop_duplicates()\n\n# Restrict to those in data\ntaxa_groupings = data[['sample']].merge(taxa_groupings, how = 'left')\ntaxa_groupings\n\n\n\n\n\n\n\n\nsample\nPopulation\n\n\n\n\n0\nZ001E0001\nB73 x B97\n\n\n1\nZ001E0002\nB73 x B97\n\n\n2\nZ001E0003\nB73 x B97\n\n\n3\nZ001E0004\nB73 x B97\n\n\n4\nZ001E0005\nB73 x B97\n\n\n...\n...\n...\n\n\n4671\nZ026E0196\nB73 x Tzi8\n\n\n4672\nZ026E0197\nB73 x Tzi8\n\n\n4673\nZ026E0198\nB73 x Tzi8\n\n\n4674\nZ026E0199\nB73 x Tzi8\n\n\n4675\nZ026E0200\nB73 x Tzi8\n\n\n\n\n4676 rows × 2 columns\n\n\n\n\ntemp = [e for e in list(set(taxa_groupings.Population))]\ntemp.sort()\ntemp\n\n['B73 x B97',\n 'B73 x CML103',\n 'B73 x CML228',\n 'B73 x CML247',\n 'B73 x CML277',\n 'B73 x CML322',\n 'B73 x CML333',\n 'B73 x CML52',\n 'B73 x CML69',\n 'B73 x Hp301',\n 'B73 x Il14H',\n 'B73 x Ki11',\n 'B73 x Ki3',\n 'B73 x Ky21',\n 'B73 x M162W',\n 'B73 x M37W',\n 'B73 x MS71',\n 'B73 x Mo18W',\n 'B73 x NC350',\n 'B73 x NC358',\n 'B73 x Oh43',\n 'B73 x Oh7B',\n 'B73 x P39',\n 'B73 x Tx303',\n 'B73 x Tzi8']\n\n\n\ntemp = taxa_groupings.copy()\ntemp['sample'] = 1\n\nfig = px.treemap(temp, \n                 path=[px.Constant(\"All Populations:\"), 'Population'], values='sample')\n# fig.update_traces(root_color=\"lightgrey\")\n# fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()\n\n\n                                                \n\n\n\n# fig = px.treemap(taxa_groupings.loc[:, ['Population', 'sample']], \n#                  path=[px.Constant(\"All\"), 'Population', 'sample'])\n# # fig.update_traces(root_color=\"lightgrey\")\n# fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n# fig.show()"
  },
  {
    "objectID": "tianetal2011.html#finalize-data",
    "href": "tianetal2011.html#finalize-data",
    "title": "Tian et al. 2011",
    "section": "Finalize Data",
    "text": "Finalize Data\n\n# Define holdout sets (Populations)\n\nuniq_pop = list(set(taxa_groupings['Population']))\nprint(str(len(uniq_pop))+\" Unique Holdout Groups.\")\ntaxa_groupings['Holdout'] = None\nfor i in range(len(uniq_pop)):\n    mask = (taxa_groupings['Population'] == uniq_pop[i])\n    taxa_groupings.loc[mask, 'Holdout'] = i\n\ntaxa_groupings\n\n25 Unique Holdout Groups.\n\n\n\n\n\n\n\n\n\nsample\nPopulation\nHoldout\n\n\n\n\n0\nZ001E0001\nB73 x B97\n1\n\n\n1\nZ001E0002\nB73 x B97\n1\n\n\n2\nZ001E0003\nB73 x B97\n1\n\n\n3\nZ001E0004\nB73 x B97\n1\n\n\n4\nZ001E0005\nB73 x B97\n1\n\n\n...\n...\n...\n...\n\n\n4671\nZ026E0196\nB73 x Tzi8\n9\n\n\n4672\nZ026E0197\nB73 x Tzi8\n9\n\n\n4673\nZ026E0198\nB73 x Tzi8\n9\n\n\n4674\nZ026E0199\nB73 x Tzi8\n9\n\n\n4675\nZ026E0200\nB73 x Tzi8\n9\n\n\n\n\n4676 rows × 3 columns\n\n\n\n\ntaxa_groupings.to_csv('../models/'+nb_name+'/taxa_groupings.csv')\n\n\n\ndata.to_csv('../models/'+nb_name+'/clean_data.csv')\n\n\nHoldout_Int = 0\nprint(\"Holding out: \"+uniq_pop[Holdout_Int])\n\nmask = (taxa_groupings['Holdout'] == Holdout_Int)\ntrain_idxs = list(taxa_groupings.loc[~mask, ].index)\ntest_idxs = list(taxa_groupings.loc[mask, ].index)\n\nHolding out: B73 x MS71\n\n\n\n# y1 = data['leaf_length']\n# y2 = data['leaf_width']\n# y3 = data['upper_leaf_angle']\n\n\nRetrieve xs\nCan we hold all the xs in memory? A ballpark estimate has the full marker dataset as 4.5 Gb. so let’s try it!\n\n# # Non-Hilbert Version\n# save_path = '../models/'+nb_name+'/markers/'\n# xs = np.zeros(shape = (len(y1), 943455, 4))\n\n# failed_idxs = []\n\n# for i in tqdm(range(len(y1))):\n#     save_file_path = save_path+'m'+str(i)+'.npy'\n#     if os.path.exists(save_file_path):\n#         xs[i, :, :] = np.load(save_file_path)\n#     else:\n#         failed_idxs += [i]\n# if failed_idxs != []:\n#     print(str(len(failed_idxs))+' indexes could not be retrieved. Examine `failed_idxs` for more information.')\n\n\n# # Hilbert version\n# save_path = '../models/'+nb_name+'/hilbert/'\n# xs = np.zeros(shape = (len(y1), 1024, 1024, 4))\n\n# failed_idxs = []\n\n# for i in tqdm(range(len(y1))):\n#     save_file_path = save_path+'h'+str(i)+'.npy'\n#     if os.path.exists(save_file_path):\n#         xs[i, :, :, :] = np.load(save_file_path)\n#     else:\n#         failed_idxs += [i]\n# if failed_idxs != []:\n#     print(str(len(failed_idxs))+' indexes could not be retrieved. Examine `failed_idxs` for more information.')\n\n\n# def calc_cs(x): return [np.mean(x, axis = 0), np.std(x, axis = 0)]\n\n# def apply_cs(xs, cs_dict_entry): return ((xs - cs_dict_entry[0]) / cs_dict_entry[0])\n\n# scale_dict = {\n#     'y1':calc_cs(y1[train_idxs]),\n#     'y2':calc_cs(y2[train_idxs]),\n#     'y3':calc_cs(y3[train_idxs])\n# }\n\n\n# y1 = apply_cs(y1, scale_dict['y1'])\n# y2 = apply_cs(y2, scale_dict['y2'])\n# y3 = apply_cs(y3, scale_dict['y3'])\n\n\n# y1_train = torch.from_numpy(y1[train_idxs]).to(device).float()[:, None]\n# y2_train = torch.from_numpy(y2[train_idxs]).to(device).float()[:, None]\n# y3_train = torch.from_numpy(y3[train_idxs]).to(device).float()[:, None]\n# xs_train = torch.from_numpy(xs[train_idxs]).to(device).float()\n\n# y1_test = torch.from_numpy(y1[test_idxs]).to(device).float()[:, None]\n# y2_test = torch.from_numpy(y2[test_idxs]).to(device).float()[:, None]\n# y3_test = torch.from_numpy(y3[test_idxs]).to(device).float()[:, None]\n# xs_test = torch.from_numpy(xs[test_idxs]).to(device).float()\n\n\n# # No need to cs xs\n\n\n# class CustomDataset(Dataset):\n#     def __init__(self, y1, y2, y3, xs, transform = None, target_transform = None):\n#         self.y1 = y1\n#         self.y2 = y2\n#         self.y3 = y3\n#         self.xs = xs\n#         self.transform = transform\n#         self.target_transform = target_transform    \n    \n#     def __len__(self):\n#         return len(self.y1)\n    \n#     def __getitem__(self, idx):\n#         y1_idx = self.y1[idx]\n#         y2_idx = self.y2[idx]\n#         y3_idx = self.y3[idx]\n#         xs_idx = self.xs[idx]\n        \n#         if self.transform:\n#             xs_idx = self.transform(xs_idx)\n            \n#         if self.target_transform:\n#             y1_idx = self.transform(y1_idx)\n#             y2_idx = self.transform(y2_idx)\n#             y3_idx = self.transform(y3_idx)\n#         return xs_idx, y1_idx, y2_idx, y3_idx\n\n\n# training_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_train,\n#         y2 = y2_train,\n#         y3 = y3_train,\n#         xs = xs_train\n#     ), \n#     batch_size = 64, \n#     shuffle = True)\n\n# testing_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_test,\n#         y2 = y2_test,\n#         y3 = y3_test,\n#         xs = xs_test\n#     ), \n#     batch_size = 64, \n#     shuffle = True)\n\n# xs.shape\n\n\n# data = pd.read_table('../ext_data/zma/panzea/phenotypes/Buckler_etal_2009_Science_flowering_time_data-090807/markergenotypes062508.txt', skiprows=1\n#                     ).reset_index().rename(columns = {'index': 'Geno_Code'})\n# data\n\n\n# px.scatter_matrix(data.loc[:, ['days2anthesis', 'days2silk', 'asi']])\n\n\n# d2a = np.array(data['days2anthesis'])\n# d2s = np.array(data['days2silk'])\n# asi = np.array(data['asi'])\n\n\n# xs = np.array(data.drop(columns = ['days2anthesis', 'days2silk', 'asi', 'pop', 'Geno_Code']))\n\n# n_obs = xs.shape[0]\n\n# np_seed = 9070707\n# rng = np.random.default_rng(np_seed)  # can be called without a seed\n\n# test_pr = 0.2\n\n# test_n = round(n_obs*test_pr)\n# idxs = np.linspace(0, n_obs-1, num = n_obs).astype(int)\n# rng.shuffle(idxs)\n\n# test_idxs = idxs[0:test_n]\n# train_idxs = idxs[test_n:-1]"
  },
  {
    "objectID": "tianetal2011.html#make-tensors",
    "href": "tianetal2011.html#make-tensors",
    "title": "Tian et al. 2011",
    "section": "Make tensors",
    "text": "Make tensors\n\n# y1_train = torch.from_numpy(y1[train_idxs]).to(device).float()[:, None]\n# y2_train = torch.from_numpy(y2[train_idxs]).to(device).float()[:, None]\n# y3_train = torch.from_numpy(y3[train_idxs]).to(device).float()[:, None]\n# xs_train = torch.from_numpy(xs[train_idxs]).to(device).float()\n\n# y1_test = torch.from_numpy(y1[test_idxs]).to(device).float()[:, None]\n# y2_test = torch.from_numpy(y2[test_idxs]).to(device).float()[:, None]\n# y3_test = torch.from_numpy(y3[test_idxs]).to(device).float()[:, None]\n# xs_test = torch.from_numpy(xs[test_idxs]).to(device).float()\n\n\n# class CustomDataset(Dataset):\n#     def __init__(self, y1, y2, y3, xs, transform = None, target_transform = None):\n#         self.y1 = y1\n#         self.y2 = y2\n#         self.y3 = y3\n#         self.xs = xs\n#         self.transform = transform\n#         self.target_transform = target_transform    \n    \n#     def __len__(self):\n#         return len(self.y1)\n    \n#     def __getitem__(self, idx):\n#         y1_idx = self.y1[idx]\n#         y2_idx = self.y2[idx]\n#         y3_idx = self.y3[idx]\n#         xs_idx = self.xs[idx]\n        \n#         if self.transform:\n#             xs_idx = self.transform(xs_idx)\n            \n#         if self.target_transform:\n#             y1_idx = self.transform(y1_idx)\n#             y2_idx = self.transform(y2_idx)\n#             y3_idx = self.transform(y3_idx)\n#         return xs_idx, y1_idx, y2_idx, y3_idx\n\n\n# training_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_train,\n#         y2 = y2_train,\n#         y3 = y3_train,\n#         xs = xs_train\n#     ), \n#     batch_size = 64, \n#     shuffle = True)\n\n# testing_dataloader = DataLoader(\n#     CustomDataset(\n#         y1 = y1_test,\n#         y2 = y2_test,\n#         y3 = y3_test,\n#         xs = xs_test\n#     ), \n#     batch_size = 64, \n#     shuffle = True)\n\n# xs.shape"
  },
  {
    "objectID": "tianetal2011.html#version-1-predict-y1-anthesis",
    "href": "tianetal2011.html#version-1-predict-y1-anthesis",
    "title": "Tian et al. 2011",
    "section": "Version 1, Predict y1 (Anthesis)",
    "text": "Version 1, Predict y1 (Anthesis)\n\n# class NeuralNetwork(nn.Module):\n#     def __init__(self):\n#         super(NeuralNetwork, self).__init__()    \n#         self.x_network = nn.Sequential(\n#             nn.Linear(1106, 64),\n#             nn.BatchNorm1d(64),\n#             nn.ReLU(),\n#             nn.Linear(64, 1))\n        \n#     def forward(self, x):\n#         x_out = self.x_network(x)\n#         return x_out\n\n# model = NeuralNetwork().to(device)\n# # print(model)\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n# model(xs_i).shape # try prediction on one batch\n\n# def train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n#     size = len(dataloader.dataset)\n#     for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n#         # Compute prediction and loss\n#         pred = model(xs_i)\n#         loss = loss_fn(pred, y1_i) # &lt;----------------------------------------\n\n#         # Backpropagation\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         if batch % 100 == 0:\n#             loss, current = loss.item(), batch * len(y1_i) # &lt;----------------\n#             if not silent:\n#                 print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n                \n# def train_error(dataloader, model, loss_fn, silent = False):\n#     size = len(dataloader.dataset)\n#     num_batches = len(dataloader)\n#     train_loss = 0\n\n#     with torch.no_grad():\n#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n#             pred = model(xs_i)\n#             train_loss += loss_fn(pred, y1_i).item() # &lt;----------------------\n            \n#     train_loss /= num_batches\n#     return(train_loss) \n\n            \n# def test_loop(dataloader, model, loss_fn, silent = False):\n#     size = len(dataloader.dataset)\n#     num_batches = len(dataloader)\n#     test_loss = 0\n\n#     with torch.no_grad():\n#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n#             pred = model(xs_i)\n#             test_loss += loss_fn(pred, y1_i).item() # &lt;-----------------------\n\n#     test_loss /= num_batches\n#     if not silent:\n#         print(f\"Test Error: Avg loss: {test_loss:&gt;8f}\")\n#     return(test_loss) \n\n\n# def train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 500\n# ):\n#     # Initialize the loss function\n#     loss_fn = nn.MSELoss()\n#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n#     loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n#     loss_df['TrainMSE'] = np.nan\n#     loss_df['TestMSE']  = np.nan\n\n#     for t in tqdm.tqdm(range(epochs)):\n#         # print(f\"Epoch {t+1}\\n-------------------------------\")\n#         train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n\n#         loss_df.loc[loss_df.index == t, 'TrainMSE'\n#                    ] = train_error(training_dataloader, model, loss_fn, silent = True)\n        \n#         loss_df.loc[loss_df.index == t, 'TestMSE'\n#                    ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n        \n#     return([model, loss_df])\n\n# model, loss_df = train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 500\n# )\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n#                     mode='lines', name='Train'))\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n#                     mode='lines', name='Test'))\n# fig.show()\n\n# # ! conda install captum -c pytorch -y\n\n\n# # imports from captum library\n# from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n# from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation\n\n\n\n# ig = IntegratedGradients(model)\n# ig_nt = NoiseTunnel(ig)\n# dl = DeepLift(model)\n# gs = GradientShap(model)\n# fa = FeatureAblation(model)\n\n# ig_attr_test = ig.attribute(xs_test, n_steps=50)\n# ig_nt_attr_test = ig_nt.attribute(xs_test)\n# dl_attr_test = dl.attribute(xs_test)\n# gs_attr_test = gs.attribute(xs_test, xs_train)\n# fa_attr_test = fa.attribute(xs_test)\n\n# [e.shape for e in [ig_attr_test,\n# ig_nt_attr_test,\n# dl_attr_test,\n# gs_attr_test,\n# fa_attr_test]]\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x = np.linspace(0, 1106-1, 1106),\n#                          y = ig_nt_attr_test.cpu().detach().numpy().mean(axis=0),\n#                          mode='lines', name='Test'))\n# fig.add_trace(go.Scatter(x = np.linspace(0, 1106-1, 1106),\n#                          y = dl_attr_test.cpu().detach().numpy().mean(axis=0),\n#                          mode='lines', name='Test'))\n# fig.add_trace(go.Scatter(x = np.linspace(0, 1106-1, 1106),\n#                          y = gs_attr_test.cpu().detach().numpy().mean(axis=0),\n#                          mode='lines', name='Test'))\n# fig.add_trace(go.Scatter(x = np.linspace(0, 1106-1, 1106),\n#                          y = fa_attr_test.cpu().detach().numpy().mean(axis=0),\n#                          mode='lines', name='Test'))\n# fig.show()\n\n# len(dl_attr_test.cpu().detach().numpy().mean(axis = 0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ## Version 2, Predict `y1` (Anthesis), `y2` (Silking), and `y3` (ASI)\n\n# Here each model will predict 3 values. The loss function is still mse, but the y tensors are concatenated\n\n# class NeuralNetwork(nn.Module):\n#     def __init__(self):\n#         super(NeuralNetwork, self).__init__()    \n#         self.x_network = nn.Sequential(\n#             nn.Linear(1106, 64),\n#             nn.BatchNorm1d(64),\n#             nn.ReLU(),\n#             nn.Linear(64, 3))\n        \n#     def forward(self, x):\n#         x_out = self.x_network(x)\n#         return x_out\n\n# model = NeuralNetwork().to(device)\n# # print(model)\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n# model(xs_i).shape # try prediction on one batch\n\n\n\n\n\n# def train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n#     size = len(dataloader.dataset)\n#     for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n#         # Compute prediction and loss\n#         pred = model(xs_i)\n#         loss = loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)) # &lt;----------------------------------------\n\n#         # Backpropagation\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         if batch % 100 == 0:\n#             loss, current = loss.item(), batch * len(y1_i) # &lt;----------------\n#             if not silent:\n#                 print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n                \n# def train_error(dataloader, model, loss_fn, silent = False):\n#     size = len(dataloader.dataset)\n#     num_batches = len(dataloader)\n#     train_loss = 0\n\n#     with torch.no_grad():\n#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n#             pred = model(xs_i)\n#             train_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # &lt;----------------------\n            \n#     train_loss /= num_batches\n#     return(train_loss) \n\n            \n# def test_loop(dataloader, model, loss_fn, silent = False):\n#     size = len(dataloader.dataset)\n#     num_batches = len(dataloader)\n#     test_loss = 0\n\n#     with torch.no_grad():\n#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n#             pred = model(xs_i)\n#             test_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # &lt;-----------------------\n\n#     test_loss /= num_batches\n#     if not silent:\n#         print(f\"Test Error: Avg loss: {test_loss:&gt;8f}\")\n#     return(test_loss) \n\n\n# def train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 500\n# ):\n#     # Initialize the loss function\n#     loss_fn = nn.MSELoss()\n#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n#     loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n#     loss_df['TrainMSE'] = np.nan\n#     loss_df['TestMSE']  = np.nan\n\n#     for t in tqdm.tqdm(range(epochs)):\n#         # print(f\"Epoch {t+1}\\n-------------------------------\")\n#         train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n\n#         loss_df.loc[loss_df.index == t, 'TrainMSE'\n#                    ] = train_error(training_dataloader, model, loss_fn, silent = True)\n        \n#         loss_df.loc[loss_df.index == t, 'TestMSE'\n#                    ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n        \n#     return([model, loss_df])\n\n# model, loss_df = train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 500\n# )\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n#                     mode='lines', name='Train'))\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n#                     mode='lines', name='Test'))\n# fig.show()\n\n# model, loss_df = train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 5000\n# )\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n#                     mode='lines', name='Train'))\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n#                     mode='lines', name='Test'))\n# fig.show()\n\n\n\n\n\n# '../ext_data/zma/panzea/phenotypes/'\n\n# # pd.read_table('../ext_data/zma/panzea/phenotypes/traitMatrix_maize282NAM_v15-130212.txt', low_memory = False)\n\n# # pd.read_excel('../ext_data/zma/panzea/phenotypes/traitMatrix_maize282NAM_v15-130212_TraitDescritptions.xlsx')"
  },
  {
    "objectID": "tianetal2011_model_simple.html",
    "href": "tianetal2011_model_simple.html",
    "title": "Tian et al. 2011 Model 1",
    "section": "",
    "text": "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n#                          Hours to wait\n# import time; time.sleep( (18*2) * (60*60))\n# Run Settings:\nnb_name = '11_TianEtAl2011'# Set manually! -----------------------------------\n\ndownsample_obs = False\ntrain_n = 90\ntest_n = 10\n\ndataloader_batch_size = 64\nrun_epochs = 200\n\nuse_gpu_num = 1\n\n# Imports --------------------------------------------------------------------\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nimport tqdm\nfrom tqdm import tqdm\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n\nimport dlgwas\nfrom dlgwas.kegg import ensure_dir_path_exists\nfrom dlgwas.kegg import get_cached_result\nfrom dlgwas.kegg import put_cached_result\n\nfrom dlgwas.dlfn import calc_cs\nfrom dlgwas.dlfn import apply_cs\nfrom dlgwas.dlfn import reverse_cs\n\nfrom dlgwas.dlfn import TianEtAl2011Dataset\nfrom dlgwas.dlfn import train_loop\nfrom dlgwas.dlfn import train_error\nfrom dlgwas.dlfn import test_loop\nfrom dlgwas.dlfn import train_nn\nfrom dlgwas.dlfn import yhat_loop\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\nUsing cuda device\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)\n\nensure_dir_path_exists(dir_path = '../models/'+nb_name)\nensure_dir_path_exists(dir_path = '../reports/'+nb_name)"
  },
  {
    "objectID": "tianetal2011_model_simple.html#load-cleaned-data",
    "href": "tianetal2011_model_simple.html#load-cleaned-data",
    "title": "Tian et al. 2011 Model 1",
    "section": "Load Cleaned Data",
    "text": "Load Cleaned Data\n\n# Read in cleaned data\ntaxa_groupings = pd.read_csv('../models/10_TianEtAl2011/taxa_groupings.csv')\ndata           = pd.read_csv('../models/10_TianEtAl2011/clean_data.csv')\n\n\n# Define holdout sets (Populations)\nuniq_pop = list(set(taxa_groupings['Population']))\nprint(str(len(uniq_pop))+\" Unique Holdout Groups.\")\ntaxa_groupings['Holdout'] = None\nfor i in range(len(uniq_pop)):\n    mask = (taxa_groupings['Population'] == uniq_pop[i])\n    taxa_groupings.loc[mask, 'Holdout'] = i\n\ntaxa_groupings\n\n25 Unique Holdout Groups.\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nsample\nPopulation\nHoldout\n\n\n\n\n0\n0\nZ001E0001\nB73 x B97\n22\n\n\n1\n1\nZ001E0002\nB73 x B97\n22\n\n\n2\n2\nZ001E0003\nB73 x B97\n22\n\n\n3\n3\nZ001E0004\nB73 x B97\n22\n\n\n4\n4\nZ001E0005\nB73 x B97\n22\n\n\n...\n...\n...\n...\n...\n\n\n4671\n4671\nZ026E0196\nB73 x Tzi8\n19\n\n\n4672\n4672\nZ026E0197\nB73 x Tzi8\n19\n\n\n4673\n4673\nZ026E0198\nB73 x Tzi8\n19\n\n\n4674\n4674\nZ026E0199\nB73 x Tzi8\n19\n\n\n4675\n4675\nZ026E0200\nB73 x Tzi8\n19\n\n\n\n\n4676 rows × 4 columns"
  },
  {
    "objectID": "tianetal2011_model_simple.html#setup-holdouts",
    "href": "tianetal2011_model_simple.html#setup-holdouts",
    "title": "Tian et al. 2011 Model 1",
    "section": "Setup Holdouts",
    "text": "Setup Holdouts\n\n#randomly holdout a population if there is not a file with the population held out.\n# Holdout_Int = 0\nHoldout_Int_path = '../models/'+nb_name+'/holdout_pop_int.pkl'\nif None != get_cached_result(Holdout_Int_path):\n    Holdout_Int = get_cached_result(Holdout_Int_path)\nelse:\n    Holdout_Int = int(np.random.choice([i for i in range(len(uniq_pop))], 1))\n    put_cached_result(Holdout_Int_path, Holdout_Int)\n\n\nprint(\"Holding out i=\"+str(Holdout_Int)+\": \"+uniq_pop[Holdout_Int])\n\nmask = (taxa_groupings['Holdout'] == Holdout_Int)\ntrain_idxs = list(taxa_groupings.loc[~mask, ].index)\ntest_idxs = list(taxa_groupings.loc[mask, ].index)\n\nHolding out i=22: B73 x B97\n\n\n\n[len(e) for e in [test_idxs, train_idxs]]\n\n[193, 4483]\n\n\n\n# downsample_obs = True\n# train_n = 900\n# test_n = 100\n\nif downsample_obs == True:\n    train_idxs = np.random.choice(train_idxs, train_n)\n    test_idxs = np.random.choice(test_idxs, test_n)\n    print([len(e) for e in [test_idxs, train_idxs]])\n\n\n# used to go from index in tensor to index in data so that the right xs tensor can be loaded in\nidx_original = np.array(data.index)\n\ny1 = data['leaf_length']\ny2 = data['leaf_width']\ny3 = data['upper_leaf_angle']\ny1 = np.array(y1)\ny2 = np.array(y2)\ny3 = np.array(y3)\n\n\nScale data\n\nscale_dict_path = '../models/'+nb_name+'/scale_dict.pkl'\nif None != get_cached_result(scale_dict_path):\n    scale_dict = get_cached_result(scale_dict_path)\nelse:\n    scale_dict = {\n        'y1':calc_cs(y1[train_idxs]),\n        'y2':calc_cs(y2[train_idxs]),\n        'y3':calc_cs(y3[train_idxs])\n    }\n    put_cached_result(scale_dict_path, scale_dict)\n\ny1 = apply_cs(y1, scale_dict['y1'])\ny2 = apply_cs(y2, scale_dict['y2'])\ny3 = apply_cs(y3, scale_dict['y3'])"
  },
  {
    "objectID": "tianetal2011_model_simple.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "href": "tianetal2011_model_simple.html#allow-for-cycling-data-onto-and-off-of-gpu",
    "title": "Tian et al. 2011 Model 1",
    "section": "Allow for cycling data onto and off of GPU",
    "text": "Allow for cycling data onto and off of GPU\n\n# loading this into memory causes the session to crash\n\ny1_train = torch.from_numpy(y1[train_idxs])[:, None]\ny2_train = torch.from_numpy(y2[train_idxs])[:, None]\ny3_train = torch.from_numpy(y3[train_idxs])[:, None]\n\nidx_original_train = torch.from_numpy(idx_original[train_idxs])\n\ny1_test = torch.from_numpy(y1[test_idxs])[:, None]\ny2_test = torch.from_numpy(y2[test_idxs])[:, None]\ny3_test = torch.from_numpy(y3[test_idxs])[:, None]\n\nidx_original_test = torch.from_numpy(idx_original[test_idxs])\n\n\n# dataloader_batch_size = 64\n\ntraining_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n                  y1 =           y1_train,\n                  y2 =           y2_train,\n                  y3 =           y3_train,\n        idx_original = idx_original_train,\n         use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\ntesting_dataloader = DataLoader(\n    TianEtAl2011Dataset(\n                  y1 =           y1_test,\n                  y2 =           y2_test,\n                  y3 =           y3_test,\n        idx_original = idx_original_test,\n         use_gpu_num = use_gpu_num,\n#         device = 'cpu'\n    ), \n    batch_size = dataloader_batch_size, \n    shuffle = True)\n\nUsing cuda device\nUsing cuda device"
  },
  {
    "objectID": "tianetal2011_model_simple.html#non-boilerplate",
    "href": "tianetal2011_model_simple.html#non-boilerplate",
    "title": "Tian et al. 2011 Model 1",
    "section": "Non-Boilerplate",
    "text": "Non-Boilerplate\n\n# xs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\n\n\n# del training_dataloader\n\n\n# torch.cuda.empty_cache()\n\n\n# xs_i.shape\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()    \n        self.x_network = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(3773820, 1),\n#             nn.BatchNorm1d(64),\n#             nn.ReLU(),\n#             nn.Linear(64, 1)\n        )\n        \n    def forward(self, x):\n        x_out = self.x_network(x)\n        return x_out\n\n\n# model = NeuralNetwork().to(device)\n# print(model)\n\n\n# model(xs_i)[0:3] # try prediction on one batch\n\n\n# def train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n#     size = len(dataloader.dataset)\n#     for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n#         # Compute prediction and loss\n#         pred = model(xs_i)\n#         loss = loss_fn(pred, y1_i) # &lt;----------------------------------------\n\n#         # Backpropagation\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         if batch % 100 == 0:\n#             loss, current = loss.item(), batch * len(y1_i) # &lt;----------------\n#             if not silent:\n#                 print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n                \n# def train_error(dataloader, model, loss_fn, silent = False):\n#     size = len(dataloader.dataset)\n#     num_batches = len(dataloader)\n#     train_loss = 0\n\n#     with torch.no_grad():\n#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n#             pred = model(xs_i)\n#             train_loss += loss_fn(pred, y1_i).item() # &lt;----------------------\n            \n#     train_loss /= num_batches\n#     return(train_loss) \n\n            \n# def test_loop(dataloader, model, loss_fn, silent = False):\n#     size = len(dataloader.dataset)\n#     num_batches = len(dataloader)\n#     test_loss = 0\n\n#     with torch.no_grad():\n#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n#             pred = model(xs_i)\n#             test_loss += loss_fn(pred, y1_i).item() # &lt;-----------------------\n\n#     test_loss /= num_batches\n#     if not silent:\n#         print(f\"Test Error: Avg loss: {test_loss:&gt;8f}\")\n#     return(test_loss)\n\n\n# def train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 500\n# ):\n#     # Initialize the loss function\n#     loss_fn = nn.MSELoss()\n#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n#     loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n#     loss_df['TrainMSE'] = np.nan\n#     loss_df['TestMSE']  = np.nan\n\n#     for t in tqdm(range(epochs)):        \n# #         print(f\"Epoch {t+1}\\n-------------------------------\")\n#         train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n\n#         loss_df.loc[loss_df.index == t, 'TrainMSE'\n#                    ] = train_error(training_dataloader, model, loss_fn, silent = True)\n        \n#         loss_df.loc[loss_df.index == t, 'TestMSE'\n#                    ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n        \n#         if (t+1)%10: # Cache in case training is interupted\n# #             print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n#             torch.save(model.state_dict(), \n#                        '../models/'+nb_name+'/model_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n        \n#     return([model, loss_df])\n\n\n# # don't run if either of these exist because there may be cases where we want the results but not the model\n\n# if not os.path.exists('../models/'+nb_name+'/model.pt'): \n#     model = NeuralNetwork().to(device)\n\n#     model, loss_df = train_nn(\n#         nb_name,\n#         training_dataloader,\n#         testing_dataloader,\n#         model,\n#         learning_rate = 1e-3,\n#         batch_size = dataloader_batch_size,\n#         epochs = run_epochs\n#     )\n    \n#     # experimental outputs:\n#     # 1. Model\n#     torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n\n#     # 2. loss_df\n#     loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)  \n    \n    \n#     # 3. predictions \n#     yhats = pd.concat([\n#         yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n#         yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n\n#     yhats.to_csv('../reports/'+nb_name+'/yhats.csv', index=False)\n\n\n# don't run if either of these exist because there may be cases where we want the results but not the model\n\nif not os.path.exists('../models/'+nb_name+'/model.pt'): \n    # Shared setup (train from scratch and load latest)\n    model = NeuralNetwork()\n\n    # find the biggest model to save\n    saved_models = os.listdir('../models/'+nb_name+'/')\n    saved_models = [e for e in saved_models if re.match('model*', e)]\n\n    if saved_models == []:\n        epochs_run = 0\n    else:\n        # if there are saved models reload and resume training\n        saved_models_numbers = [int(e.replace('model_', ''\n                                    ).replace('.pt', ''\n                                    ).split('_')[0]) for e in saved_models]\n        # saved_models\n        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n        latest_model = [e for e in saved_models if re.match(\n            '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n        model.load_state_dict(torch.load('../models/'+nb_name+'/'+latest_model))\n        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n    \n    model.to(device)\n#     model = NeuralNetwork().to(device)\n\n    model, loss_df = train_nn(\n        nb_name,\n        training_dataloader,\n        testing_dataloader,\n        model,\n        learning_rate = 1e-3,\n        batch_size = dataloader_batch_size,\n        epochs = (run_epochs - epochs_run)\n    )\n    \n    # experimental outputs:\n    # 1. Model\n    torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n\n    # 2. loss_df\n    loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)  \n    \n    # 3. predictions \n    yhats = pd.concat([\n        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n\n    yhats.to_csv('../reports/'+nb_name+'/yhats.csv', index=False)\n\nResuming Training: 52/200 epochs run.\n\n\n 75%|████████████████████████████████████████████████████████████████████████▊                        | 111/148 [8:19:21&lt;2:44:55, 267.44s/it]\n\n\n\nNeuralNetwork()\n\n\n# model, loss_df = train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 1\n# )\n\n\n# # don't run if either of these exist because there may be cases where we want the results but not the model\n# #| os.path.exists('../reports/'+nb_name+'/loss_df.csv')\n\n# if not os.path.exists('../models/'+nb_name+'/model.pt'): \n\n#     model, loss_df = train_nn(\n#         training_dataloader,\n#         testing_dataloader,\n#         model,\n#         learning_rate = 1e-3,\n#         batch_size = 64,\n#         epochs = 250\n#     )\n    \n#     # experimental outputs:\n#     # 1. Model\n#     torch.save(model.state_dict(), '../models/'+nb_name+'/model.pt') # convention is to use .pt or .pth\n\n#     # 2. loss_df\n#     loss_df.to_csv('../reports/'+nb_name+'/loss_df.csv', index=False)\n\n\n# This uses 6650/8192 MiB Available\n# Perhaps by using a larger batch size we can increase the used amount.\n# Increasing the size of the model will require more memory so this should be monitored.\n\n# for one iteration it takes 1/1 [04:28&lt;00:00, 268.24s/it]\n# NeuralNetwork(\n#   (x_network): Sequential(\n#     (0): Flatten(start_dim=1, end_dim=-1)\n#     (1): Linear(in_features=3773820, out_features=64, bias=True)\n#     (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#     (3): ReLU()\n#     (4): Linear(in_features=64, out_features=1, bias=True)\n#   )\n# )\n\n#     45/500 [3:22:20&lt;34:19:22, 271.57s/it]"
  },
  {
    "objectID": "tianetal2011_model_simple.html#standard-visualizations",
    "href": "tianetal2011_model_simple.html#standard-visualizations",
    "title": "Tian et al. 2011 Model 1",
    "section": "Standard Visualizations",
    "text": "Standard Visualizations\n\nloss_df = pd.read_csv('../reports/'+nb_name+'/loss_df.csv')\n\nloss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\nloss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n                    mode='lines', name='Train'))\nfig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n                    mode='lines', name='Test'))\nfig.show()\n\n\nyhats = pd.read_csv('../reports/'+nb_name+'/yhats.csv')\n\nyhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\nyhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n\npx.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")\n\n\nyhats['Error'] = yhats.y_true - yhats.y_pred\n\npx.histogram(yhats, x = 'Error', color = 'Split',\n             marginal=\"rug\", # can be `box`, `violin`\n             nbins= 50)\n\n\n# automatically kill kernel after running. \n# This is a hacky way to free up _all_ space on the gpus\nos._exit(00)"
  },
  {
    "objectID": "zma_retrieve_nucleotide_data.html",
    "href": "zma_retrieve_nucleotide_data.html",
    "title": "Retrieve Nucleotide Data",
    "section": "",
    "text": "# !cd ../ && pip install -e '.[dev]'\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nimport plotly.express as px\n\nimport hilbertcurve\nfrom hilbertcurve.hilbertcurve import HilbertCurve\n\n    # !conda install openpyxl -y\n    # ! conda install h5py -y\n# ! pip install hilbertcurve"
  },
  {
    "objectID": "zma_retrieve_nucleotide_data.html#access-marker-records",
    "href": "zma_retrieve_nucleotide_data.html#access-marker-records",
    "title": "Retrieve Nucleotide Data",
    "section": "Access marker records",
    "text": "Access marker records\nWorking with these records proved tricky. Ultimately I need the nucleotide data in a tensor, but after using tassel to save the data (ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023) as a table (along with position list and taxa list) it’s too big to easily load (&gt;30Gb). As a work around to easily access specific genomes, I split the table into a separate file for the header and each genome so that these files can be read piecemeal. See the Readme below for more details.\n\nsource\n\nread_txt\n\n read_txt (path, **kwargs)\n\n\nsource\n\n\nprint_txt\n\n print_txt (path)\n\n\nAGPv4_path = '../data/zma/panzea/genotypes/GBS/v27/'\n\n\nprint_txt(path = AGPv4_path+'Readme')\n\nGetting a script to split the table by line and rename all to the taxa name didn't work. It's possible that this is from sed, but it's not worth debugging. Instead I'm doing this manually which is what I should have done to begin with.\n\n1. use split to produce the needed files\nsplit -l 1 ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_Table.txt\n\n2. move those to their own folder\nmv xz* GBSv27_publicSamples_imputedV5_AGPv4-181023_Table/\n\n3. go over all files in the folder, pull out column one, swap out the : for another character and rename it.\n\n\n\nThis last point was completed with the following shell script.\n\nprint_txt(path = AGPv4_path+'rename_all.sh')\n\n#!/usr/bin/bash\nfiles_path='./ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_Table/'\nout_path='./ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_Table/'\n#out_path='./out/'\nfiles=$(ls \"$files_path\")\n\n#echo $files\n\nfor file in $files\ndo\n    #echo $file\n    taxa=$(awk '{print $1}' &lt;&lt;&lt; cat \"$files_path$file\")\n    taxa_sub=$(sed -r 's/[:]/__/g' &lt;&lt;&lt; \"$taxa\")\n    #echo $taxa_sub\n    cp $files_path$file $out_path$taxa_sub\n    #echo $taxa\ndone\n\n\n\nWith that done, and with the summary files from tassel (position and taxa), the genomes can be individually loaded as needed.\n\n# Other than listing the taxa this isn't expected to be of much use for our purposes.\nAGPv4_taxa=pd.read_table(AGPv4_path+'ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_TaxaList.txt')\nAGPv4_taxa.head()\n\n\n\n\n\n\n\n\nTaxa\nLibraryPrepID\nStatus\nDNAPlate\nGENUS\nINBREEDF\nSPECIES\nDNASample\nFlowcell_Lane\nNumLanes\n...\nGermplasmSet\nBarcode\nLibraryPlate\nTassel4SampleName\nPopulation\nLibraryPlateWell\nSampleDNAWell\nOwnerEmail\nPEDIGREE\nSeedLot\n\n\n\n\n0\n05-397:250007467\n250007467\npublic\nP3-GS-F\nZea\n0.95\nmays\n05-397\nC00R8ABXX_4\n1.0\n...\nMargaret Smith lines\nGTTGAA\nP3-GS-F\n05-397:C00R8ABXX:4:250007467\nInbred\nF11\nF11\nesb33@cornell.edu\nNaN\nNaN\n\n\n1\n05-438:250007407\n250007407\npublic\nP3-GS-F\nZea\n0.95\nmays\n05-438\nC00R8ABXX_4\n1.0\n...\nMargaret Smith lines\nGTATT\nP3-GS-F\n05-438:C00R8ABXX:4:250007407\nInbred\nB03\nB03\nesb33@cornell.edu\nNaN\nNaN\n\n\n2\n12E:250032344\n250032344\npublic\nAmes12\nZea\n0.95\nmays\n12E\n81FE8ABXX_4\n1.0\n...\nAmes\nGCTGTGGA\nAmes12\n12E:81FE8ABXX:4:250032344\ninbred\nH08\nH08\nesb33@cornell.edu\n12E\nNaN\n\n\n3\n207:250007202\n250007202\npublic\nP1-GS-F\nZea\n0.95\nmays\n207\nC00R8ABXX_2\n1.0\n...\nMargaret Smith lines\nTACAT\nP1-GS-F\n207:C00R8ABXX:2:250007202\nInbred\nE12\nE12\nesb33@cornell.edu\nNaN\nNaN\n\n\n4\n22612:250007466\n250007466\npublic\nP3-GS-F\nZea\n0.95\nmays\n22612\nC00R8ABXX_4\n1.0\n...\nMargaret Smith lines\nGTACTT\nP3-GS-F\n22612:C00R8ABXX:4:250007466\nInbred\nF10\nF10\nesb33@cornell.edu\nNaN\nNaN\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n# Useful for converting between the physical location and site\nAGPv4_site = pd.read_table(AGPv4_path+'ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_PositionList.txt')\nAGPv4_site.head()\n\n\n\n\n\n\n\n\nSite\nName\nChromosome\nPosition\n\n\n\n\n0\n0\nS1_6370\n1\n52399\n\n\n1\n1\nS1_8210\n1\n54239\n\n\n2\n2\nS1_8376\n1\n54405\n\n\n3\n3\nS1_9889\n1\n55917\n\n\n4\n4\nS1_9899\n1\n55927\n\n\n\n\n\n\n\nRetrieving a genome by taxa name:\n\n# The genomes are in a folder with an identical name as their source table\ntable_directory = AGPv4_path+'ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_Table/'\n# Note however that the naming is altered to not use ':'\nos.listdir(table_directory)[0:3]\n\n['xztea', 'xzedh', 'Z020E0024__250026132']\n\n\n\nsource\n\n\ntaxa_to_filename\n\n taxa_to_filename (taxa='05-397:250007467')\n\n\ntaxa_to_filename(taxa = '05-397:250007467')\n\n'05-397__250007467'\n\n\n\nsource\n\n\nfind_AGPv4\n\n find_AGPv4 (taxa, **kwargs)\n\nSearch for existing marker sets __\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ntaxa\nshould be the desired taxa or a regex fragment (stopping before the __). E.g. ‘B73’ or ‘B’\n\n\nkwargs\n\n\n\n\n\nsource\n\n\nget_AGPv4\n\n get_AGPv4 (taxa, **kwargs)\n\nRetrieve an existing marker set\n\n# def get_AGPv4( \n#     taxa,\n#     table_directory = '../data/zma/panzea/genotypes/GBS/v27/ZeaGBSv27_publicSamples_imputedV5_AGPv4-181023_Table/'\n# ):\n#     with open(table_directory+taxa, 'r') as f:\n#         data = f.read()    \n#     data = data.split('\\t')\n#     return(data)\n\n\nget_AGPv4('05-397__250007467')[0:4]\n\n['05-397:250007467', 'T', 'T', 'A']\n\n\nIn addition to returning a specific taxa, the table’s headers can be retieved with “taxa”.\n\nget_AGPv4(taxa = 'taxa')[0:4]\n\n['Taxa', '52399', '54239', '54405']\n\n\nConverting between site and chromosome/position requires the AGPv4_site dataframe. A given record contains the taxa as well as the nucleotides, so with that entry excluded the chromosome / position can be paired up.\n\nlen(get_AGPv4(taxa = 'taxa')), AGPv4_site.shape\n\n(943456, (943455, 4))\n\n\n\nith_taxa = '05-397:250007467'\nres = get_AGPv4(taxa_to_filename(taxa = ith_taxa))   # Retrieve record\ntemp = AGPv4_site.loc[:, ['Chromosome', 'Position']]  \ntemp[res[0]] = res[1:]                               # Add Col. with Nucleotides\ntemp.head()\n\n\n\n\n\n\n\n\nChromosome\nPosition\n05-397:250007467\n\n\n\n\n0\n1\n52399\nT\n\n\n1\n1\n54239\nT\n\n\n2\n1\n54405\nA\n\n\n3\n1\n55917\nN\n\n\n4\n1\n55927\nN"
  },
  {
    "objectID": "zma_retrieve_nucleotide_data.html#look-at-snp-coverage",
    "href": "zma_retrieve_nucleotide_data.html#look-at-snp-coverage",
    "title": "Retrieve Nucleotide Data",
    "section": "Look at SNP coverage",
    "text": "Look at SNP coverage\n\nmask = (temp.Chromosome == 1)\n\ntemp_pos = temp.loc[mask, ['Position']]\n\n\ntemp_pos['Shift'] = 0\ntemp_pos.loc[1: , ['Shift']] = np.array(temp_pos.Position)[:-1]\ntemp_pos['Diff'] = temp_pos['Position'] - temp_pos['Shift']\n\ntemp_pos.loc[0, 'Diff'] = None\n\n\ntemp_pos\n\n\n\n\n\n\n\n\nPosition\nShift\nDiff\n\n\n\n\n0\n52399\n0\nNaN\n\n\n1\n54239\n52399\n1840.0\n\n\n2\n54405\n54239\n166.0\n\n\n3\n55917\n54405\n1512.0\n\n\n4\n55927\n55917\n10.0\n\n\n...\n...\n...\n...\n\n\n147145\n306971046\n306910117\n60929.0\n\n\n147146\n306971061\n306971046\n15.0\n\n\n147147\n306971063\n306971061\n2.0\n\n\n147148\n306971073\n306971063\n10.0\n\n\n147149\n306971080\n306971073\n7.0\n\n\n\n\n147150 rows × 3 columns\n\n\n\n\n# px.histogram(temp_pos, x = 'Diff')"
  },
  {
    "objectID": "zma_retrieve_nucleotide_data.html#encode-a-marker-sequence-into-atcg",
    "href": "zma_retrieve_nucleotide_data.html#encode-a-marker-sequence-into-atcg",
    "title": "Retrieve Nucleotide Data",
    "section": "Encode a marker sequence into ATCG",
    "text": "Encode a marker sequence into ATCG\n\nres = get_AGPv4(taxa_to_filename(taxa = '05-397:250007467')) \nres = res[1:] # drop taxa\n\n\nsource\n\nlist_to_ACGT\n\n list_to_ACGT (in_seq, progress=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_seq\n\n\nThis should be a list with strings corresponding to IUPAC codes e.g. [‘A’, ‘C’, ‘Y’]\n\n\nprogress\nbool\nFalse\n\n\n\n\n\nres = list_to_ACGT(in_seq = res)\nres = res[0:1000]\n\n100%|███████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00&lt;00:00, 14.64it/s]"
  },
  {
    "objectID": "zma_retrieve_nucleotide_data.html#hilbert-curve-transform",
    "href": "zma_retrieve_nucleotide_data.html#hilbert-curve-transform",
    "title": "Retrieve Nucleotide Data",
    "section": "Hilbert Curve Transform",
    "text": "Hilbert Curve Transform\n\nres.shape\n\n(1000, 4)\n\n\n\nsource\n\ncalc_needed_hilbert_p\n\n calc_needed_hilbert_p (n_needed=1048576, max_p=20)\n\n\nsource\n\n\nnp_2d_to_hilbert\n\n np_2d_to_hilbert (in_seq)\n\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nin_seq\nThis should be a 2d numpy array with dimensions of [sequence, channels]\n\n\n\n\nsource\n\n\nnp_3d_to_hilbert\n\n np_3d_to_hilbert (in_seq)\n\nThis is the 3d version of np_2d_to_hilbert. The goal is to process all of the samples of an array in one go.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nin_seq\nThis should be a 3d numpy array with dimensions of [samples, sequence, channels]\n\n\n\n\ndemo = np_2d_to_hilbert(\n    in_seq = np.asarray([np.linspace(1, 100, num= 50),\n                         np.linspace(100, 1, num= 50)]).T\n)\n\n100%|███████████████████████████████████████████████████████████████████████████████| 50/50 [00:00&lt;00:00, 168852.82it/s]\n\n\n\npx.imshow(demo[:,:,0])\n\n\n                                                \n\n\n\npx.imshow(demo[:,:,1])\n\n\n                                                \n\n\n\n\nApply to subset of real marker data\nExplictly convert a taxa\n\ntaxa_to_filename(taxa = '05-397:250007467')\n\n'05-397__250007467'\n\n\nOr search for a taxa\n\nfind_AGPv4(taxa = '05-397')\n\n['05-397__250007467']\n\n\nRetrieve the sequence data\n\nres = get_AGPv4(taxa_to_filename(taxa = '05-397:250007467')) \nres = res[1:] # drop taxa\nres[0:10]\n\n['T', 'T', 'A', 'N', 'N', 'N', 'N', 'C', 'C', 'N']\n\n\nConvert from characters to encoded nucleotide probabilities\n\nres = list_to_ACGT(in_seq = res)\nres = res[0:1000]\nres\n\n100%|███████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00&lt;00:00, 15.00it/s]\n\n\narray([[0., 0., 0., 1.],\n       [0., 0., 0., 1.],\n       [1., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 1.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.]])\n\n\nConvert the sequence to a hilbert curve\n\n# This will happen under the hood\n# calc_needed_hilbert_p(n_needed=res.shape[0])\nres_hilb = np_2d_to_hilbert(\n    in_seq = res\n)\n\n100%|██████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00&lt;00:00, 1209081.58it/s]\n\n\n\npx.imshow( res[0:20, 0:1] )\n\n\n                                                \n\n\n\npx.imshow( res_hilb[:, :, 0] )\n\n\n                                                \n\n\n\npx.imshow( res_hilb[:, :, 1] )"
  },
  {
    "objectID": "buckleretal2009.html",
    "href": "buckleretal2009.html",
    "title": "Buckler et al. 2009",
    "section": "",
    "text": "It used data from panzea - Phenotypic data panzea_etal_2009_Science_flowering_time_data-090807\n- Genotypic Data panzea7_publicSamples_imputedV5_AGPv4-181023.vcf.gz - Genomic Data …\nuse_gpu_num = 1\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n# TODO fixme\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\nimport tqdm\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# [e for e in os.listdir() if re.match(\".+\\\\.txt\", e)]\n\n/home/labmember/mambaforge/envs/pytorch_mamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nUsing cuda device\nnam_overview = pd.read_table('../ext_data/zma/panzea/phenotypes/Buckler_etal_2009_Science_flowering_time_data-090807/NAMSum0607FloweringTraitBLUPsAcross8Envs.txt')\nnam_overview\n\n\n\n\n\n\n\n\nGeno_Code\nEntry_ID\nGroup\npop\nentry\nDays_To_Anthesis_BLUP_Sum0607\nDays_To_Silk_BLUP_Sum0607\nASI_BLUP_Sum0607\n\n\n\n\n0\nZ001E0001\n04P1367A51A\nZ001\n1\n1\n75.5364\n77.1298\n1.4600\n\n\n1\nZ001E0002\n04P1368A51A\nZ001\n1\n2\n76.9075\n77.7945\n1.3928\n\n\n2\nZ001E0003\n04P1368B51A\nZ001\n1\n3\n75.2646\n75.2555\n0.8644\n\n\n3\nZ001E0004\n04P1370B51A\nZ001\n1\n4\n73.6933\n75.7604\n2.0012\n\n\n4\nZ001E0005\n04P1371B51A\nZ001\n1\n5\n79.2441\n81.2611\n1.8931\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5458\nZ027E0277\nW64A\nNaN\n27\n277\n71.9008\n73.9811\n2.6756\n\n\n5459\nZ027E0278\nWD\nNaN\n27\n278\n62.0212\n60.5992\n-0.5733\n\n\n5460\nZ027E0279\nWf9\nNaN\n27\n279\n71.9970\n72.2319\n0.8338\n\n\n5461\nZ027E0280\nYu796_NS\nNaN\n27\n280\n74.5107\n73.9727\n0.2935\n\n\n5462\nZ027E0282\nMo17\nNaN\n27\n282\n72.7428\n75.5080\n3.0455\n\n\n\n\n5463 rows × 8 columns\ndata = pd.read_table('../ext_data/zma/panzea/phenotypes/Buckler_etal_2009_Science_flowering_time_data-090807/markergenotypes062508.txt', skiprows=1\n                    ).reset_index().rename(columns = {'index': 'Geno_Code'})\ndata\n\n\n\n\n\n\n\n\nGeno_Code\ndays2anthesis\ndays2silk\nasi\npop\ni0\ni1\ni2\ni3\ni4\n...\ni1096\ni1097\ni1098\ni1099\ni1100\ni1101\ni1102\ni1103\ni1104\ni1105\n\n\n\n\n0\nZ001E0001\n75.5364\n77.1298\n1.4600\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n2.0\n2.0\n2.0\n2.0\n2.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nZ001E0002\n76.9075\n77.7945\n1.3928\n1\n2.0\n2.0\n2.0\n2.0\n2.0\n...\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n\n\n2\nZ001E0003\n75.2646\n75.2555\n0.8644\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\nZ001E0004\n73.6933\n75.7604\n2.0012\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n2.0\n2.0\n2.0\n2.0\n\n\n4\nZ001E0005\n79.2441\n81.2611\n1.8931\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n2.0\n2.0\n2.0\n2.0\n2.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4694\nZ026E0196\n77.6523\n80.2916\n1.9698\n26\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1.0\n1.0\n1.0\n1.0\n\n\n4695\nZ026E0197\n78.5015\n82.2767\n3.2979\n26\n2.0\n2.0\n2.0\n2.0\n2.0\n...\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n1.0\n1.0\n1.0\n1.0\n\n\n4696\nZ026E0198\n77.4219\n79.7868\n2.2208\n26\n1.0\n1.0\n1.0\n1.0\n2.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4697\nZ026E0199\n78.6712\n82.8476\n4.1247\n26\n2.0\n2.0\n0.0\n0.0\n0.0\n...\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n\n\n4698\nZ026E0200\n77.4937\n82.4678\n4.2915\n26\n0.0\n0.0\n0.0\n2.0\n2.0\n...\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n2.0\n\n\n\n\n4699 rows × 1111 columns\npx.scatter_matrix(data.loc[:, ['days2anthesis', 'days2silk', 'asi']])\n\n/home/labmember/mambaforge/envs/pytorch_mamba/lib/python3.10/site-packages/plotly/express/_core.py:279: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  dims = [\nd2a = np.array(data['days2anthesis'])\nd2s = np.array(data['days2silk'])\nasi = np.array(data['asi'])\n\nxs = np.array(data.drop(columns = ['days2anthesis', 'days2silk', 'asi', 'pop', 'Geno_Code']))\n\nn_obs = xs.shape[0]\n\nnp_seed = 9070707\nrng = np.random.default_rng(np_seed)  # can be called without a seed\n\ntest_pr = 0.2\n\ntest_n = round(n_obs*test_pr)\nidxs = np.linspace(0, n_obs-1, num = n_obs).astype(int)\nrng.shuffle(idxs)\n\ntest_idxs = idxs[0:test_n]\ntrain_idxs = idxs[test_n:-1]\n# make up tensors\ndef calc_cs(x): return [np.mean(x, axis = 0), np.std(x, axis = 0)]\ndef apply_cs(xs, cs_dict_entry): return ((xs - cs_dict_entry[0]) / cs_dict_entry[0])\nscale_dict = {\n    'd2a':calc_cs(d2a[train_idxs]),\n    'd2s':calc_cs(d2s[train_idxs]),\n    'asi':calc_cs(asi[train_idxs]),\n    'xs' :calc_cs(xs[train_idxs])\n}\ny1 = apply_cs(d2a, scale_dict['d2a'])\ny2 = apply_cs(d2s, scale_dict['d2s'])\ny3 = apply_cs(asi, scale_dict['asi'])\n\n# No need to cs xs -- 0-2 scale\n# apply_cs(xs, scale_dict['xs'])\n\ny1_train = torch.from_numpy(y1[train_idxs]).to(device).float()[:, None]\ny2_train = torch.from_numpy(y2[train_idxs]).to(device).float()[:, None]\ny3_train = torch.from_numpy(y3[train_idxs]).to(device).float()[:, None]\nxs_train = torch.from_numpy(xs[train_idxs]).to(device).float()\n\ny1_test = torch.from_numpy(y1[test_idxs]).to(device).float()[:, None]\ny2_test = torch.from_numpy(y2[test_idxs]).to(device).float()[:, None]\ny3_test = torch.from_numpy(y3[test_idxs]).to(device).float()[:, None]\nxs_test = torch.from_numpy(xs[test_idxs]).to(device).float()\nclass CustomDataset(Dataset):\n    def __init__(self, y1, y2, y3, xs, transform = None, target_transform = None):\n        self.y1 = y1\n        self.y2 = y2\n        self.y3 = y3\n        self.xs = xs\n        self.transform = transform\n        self.target_transform = target_transform    \n    \n    def __len__(self):\n        return len(self.y1)\n    \n    def __getitem__(self, idx):\n        y1_idx = self.y1[idx]\n        y2_idx = self.y2[idx]\n        y3_idx = self.y3[idx]\n        xs_idx = self.xs[idx]\n        \n        if self.transform:\n            xs_idx = self.transform(xs_idx)\n            \n        if self.target_transform:\n            y1_idx = self.transform(y1_idx)\n            y2_idx = self.transform(y2_idx)\n            y3_idx = self.transform(y3_idx)\n        return xs_idx, y1_idx, y2_idx, y3_idx\ntraining_dataloader = DataLoader(\n    CustomDataset(\n        y1 = y1_train,\n        y2 = y2_train,\n        y3 = y3_train,\n        xs = xs_train\n    ), \n    batch_size = 64, \n    shuffle = True)\n\ntesting_dataloader = DataLoader(\n    CustomDataset(\n        y1 = y1_test,\n        y2 = y2_test,\n        y3 = y3_test,\n        xs = xs_test\n    ), \n    batch_size = 64, \n    shuffle = True)\n\nxs.shape\n\n(4699, 1106)"
  },
  {
    "objectID": "buckleretal2009.html#version-1-predict-y1-anthesis",
    "href": "buckleretal2009.html#version-1-predict-y1-anthesis",
    "title": "Buckler et al. 2009",
    "section": "Version 1, Predict y1 (Anthesis)",
    "text": "Version 1, Predict y1 (Anthesis)\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()    \n        self.x_network = nn.Sequential(\n            nn.Linear(1106, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Linear(64, 1))\n        \n    def forward(self, x):\n        x_out = self.x_network(x)\n        return x_out\n\nmodel = NeuralNetwork().to(device)\n# print(model)\n\n\nxs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\nmodel(xs_i).shape # try prediction on one batch\n\ntorch.Size([64, 1])\n\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n    size = len(dataloader.dataset)\n    for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(xs_i)\n        loss = loss_fn(pred, y1_i) # &lt;----------------------------------------\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(y1_i) # &lt;----------------\n            if not silent:\n                print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n                \ndef train_error(dataloader, model, loss_fn, silent = False):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    train_loss = 0\n\n    with torch.no_grad():\n        for xs_i, y1_i, y2_i, y3_i in dataloader:\n            pred = model(xs_i)\n            train_loss += loss_fn(pred, y1_i).item() # &lt;----------------------\n            \n    train_loss /= num_batches\n    return(train_loss) \n\n            \ndef test_loop(dataloader, model, loss_fn, silent = False):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss = 0\n\n    with torch.no_grad():\n        for xs_i, y1_i, y2_i, y3_i in dataloader:\n            pred = model(xs_i)\n            test_loss += loss_fn(pred, y1_i).item() # &lt;-----------------------\n\n    test_loss /= num_batches\n    if not silent:\n        print(f\"Test Error: Avg loss: {test_loss:&gt;8f}\")\n    return(test_loss) \n\n\ndef train_nn(\n    training_dataloader,\n    testing_dataloader,\n    model,\n    learning_rate = 1e-3,\n    batch_size = 64,\n    epochs = 500\n):\n    # Initialize the loss function\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n    loss_df['TrainMSE'] = np.nan\n    loss_df['TestMSE']  = np.nan\n\n    for t in tqdm.tqdm(range(epochs)):\n        # print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n\n        loss_df.loc[loss_df.index == t, 'TrainMSE'\n                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n        \n        loss_df.loc[loss_df.index == t, 'TestMSE'\n                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n        \n    return([model, loss_df])\n\n\n# model, loss_df = train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 500\n# )\n\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n#                     mode='lines', name='Train'))\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n#                     mode='lines', name='Test'))\n# fig.show()\n\n\n# ! conda install captum -c pytorch -y\n\n\n# # imports from captum library\n# from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n# from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation\n\n\n# ig = IntegratedGradients(model)\n# ig_nt = NoiseTunnel(ig)\n# dl = DeepLift(model)\n# gs = GradientShap(model)\n# fa = FeatureAblation(model)\n\n# ig_attr_test = ig.attribute(xs_test, n_steps=50)\n# ig_nt_attr_test = ig_nt.attribute(xs_test)\n# dl_attr_test = dl.attribute(xs_test)\n# gs_attr_test = gs.attribute(xs_test, xs_train)\n# fa_attr_test = fa.attribute(xs_test)\n\n# [e.shape for e in [ig_attr_test,\n# ig_nt_attr_test,\n# dl_attr_test,\n# gs_attr_test,\n# fa_attr_test]]\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x = np.linspace(0, 1106-1, 1106),\n#                          y = ig_nt_attr_test.cpu().detach().numpy().mean(axis=0),\n#                          mode='lines', name='Test'))\n# fig.add_trace(go.Scatter(x = np.linspace(0, 1106-1, 1106),\n#                          y = dl_attr_test.cpu().detach().numpy().mean(axis=0),\n#                          mode='lines', name='Test'))\n# fig.add_trace(go.Scatter(x = np.linspace(0, 1106-1, 1106),\n#                          y = gs_attr_test.cpu().detach().numpy().mean(axis=0),\n#                          mode='lines', name='Test'))\n# fig.add_trace(go.Scatter(x = np.linspace(0, 1106-1, 1106),\n#                          y = fa_attr_test.cpu().detach().numpy().mean(axis=0),\n#                          mode='lines', name='Test'))\n# fig.show()\n\n# len(dl_attr_test.cpu().detach().numpy().mean(axis = 0))"
  },
  {
    "objectID": "buckleretal2009.html#version-2-predict-y1-anthesis-y2-silking-and-y3-asi",
    "href": "buckleretal2009.html#version-2-predict-y1-anthesis-y2-silking-and-y3-asi",
    "title": "Buckler et al. 2009",
    "section": "Version 2, Predict y1 (Anthesis), y2 (Silking), and y3 (ASI)",
    "text": "Version 2, Predict y1 (Anthesis), y2 (Silking), and y3 (ASI)\nHere each model will predict 3 values. The loss function is still mse, but the y tensors are concatenated\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()    \n        self.x_network = nn.Sequential(\n            nn.Linear(1106, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Linear(64, 3))\n        \n    def forward(self, x):\n        x_out = self.x_network(x)\n        return x_out\n\nmodel = NeuralNetwork().to(device)\n# print(model)\n\n\nxs_i, y1_i, y2_i, y3_i = next(iter(training_dataloader))\nmodel(xs_i).shape # try prediction on one batch\n\ntorch.Size([64, 3])\n\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n    size = len(dataloader.dataset)\n    for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(xs_i)\n        loss = loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)) # &lt;----------------------------------------\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(y1_i) # &lt;----------------\n            if not silent:\n                print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n                \ndef train_error(dataloader, model, loss_fn, silent = False):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    train_loss = 0\n\n    with torch.no_grad():\n        for xs_i, y1_i, y2_i, y3_i in dataloader:\n            pred = model(xs_i)\n            train_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # &lt;----------------------\n            \n    train_loss /= num_batches\n    return(train_loss) \n\n            \ndef test_loop(dataloader, model, loss_fn, silent = False):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss = 0\n\n    with torch.no_grad():\n        for xs_i, y1_i, y2_i, y3_i in dataloader:\n            pred = model(xs_i)\n            test_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # &lt;-----------------------\n\n    test_loss /= num_batches\n    if not silent:\n        print(f\"Test Error: Avg loss: {test_loss:&gt;8f}\")\n    return(test_loss) \n\n\ndef train_nn(\n    training_dataloader,\n    testing_dataloader,\n    model,\n    learning_rate = 1e-3,\n    batch_size = 64,\n    epochs = 500\n):\n    # Initialize the loss function\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n    loss_df['TrainMSE'] = np.nan\n    loss_df['TestMSE']  = np.nan\n\n    for t in tqdm.tqdm(range(epochs)):\n        # print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n\n        loss_df.loc[loss_df.index == t, 'TrainMSE'\n                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n        \n        loss_df.loc[loss_df.index == t, 'TestMSE'\n                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n        \n    return([model, loss_df])\n\n\n# model, loss_df = train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 500\n# )\n\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n#                     mode='lines', name='Train'))\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n#                     mode='lines', name='Test'))\n# fig.show()\n\n\n# model, loss_df = train_nn(\n#     training_dataloader,\n#     testing_dataloader,\n#     model,\n#     learning_rate = 1e-3,\n#     batch_size = 64,\n#     epochs = 5000\n# )\n\n\n# fig = go.Figure()\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n#                     mode='lines', name='Train'))\n# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n#                     mode='lines', name='Test'))\n# fig.show()\n\n\n'../ext_data/zma/panzea/phenotypes/'\n\n'../ext_data/zma/panzea/phenotypes/'\n\n\n\n# pd.read_table('../ext_data/zma/panzea/phenotypes/traitMatrix_maize282NAM_v15-130212.txt', low_memory = False)\n\n\n# pd.read_excel('../ext_data/zma/panzea/phenotypes/traitMatrix_maize282NAM_v15-130212_TraitDescritptions.xlsx')"
  }
]